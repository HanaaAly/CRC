PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Qin, ZP; Sun, WH; Guo, TH; Lu, GD				Qin, Zhuanping; Sun, Wenhao; Guo, Tinghang; Lu, Guangda			Colorectal cancer image recognition algorithm based on improved transformer	DISCOVER APPLIED SCIENCES			English	Article						Medical image processing; Colorectal cancer; Transformer; Feature extraction		Aiming at the problems of the complex background of colorectal cancer tissue cell images and the difficulty of detection caused by the low differentiation of cancer cell regions, a deep learning method is used to detect the cancer cell regions. By integrating the skip feedback connection structure into U-Net and combining it with the Swin Transformer for feature extraction, we improve the multi-level feature extraction capabilities of the model. This algorithm enables end-to-end recognition of colorectal adenocarcinoma tissue images and achieves an accuracy of 95.8% on the NCT-CRC-HE-100K dataset, demonstrating its potential to significantly support colorectal cancer detection and treatment. For the first time, the method of introducing the jump feedback connection structure of U-Net combined with the Swin Transformer backbone is used to recognize the pathological section cells of colorectal cancer with good results.	[Qin, Zhuanping; Sun, Wenhao; Guo, Tinghang; Lu, Guangda] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China; [Qin, Zhuanping; Sun, Wenhao; Guo, Tinghang; Lu, Guangda] Tianjin Key Lab Informat Sensing & Intelligent Con, Tianjin 300222, Peoples R China	Tianjin University of Technology & Education	Qin, ZP (corresponding author), Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.; Qin, ZP (corresponding author), Tianjin Key Lab Informat Sensing & Intelligent Con, Tianjin 300222, Peoples R China.	qinzhuanping@126.com; whsun@tute.edu.cn; guotinghang@tute.edu.cn; lugd1229@163.com			Tianjin Municipal Education Commission Scientific Research Program Project	Tianjin Municipal Education Commission Scientific Research Program Project	No Statement Available	Acheampong FA, 2021, ARTIF INTELL REV, V54, P5789, DOI 10.1007/s10462-021-09958-2; Dai Y, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11081384; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Ernst P, 2023, NEURAL NETWORKS, V166, P704, DOI 10.1016/j.neunet.2023.08.004; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang P, 2023, IEEE T MED IMAGING, V42, P15, DOI 10.1109/TMI.2022.3202248; Jiabao Z., 2023, Laser Optoelectron Progr, V60, P291; Jianzhi D., 2023, Sci Technol Eng, V23, P2922; Kather Jakob Nikolas, 2018, Zenodo, Vv0.1, DOI 10.5281/zenodo.1214455; Lan YC, 2020, IEEE ACCESS, V8, P195327, DOI 10.1109/ACCESS.2020.3034230; Lin AQ, 2022, FRONT NUTR, V9, DOI 10.3389/fnut.2022.869263; Liu R, 2022, COMPUT METH PROG BIO, V226, DOI 10.1016/j.cmpb.2022.107162; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Mármol I, 2017, INT J MOL SCI, V18, DOI 10.3390/ijms18010197; Mohammed MA, 2023, COMPUT BIOL MED, V154, DOI 10.1016/j.compbiomed.2023.106617; Pei Xiaoyue Hu., 2020, J Clin Pathol Res, V40, P1941; Pierre K, 2023, EXPERT REV ANTICANC, V23, P1265, DOI 10.1080/14737140.2023.2286001; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Tan MX, 2019, PR MACH LEARN RES, V97; Vaswani A, 2023, Arxiv, DOI [arXiv:1706.03762, DOI 10.48550/ARXIV.1706.03762]; Wightman R., 2021, arXiv; Xiao HG, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104791; Xiusen Q., 2020, Chin J Bases Clin Gen Surg, V27, P906; Xu J, 2023, IEEE T NEUR NET LEAR, V34, P9562, DOI 10.1109/TNNLS.2022.3158966	26	0	0	0	0	SPRINGER	NEW YORK	ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES		3004-9261		DISCOV APPL SCI	DISCOV. APPL. SCI.	AUG 1	2024	6	8							422	10.1007/s42452-024-06127-2	http://dx.doi.org/10.1007/s42452-024-06127-2			11	Multidisciplinary Sciences	Emerging Sources Citation Index (ESCI)	Science & Technology - Other Topics	A4I0W		hybrid			2024-09-18	WOS:001282173600002
J	Khalid, M; Deivasigamani, S; Sathiya, ; Rajendran, S				Khalid, Majdi; Deivasigamani, Sugitha; Sathiya, V; Rajendran, Surendran			An efficient colorectal cancer detection network using atrous convolution with coordinate attention transformer and histopathological images	SCIENTIFIC REPORTS			English	Article						Colorectal cancer; Colorectal cancer detection network; Atrous convolution with coordinate attention transformer; Cross-shaped window transformer; Histopathology image	DIAGNOSIS	The second most common type of malignant tumor worldwide is colorectal cancer. Histopathology image analysis offers crucial data for the clinical diagnosis of colorectal cancer. Currently, deep learning techniques are applied to enhance cancer classification and tumor localization in histopathological image analysis. Moreover, traditional deep learning techniques might loss integrated information in the image while evaluating thousands of patches recovered from whole slide images (WSIs). This research proposes a novel colorectal cancer detection network (CCDNet) that combines coordinate attention transformer with atrous convolution. CCDNet first denoises the input histopathological image using a Wiener based Midpoint weighted non-local means filter (WMW-NLM) for guaranteeing precise diagnoses and maintain image features. Also, a novel atrous convolution with coordinate attention transformer (AConvCAT) is introduced, which successfully combines the advantages of two networks to classify colorectal tissue at various scales by capturing local and global information. Further, coordinate attention model is integrated with a Cross-shaped window (CrSWin) transformer for capturing tiny changes in colorectal tissue from multiple angles. The proposed CCDNet achieved accuracy rates of 98.61% and 98.96%, on the colorectal histological image and NCT-CRC-HE-100 K datasets correspondingly. The comparison analysis demonstrates that the suggested framework performed better than the most advanced methods already in use. In hospitals, clinicians can use the proposed CCDNet to verify the diagnosis.	[Khalid, Majdi] Umm Al Qura Univ, Coll Comp, Dept Comp Sci & Artificial Intelligence, Mecca 21955, Saudi Arabia; [Deivasigamani, Sugitha] Univ Coll Engn, Dept Comp Sci & Engn, Chennai, India; [Sathiya, V] Panimalar Engn Coll, Dept Comp Sci & Engn, Chennai 600123, India; [Rajendran, Surendran] Saveetha Inst Med & Tech Sci, Saveetha Sch Engn, Dept Comp Sci & Engn, Chennai 602105, Tamil Nadu, India	Umm Al Qura University; Saveetha Institute of Medical & Technical Science; Saveetha School of Engineering	Rajendran, S (corresponding author), Saveetha Inst Med & Tech Sci, Saveetha Sch Engn, Dept Comp Sci & Engn, Chennai 602105, Tamil Nadu, India.	surendran.phd.it@gmail.com	Rajendran, Surendran/AAC-8702-2021	Khalid, Majdi/0000-0002-5397-0428	Umm Al- Qura University	Umm Al- Qura University	No Statement Available	Bousis D, 2023, GASTROENTEROL REV, V18, P266, DOI 10.5114/pg.2023.129494; Chattopadhyay A., 2022, NEUROSCI INFORM, V2, P100060, DOI [10.1016/J.NEURI.2022.100060, DOI 10.1016/J.NEURI.2022.100060, 10.1016/j.neuri.2022.100060]; Chlorogiannis DD, 2023, GASTROENTEROL REV, V18, P353, DOI 10.5114/pg.2023.130337; Davri A, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12040837; Fadafen MK, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-35431-x; Ghosh S, 2021, ENG APPL ARTIF INTEL, V100, DOI 10.1016/j.engappai.2021.104202; Graham S, 2023, MED IMAGE ANAL, V83, DOI 10.1016/j.media.2022.102685; Jain Astha, 2022, Proceedings of Data Analytics and Management: ICDAM 2021. Lecture Notes on Data Engineering and Communications Technologies (90), P493, DOI 10.1007/978-981-16-6289-8_42; Khan A, 2023, MODERN PATHOL, V36, DOI 10.1016/j.modpat.2023.100118; Kumar A, 2023, BIOMED SIGNAL PROCES, V79, DOI 10.1016/j.bspc.2022.104172; Li YK, 2020, IEEE ACCESS, V8, P114916, DOI 10.1109/ACCESS.2020.3003999; Liang MY, 2020, IEEE ACCESS, V8, P208969, DOI 10.1109/ACCESS.2020.3038764; Luo YX, 2024, AIMS MATH, V9, P8814, DOI 10.3934/math.2024429; Murakami T, 2022, DIGEST ENDOSC, V34, P1096, DOI 10.1111/den.14273; Ogudo KA, 2023, COMPUT SYST SCI ENG, V44, P693, DOI 10.32604/csse.2023.024154; Pacal I, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104003; Rajendran S, 2023, IEEE ACCESS, V11, P64758, DOI 10.1109/ACCESS.2023.3288017; Sarvamangala DR, 2022, EVOL INTELL, V15, P1, DOI 10.1007/s12065-020-00540-3; Selvanarayanan R, 2024, SCI REP-UK, V14, DOI 10.1038/s41598-024-56954-x; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Thanarajan T, 2023, CMC-COMPUT MATER CON, V76, P1995, DOI 10.32604/cmc.2023.039644; Thanarajan T, 2023, AIMS MATH, V8, P12520, DOI 10.3934/math.2023629; Tsai MJ, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10141662; Wang KS, 2021, BMC MED, V19, DOI 10.1186/s12916-021-01942-5; Wang XY, 2022, MED IMAGE ANAL, V81, DOI 10.1016/j.media.2022.102559; Xu H, 2023, CLIN GASTROENTEROL H, V21, DOI 10.1016/j.cgh.2022.07.006; Yin ZG, 2023, FRONT MED-LAUSANNE, V10, DOI 10.3389/fmed.2023.1128084; Yu CR, 2022, ARTIF INTELL REV, V55, P323, DOI 10.1007/s10462-021-10034-y; Yu S., 2021, MED IM COMP COMP ASS; Zidan U, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119452	30	0	0	0	0	NATURE PORTFOLIO	BERLIN	HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY	2045-2322			SCI REP-UK	Sci Rep	AUG 17	2024	14	1							19109	10.1038/s41598-024-70117-y	http://dx.doi.org/10.1038/s41598-024-70117-y			15	Multidisciplinary Sciences	Science Citation Index Expanded (SCI-EXPANDED)	Science & Technology - Other Topics	D0A6D	39154091	gold			2024-09-18	WOS:001292901700047
J	Daungsupawong, H; Wiwanitkit, V				Daungsupawong, Hinpetch; Wiwanitkit, Viroj			Colorectal Cancer Prevention and Chat Generative Pretrained Transformer (ChatGPT)	JOURNAL OF CLINICAL GASTROENTEROLOGY			English	Article									[Wiwanitkit, Viroj] Saveetha Inst Med & Tech Sci, Saveetha Med Coll, Chennai, India	Saveetha Institute of Medical & Technical Science; Saveetha Medical College & Hospital	Wiwanitkit, V (corresponding author), Saveetha Inst Med & Tech Sci, Saveetha Med Coll, Chennai, India.	hinpetchdaung@gmail.com; wviroj@yahoo.com						Kleebayoon A, 2023, CELL MOL BIOENG, V16, P173, DOI 10.1007/s12195-023-00759-x; Pereyra Lisandro, 2024, J Clin Gastroenterol, DOI 10.1097/MCG.0000000000001979	2	0	0	1	1	LIPPINCOTT WILLIAMS & WILKINS	PHILADELPHIA	TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA	0192-0790	1539-2031		J CLIN GASTROENTEROL	J. Clin. Gastroenterol.	MAY-JUN	2024	58	5					531	531		10.1097/MCG.0000000000001989	http://dx.doi.org/10.1097/MCG.0000000000001989			1	Gastroenterology & Hepatology	Science Citation Index Expanded (SCI-EXPANDED)	Gastroenterology & Hepatology	MZ6D3	38457428				2024-09-18	WOS:001197487000002
J	Lv, ZL; Lin, YX; Yan, R; Wang, Y; Zhang, F				Lv, Zhilong; Lin, Yuexiao; Yan, Rui; Wang, Ying; Zhang, Fa			TransSurv: Transformer-Based Survival Analysis Model Integrating Histopathological Images and Genomic Data for Colorectal Cancer	IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS			English	Article						Cancer; Genomics; Bioinformatics; Transformers; Tumors; Feature extraction; Prognostics and health management; Survival analysis; multi-modal learning; transformer; histopathological slides; genomic data	REGRESSION; EXPRESSION; MODULES	Survival analysis is a significant study in cancer prognosis, and the multi-modal data, including histopathological images, genomic data, and clinical information, provides unprecedented opportunities for its development. However, because of the high dimensionality and the heterogeneity of histopathological images and genomic data, acquiring effective predictive characters from these multi-modal data has always been a challenge for survival analysis. In this article, we propose a transformer-based survival analysis model (TransSurv) for colorectal cancer that can effectively integrate intra-modality and inter-modality features of histopathological images, genomic data, and clinical information. Specifically, to integrate the intra-modality relationship of image patches, we develop a multi-scale histopathological features fusion transformer (MS-Trans). Furthermore, we provide a cross-modal fusion transformer based on cross attention for multi-scale pathological representation and multi-omics representation, which includes RNA-seq expression and copy number alteration (CNA). At the output layer of the TransSurv, we adopt the Cox layer to integrate multi-modal fusion representation with clinical information for end-to-end survival analysis. The experimental results on the Cancer Genome Atlas (TCGA) colorectal cancer cohort demonstrate that the proposed TransSurv outperforms the existing methods and improves the prognosis prediction of colorectal cancer.	[Lv, Zhilong; Yan, Rui; Zhang, Fa] Chinese Acad Sci, Inst Comp Technol, High Performance Comp Res Ctr, Beijing 100190, Peoples R China; [Lv, Zhilong; Yan, Rui] Univ Chinese Acad Sci, Beijing 101408, Peoples R China; [Lin, Yuexiao] Capital Med Univ, Beijing Chaoyang Hosp, Dept Gen Surg, Beijing 100020, Peoples R China; [Wang, Ying] Capital Med Univ, Beijing Chaoyang Hosp, Dept Pathol, Beijing 100020, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Capital Medical University; Capital Medical University	Zhang, F (corresponding author), Chinese Acad Sci, Inst Comp Technol, High Performance Comp Res Ctr, Beijing 100190, Peoples R China.; Wang, Y (corresponding author), Capital Med Univ, Beijing Chaoyang Hosp, Dept Pathol, Beijing 100020, Peoples R China.	lvzhilong17g@ict.ac.cn; linyuexiao2506@163.com; yanrui20b@ict.ac.cn; wangying_blk@126.com; zhangfa@ict.ac.cn			Chinese Academy of Sciences	Chinese Academy of Sciences(Chinese Academy of Sciences)	No Statement Available	Aperio Technologies Inc., 2008, Digital slides and third-party data interchange; Bándi P, 2019, IEEE T MED IMAGING, V38, P550, DOI 10.1109/TMI.2018.2867350; Boughorbel S, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0146413; Calderaro J, 2019, J HEPATOL, V70, P58, DOI 10.1016/j.jhep.2018.09.003; Cheerla A, 2019, BIOINFORMATICS, V35, pI446, DOI 10.1093/bioinformatics/btz342; Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041; Chen RJ, 2022, IEEE T MED IMAGING, V41, P757, DOI [10.1109/TITS.2020.3030218, 10.1109/TMI.2020.3021387]; COX DR, 1972, J R STAT SOC B, V34, P187; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Ferlay J., 2022, Int. Agency Res. Cancer; Gripp S, 2007, J CLIN ONCOL, V25, P3313, DOI 10.1200/JCO.2006.10.5411; Guadagno E, 2016, PATHOL RES PRACT, V212, P825, DOI 10.1016/j.prp.2016.07.002; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hongyi Duanmu, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12262), P242, DOI 10.1007/978-3-030-59713-9_24; Jiawen Yao, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P406, DOI 10.1007/978-3-319-66185-8_46; KAPLAN EL, 1958, J AM STAT ASSOC, V53, P457, DOI 10.2307/2281868; Kather JN, 2019, PLOS MED, V16, DOI 10.1371/journal.pmed.1002730; Li RQ, 2022, BIOINFORMATICS, V38, P2587, DOI 10.1093/bioinformatics/btac113; Li WY, 2012, BIOINFORMATICS, V28, P2458, DOI 10.1093/bioinformatics/bts476; Lightbody G, 2019, BRIEF BIOINFORM, V20, P1795, DOI 10.1093/bib/bby051; Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8; Marusyk A, 2012, NAT REV CANCER, V12, P323, DOI 10.1038/nrc3261; Mayr A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0084483; Mermel CH, 2011, GENOME BIOL, V12, DOI 10.1186/gb-2011-12-4-r41; Mobadersany P, 2018, P NATL ACAD SCI USA, V115, pE2970, DOI 10.1073/pnas.1717139115; Renne SL, 2020, HEPATOLOGY, V71, P183, DOI 10.1002/hep.30814; Ried T, 2019, MOL ASPECTS MED, V69, P48, DOI 10.1016/j.mam.2019.07.007; Robinson MD, 2010, BIOINFORMATICS, V26, P139, DOI 10.1093/bioinformatics/btp616; Sari CT, 2019, IEEE T MED IMAGING, V38, P1139, DOI 10.1109/TMI.2018.2879369; Schulz S, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.788740; Shao W, 2020, IEEE T MED IMAGING, V39, P99, DOI 10.1109/TMI.2019.2920608; Shi JY, 2021, GUT, V70, P951, DOI 10.1136/gutjnl-2020-320930; Smyth GK, 2005, STAT BIOL HEALTH, P397, DOI 10.1007/0-387-29362-0_23; Sun DD, 2019, IEEE ACM T COMPUT BI, V16, P841, DOI 10.1109/TCBB.2018.2806438; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Vaswani A, 2017, ADV NEUR IN, V30; Wang SD, 2020, CANCER RES, V80, P2056, DOI 10.1158/0008-5472.CAN-19-1629; Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339; Wang ZQ, 2021, BIOINFORMATICS, V37, P2963, DOI 10.1093/bioinformatics/btab185; Wang Z, 2009, NAT REV GENET, V10, P57, DOI 10.1038/nrg2484; Weng WL, 2020, NEUROCOMPUTING, V378, P375, DOI 10.1016/j.neucom.2019.10.014; Yang H, 2020, IEEE T MED IMAGING, V39, P1306, DOI 10.1109/TMI.2019.2948026; Yang Y, 2013, STAT INTERFACE, V6, P167; Yang Z, 2016, BIOINFORMATICS, V32, P1, DOI 10.1093/bioinformatics/btv544; Zhilong Lv, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P491, DOI 10.1109/BIBM52615.2021.9669445; Zhou R, 2020, INT J CLIN ONCOL, V25, P1822, DOI 10.1007/s10147-020-01730-w; Zhou T, 2019, HUM BRAIN MAPP, V40, P1001, DOI 10.1002/hbm.24428	48	9	10	5	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	1545-5963	1557-9964		IEEE ACM T COMPUT BI	IEEE-ACM Trans. Comput. Biol. Bioinform.	NOV	2023	20	6					3411	3420		10.1109/TCBB.2022.3199244	http://dx.doi.org/10.1109/TCBB.2022.3199244			10	Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Mathematics, Interdisciplinary Applications; Statistics & Probability	Science Citation Index Expanded (SCI-EXPANDED)	Biochemistry & Molecular Biology; Computer Science; Mathematics	DQ5P6	35976825				2024-09-18	WOS:001133540000008
J	Wagner, SJ; Reisenbüchler, D; West, NP; Niehues, JM; Zhu, JF; Foersch, S; Veldhuizen, GP; Quirke, P; Grabsch, HI; van den Brandt, PA; Hutchins, GGA; Richman, SD; Yuan, T; Langer, R; Jenniskens, JCA; Offermans, K; Mueller, W; Gray, R; Gruber, SB; Greenson, JK; Rennert, G; Bonner, JD; Schmolze, D; Jonnagaddala, J; Hawkins, NJ; Ward, RL; Morton, D; Seymour, M; Magill, L; Nowak, M; Hay, J; Koelzer, VH; Church, DN; Matek, C; Geppert, C; Peng, CL; Zhi, C; Ouyang, XM; James, JA; Loughrey, MB; Salto-Tellez, M; Brenner, H; Hoffmeister, M; Truhn, D; Schnabel, JA; Boxberg, M; Peng, TY; Kather, JN				Wagner, Sophia J.; Reisenbuechler, Daniel; West, Nicholas P.; Niehues, Jan Moritz; Zhu, Jiefu; Foersch, Sebastian; Veldhuizen, Gregory Patrick; Quirke, Philip; Grabsch, Heike I.; van den Brandt, Piet A.; Hutchins, Gordon G. A.; Richman, Susan D.; Yuan, Tanwei; Langer, Rupert; Jenniskens, Josien C. A.; Offermans, Kelly; Mueller, Wolfram; Gray, Richard; Gruber, Stephen B.; Greenson, Joel K.; Rennert, Gad; Bonner, Joseph D.; Schmolze, Daniel; Jonnagaddala, Jitendra; Hawkins, Nicholas J.; Ward, Robyn L.; Morton, Dion; Seymour, Matthew; Magill, Laura; Nowak, Marta; Hay, Jennifer; Koelzer, Viktor H.; Church, David N.; Matek, Christian; Geppert, Carol; Peng, Chaolong; Zhi, Cheng; Ouyang, Xiaoming; James, Jacqueline A.; Loughrey, Maurice B.; Salto-Tellez, Manuel; Brenner, Hermann; Hoffmeister, Michael; Truhn, Daniel; Schnabel, Julia A.; Boxberg, Melanie; Peng, Tingying; Kather, Jakob Nikolas		TransSCOT Consortium	Transformer-based biomarker prediction from colorectal cancer histology: A large-scale multicentric study	CANCER CELL			English	Article							COLON-CANCER; MICROSATELLITE INSTABILITY; SURVIVAL; DECISION; MODEL	Deep learning (DL) can accelerate the prediction of prognostic biomarkers from routine pathology slides in colorectal cancer (CRC). However, current approaches rely on convolutional neural networks (CNNs) and have mostly been validated on small patient cohorts. Here, we develop a new transformer-based pipeline for end-to-end biomarker prediction from pathology slides by combining a pre-trained transformer encoder with a transformer network for patch aggregation. Our transformer-based approach substantially improves the performance, generalizability, data efficiency, and interpretability as compared with current state-of-theart algorithms. After training and evaluating on a large multicenter cohort of over 13,000 patients from 16 colorectal cancer cohorts, we achieve a sensitivity of 0.99 with a negative predictive value of over 0.99 for prediction of microsatellite instability (MSI) on surgical resection specimens. We demonstrate that resection specimen-only training reaches clinical-grade performance on endoscopic biopsy tissue, solving a longstanding diagnostic problem.	[Wagner, Sophia J.; Reisenbuechler, Daniel; Matek, Christian; Schnabel, Julia A.; Peng, Tingying] Helmholtz Munich German Res Ctr Environm & Hlth, Munich, Germany; [Wagner, Sophia J.; Schnabel, Julia A.] Tech Univ Munich, Sch Computat Informat & Technol, Munich, Germany; [Wagner, Sophia J.; Niehues, Jan Moritz; Zhu, Jiefu; Veldhuizen, Gregory Patrick; Kather, Jakob Nikolas] Tech Univ Dresden, Else Kroener Fresenius Ctr Digital Hlth EFFZ, Dresden, Germany; [West, Nicholas P.; Foersch, Sebastian] Univ Med Ctr Mainz, Inst Pathol, Mainz, Germany; [Quirke, Philip; Grabsch, Heike I.; Hutchins, Gordon G. A.; Richman, Susan D.; Kather, Jakob Nikolas] Univ Leeds, Leeds Inst Med Res St Jamess, Div Pathol & Data Analyt, Leeds, W Yorkshire, England; [Grabsch, Heike I.] Maastricht Univ Med Ctr, GROW Sch Oncol & Dev Biol, Dept Pathol, Maastricht, Netherlands; [van den Brandt, Piet A.; Jenniskens, Josien C. A.; Offermans, Kelly] Maastricht Univ Med Ctr, Dept Epidemiol, Maastricht, Netherlands; [Yuan, Tanwei; Brenner, Hermann; Hoffmeister, Michael] German Canc Res Ctr, Div Clin Epidemiol & Aging Res, Heidelberg, Germany; [Langer, Rupert] Johannes Kepler Univ Hosp Linz, Inst Pathol & Mol Pathol, Linz, Austria; [Mueller, Wolfram] Gemeinschaftspraxis Pathol, Starnberg, Germany; [Gray, Richard] Univ Oxford, Nuffield Dept Populat Hlth, Oxford, England; [Gruber, Stephen B.; Schmolze, Daniel] City Hope Natl Med Ctr, Ctr Precis Med, Duarte, CA USA; [Gruber, Stephen B.; Schmolze, Daniel] City Hope Natl Med Ctr, Dept Med Oncol, Duarte, CA USA; [Greenson, Joel K.] City Hope Comprehens Canc Ctr, Dept Pathol, Duarte, CA USA; [Rennert, Gad; Bonner, Joseph D.] Technion Israel Inst Technol, Ruth & Bruce Rappaport Fac Med, Lady Davis Carmel Med Ctr, Dept Community Med & Epidemiol, Haifa, Israel; [Rennert, Gad] Clalit Natl Canc Control Ctr, Lady Davis Carmel Med Ctr, Steve & Cindy Rasmussen Inst Genom Med, Haifa, Israel; [Rennert, Gad] Clalit Natl Canc Control Ctr, Technion Fac Med, Haifa, Israel; [Jonnagaddala, Jitendra] UNSW Sydney, Fac Med & Hlth, Sch Populat Hlth, Sydney, NSW, Australia; [Hawkins, Nicholas J.; Ward, Robyn L.] UNSW Sydney, Fac Med & Hlth, Sch Med Sci, Sydney, NSW, Australia; [Ward, Robyn L.] Univ Sydney, Fac Med & Hlth, Sydney, NSW, Australia; [Morton, Dion] Univ Hosp Birmingham, Birmingham, W Midlands, England; [Seymour, Matthew] St James Univ Hosp, Leeds, W Yorkshire, England; [Magill, Laura] Univ Birmingham Clin Trials Unit, Birmingham, W Midlands, England; [Nowak, Marta; Koelzer, Viktor H.] Univ Zurich, Univ Zurich Hosp, Dept Pathol & Mol Pathol, Zurich, Switzerland; [Hay, Jennifer] Univ Glasgow, Queen Elizabeth Univ Hosp, Glasgow Tissue Res Facil, Glasgow, Lanark, Scotland; [Koelzer, Viktor H.] Univ Oxford, Dept Oncol, Oxford, England; [Koelzer, Viktor H.; Church, David N.] Univ Oxford, Dept Med, Roosevelt Dr, Oxford, England; [Church, David N.] Oxford Univ Hosp NHS Fdn Trust, Oxford NIHR Comprehens Biomed Res Ctr, Oxford, England; [Matek, Christian; Geppert, Carol] FAU Erlangen Nuremberg, Univ Hosp Erlangen, Inst Pathol, Erlangen, Germany; [Matek, Christian; Geppert, Carol] FAU Erlangen Nuremberg, Univ Hosp Erlangen, Comprehens Canc Ctr Erlangen EMN CCC, Erlangen, Germany; [Peng, Chaolong] Jianggang Shan Univ, Med Sch, Jian, Jiangxi, Peoples R China; [Zhi, Cheng; Ouyang, Xiaoming] Guangzhou Med Univ, Affiliated Hosp 2, Depat Pathol, Guangzhou, Peoples R China; [James, Jacqueline A.; Salto-Tellez, Manuel] Queens Univ Belfast, Patrick G Johnston Ctr Canc Res, Precis Med Ctr Excellence, Hlth Sci Bldg, Belfast, Antrim, North Ireland; [Ouyang, Xiaoming; Loughrey, Maurice B.] Belfast Hlth & Social Care Trust, Reg Mol Diagnost Serv, Belfast, Antrim, North Ireland; [James, Jacqueline A.; Loughrey, Maurice B.] Queens Univ Belfast, Patrick G Johnston Ctr Canc Res, Belfast, Antrim, North Ireland; [Loughrey, Maurice B.] Belfast Hlth & Social Care Trust, Dept Cellular Pathol, Belfast, Antrim, North Ireland; [Loughrey, Maurice B.] Queens Univ Belfast, Ctr Publ Hlth, Belfast, Antrim, North Ireland; [Salto-Tellez, Manuel] Inst Canc Res, Integrated Pathol Unit, London, England; [Salto-Tellez, Manuel] Royal Marsden Hosp, London, England; [Brenner, Hermann] German Canc Res Ctr, Div Prevent Oncol, Heidelberg, Germany; [Brenner, Hermann] Natl Ctr Tumor Dis NCT, Heidelberg, Germany; [Brenner, Hermann] German Canc Res Ctr, German Canc Consortium DKTK, Heidelberg, Germany; [Truhn, Daniel] Univ Hosp RWTH Aachen, Dept Diagnost & Intervent Radiol, Aachen, Germany; [Schnabel, Julia A.] Kings Coll London, Sch Biomed Engn & Imaging Sci, London, England; [Boxberg, Melanie] Tech Univ Munich, Inst Pathol, Munich, Germany; [Boxberg, Melanie] Inst Pathol Munich North, Munich, Germany; [Kather, Jakob Nikolas] Univ Heidelberg Hosp, Natl Ctr Tumor Dis NCT, Med Oncol, Heidelberg, Germany	Technical University of Munich; Technische Universitat Dresden; Johannes Gutenberg University of Mainz; University of Leeds; Maastricht University; Maastricht University Medical Centre (MUMC); Maastricht University; Maastricht University Medical Centre (MUMC); Helmholtz Association; German Cancer Research Center (DKFZ); University of Oxford; City of Hope; City of Hope; City of Hope; Technion Israel Institute of Technology; Rappaport Faculty of Medicine; Clalit Health Services; Carmel Medical Center; Clalit Health Services; Carmel Medical Center; Technion Israel Institute of Technology; Rappaport Faculty of Medicine; University of New South Wales Sydney; University of New South Wales Sydney; University of Sydney; University of Birmingham; Saint James's University Hospital; University of Zurich; University Zurich Hospital; Queen Elizabeth University Hospital (QEUH); University of Glasgow; University of Oxford; University of Oxford; Oxford University Hospitals NHS Foundation Trust; University of Erlangen Nuremberg; University of Erlangen Nuremberg; Guangzhou Medical University; Queens University Belfast; Queens University Belfast; Queens University Belfast; Royal Marsden NHS Foundation Trust; University of London; Institute of Cancer Research - UK; Royal Marsden NHS Foundation Trust; Helmholtz Association; German Cancer Research Center (DKFZ); Helmholtz Association; German Cancer Research Center (DKFZ); Ruprecht Karls University Heidelberg; Helmholtz Association; German Cancer Research Center (DKFZ); RWTH Aachen University; RWTH Aachen University Hospital; University of London; King's College London; Technical University of Munich; Helmholtz Association; German Cancer Research Center (DKFZ); Ruprecht Karls University Heidelberg	Peng, TY (corresponding author), Helmholtz Munich German Res Ctr Environm & Hlth, Munich, Germany.; Kather, JN (corresponding author), Tech Univ Dresden, Else Kroener Fresenius Ctr Digital Hlth EFFZ, Dresden, Germany.; Kather, JN (corresponding author), Univ Leeds, Leeds Inst Med Res St Jamess, Div Pathol & Data Analyt, Leeds, W Yorkshire, England.; Kather, JN (corresponding author), Univ Heidelberg Hosp, Natl Ctr Tumor Dis NCT, Med Oncol, Heidelberg, Germany.	tingying.peng@helmholtz-munich.de; jakob_nikolas.kather@tu-dresden.de	Truhn, Daniel/AAL-1950-2021; Koelzer, Viktor/I-6915-2013; Wagner, Sophia/HII-6467-2022; Schnabel, Julia/ACE-6863-2022; Bonner, Joe/AAN-6272-2021; Langer, Rupert/AAU-1174-2021; Hoffmeister, Michael/T-7187-2019; Jonnagaddala, Jitendra/F-7372-2015; Brenner, Hermann/ABE-6383-2020; Kather, Jakob Nikolas/D-4279-2015	Richman, Susan/0000-0003-3993-5041; Salto-Tellez, Manuel/0000-0001-8586-282X; Quirke, Philip/0000-0002-3597-5444; Kather, Jakob Nikolas/0000-0002-3730-5348; van den Brandt, Piet/0000-0001-8781-8099; Magill, Laura/0000-0003-2498-8407; Veldhuizen, Gregory/0000-0001-7364-8498; Langer, Rupert/0000-0001-9491-3609; Hoffmeister, Michael/0000-0002-8307-3197; Seymour, Matthew/0000-0002-2441-9629	Helmholtz Association under the joint research school "Munich School for Data Science-MUDS"; Add-on Fellowship of the Joachim Herz Foundation; German Federal Ministry of Health [ZMVI1-2520DAT111]; Max-Eder-Programme of the German Cancer Aid [70113864]; German Federal Ministry of Education and Research [14/140/84, C6716/A9894, C6716/A13941, C26642/A27963, A25142]; German Academic Exchange Service [01KD2215A]; German Federal Joint Committee [031L0312A]; European Union [01KT2302, 01KD2104C]; Yorkshire Cancer Research [F-87701-41-01]; National Institute for Health and Care Research (NIHR) Leeds Biomedical Research Center; Cancer Research UK; Birmingham and Leeds ECMC network; NIHR Senior Investigator award; RCS Eng and Rosetrees Trust; Swedish Cancer Society; Yorkshire Cancer Research; Medical Research Council; BBMRI-NL; Dutch government (NWO); Dutch Cancer Society; German Research Council; Interdisciplinary Research Program of the National Center for Tumor Diseases (NCT; Germany); Pearl consortium from the German Federal Ministry of Education and Research; Deutsche Forschungsgemeinschaft (DFG); Mainz Research School of Translational Biomedicine (TransMed); Manfred-Stolte-Foundation; Interdisciplinary Center for Clinical Research (IZKF) at the University Hospital of the University of Erlangen-Nuremberg; NIH; Medical Research Council; NIHR Health Technology Assessment; Cancer Research UK Core CTU Glasgow Funding; Cancer Research UK Clinical Trials Awards; Advisory Committee - Sample Collection; Oxford NIHR Comprehensive Biomedical Research Centre (BRC); Cancer Research UK (CRUK) Advanced Clinician Scientist Fellowship; CRUK award; Preeclampsia Foundation;  [57616814];  [01VSF21048];  [101057091];  [101096312];  [01EO2101];  [01KH0404];  [01ER0814];  [01ER0815];  [01ER1505A];  [01ER1505B];  [L386];  [L394];  [C551/A8283];  [184.021.007];  [KWF 11044];  [BR 1704/6-1];  [BR 1704/6-3];  [BR 1704/6-4];  [CH 117/1-1];  [HO 5117/2-1];  [HO 5117/2-2];  [HE 5998/2-1];  [HE 5998/2-2];  [KL 2354/3-1];  [KL 2354/3-2];  [RO 2270/8-1];  [RO 2270/8-2];  [BR 1704/17-1];  [BR 1704/17-2];  [01KD2104A];  [FO 942/2-1];  [J101];  [R01 CA263318];  [G0601705]	Helmholtz Association under the joint research school "Munich School for Data Science-MUDS"; Add-on Fellowship of the Joachim Herz Foundation; German Federal Ministry of Health; Max-Eder-Programme of the German Cancer Aid; German Federal Ministry of Education and Research(Federal Ministry of Education & Research (BMBF)); German Academic Exchange Service(Deutscher Akademischer Austausch Dienst (DAAD)); German Federal Joint Committee; European Union(European Union (EU)); Yorkshire Cancer Research; National Institute for Health and Care Research (NIHR) Leeds Biomedical Research Center; Cancer Research UK(Cancer Research UK); Birmingham and Leeds ECMC network; NIHR Senior Investigator award; RCS Eng and Rosetrees Trust; Swedish Cancer Society(Swedish Cancer Society); Yorkshire Cancer Research; Medical Research Council(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); BBMRI-NL; Dutch government (NWO); Dutch Cancer Society(KWF Kankerbestrijding); German Research Council(German Research Foundation (DFG)); Interdisciplinary Research Program of the National Center for Tumor Diseases (NCT; Germany); Pearl consortium from the German Federal Ministry of Education and Research; Deutsche Forschungsgemeinschaft (DFG)(German Research Foundation (DFG)); Mainz Research School of Translational Biomedicine (TransMed); Manfred-Stolte-Foundation; Interdisciplinary Center for Clinical Research (IZKF) at the University Hospital of the University of Erlangen-Nuremberg; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Medical Research Council(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); NIHR Health Technology Assessment; Cancer Research UK Core CTU Glasgow Funding; Cancer Research UK Clinical Trials Awards; Advisory Committee - Sample Collection; Oxford NIHR Comprehensive Biomedical Research Centre (BRC); Cancer Research UK (CRUK) Advanced Clinician Scientist Fellowship; CRUK award; Preeclampsia Foundation; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; 	SJW and DR are supported by the Helmholtz Association under the joint research school "Munich School for Data Science-MUDS" and SJW is supported by the Add-on Fellowship of the Joachim Herz Foundation. JNK is supported by the German Federal Ministry of Health (DEEP LIVER, ZMVI1-2520DAT111) , the Max-Eder-Programme of the German Cancer Aid (grant #70113864) , the German Federal Ministry of Education and Research (CAMINO, 01EO2101; SWAG, 01KD2215A; TRANSFORM LIVER, 031L0312A; TANGERINE, 01KT2302 through ERA-NET Transcan) , the German Academic Exchange Service (SECAI, 57616814) , the German Federal Joint Committee (Transplant.KI, 01VSF21048) and the European Union's Horizon Europe and innovation program (ODELIA, 101057091; GENIAL, 101096312) . JNK and MH are funded by the German Federal Ministry of Education and Research (PEARL, 01KD2104C) . PQ, NW, SD, and GH are supported by Yorkshire Cancer Research grants L386 and L394. PQ, HG, NW, JNK, and SD are supported in part by the National Institute for Health and Care Research (NIHR) Leeds Biomedical Research Center. The views expressed are those of the author (s) and not necessarily those of the NHS, the NIHR, or the Department of Health and Social Care. PQ is also supported by an NIHR Senior Investigator award. FOxTROT was funded by Cancer Research UK (grant reference: C551/A8283; recipient: D.M.) . Additional support was provided by the Birmingham and Leeds ECMC network, the RCS Eng and Rosetrees Trust, and the Swedish Cancer Society. Panitumumab was provided free of charge by Amgen, who also supported RAS testing and additional CT scans (recipient: D.M.) . P.Q., N.W., and M.S. are supported by Yorkshire Cancer Research, R.G. by the Medical Research Council. Tumor tissue collection in the NLCS was done in the Rainbow-TMA study, which was financially supported by BBMRI-NL, a Research Infrastructure financed by the Dutch government (NWO 184.021.007 to PvdB) . The analyses of MSI, BRAF, and KRAS in the NLCS were funded by The Dutch Cancer Society (KWF 11044 to PvdB) . The DACHS study (HB, JCC, and MH) was supported by the German Research Council (BR 1704/6-1, BR 1704/6-3, BR 1704/6-4, CH 117/1-1, HO 5117/2-1, HO 5117/2-2, HE 5998/2-1, HE 5998/2-2, KL 2354/3-1, KL 2354/3-2, RO 2270/8-1, RO 2270/8-2, BR 1704/17-1 and BR 1704/17-2) , the Interdisciplinary Research Program of the National Center for Tumor Diseases (NCT; Germany) and the German Federal Ministry of Education and Research (01KH0404, 01ER0814, 01ER0815, 01ER1505A and 01ER1505B) . The study was further supported by project funding for the Pearl consortium from the German Federal Ministry of Education and Research (01KD2104A) . SF is supported by the Deutsche Forschungsgemeinschaft (DFG) (FO 942/2-1) , the German Federal Ministry of Education and Research (SWAG, 01KD2215A) , the Mainz Research School of Translational Biomedicine (TransMed) and the Manfred-Stolte-Foundation. CM is supported by the Interdisciplinary Center for Clinical Research (IZKF) at the University Hospital of the University of Erlangen-Nuremberg (Junior Project J101) . This work was supported in part by NIH R01 CA263318 (SG) . DACHS study: The authors thank the hospitals recruiting patients for the DACHS study and the cooperating pathology institutes. We thank the National Center for Tumor Diseases (NCT) Tissue Bank, Heidelberg, Germany, for managing, archiving, and processing tissue samples in the DACHS study. The SCOT trial was funded by the Medical Research Council (transferred to NETSCC-Efficacy and Mechanism Evaluation) (Grant Ref: G0601705) , NIHR Health Technology Assessment (Grant ref. 14/140/84) , Cancer Research UK Core CTU Glasgow Funding (Funding Ref: C6716/A9894) , and the Swedish Cancer Society. The TransSCOT sample collection was funded by a Cancer Research UK Clinical Trials Awards and Advisory Committee - Sample Collection (Grant Ref: C6716/A13941) . Molecular analysis of the SCOT samples were funded by the Oxford NIHR Comprehensive Biomedical Research Centre (BRC) , a Cancer Research UK (CRUK) Advanced Clinician Scientist Fellowship (C26642/A27963) to DNC, CRUK award A25142 to the CRUK Glasgow Center. V.H.K. acknowledges funding by the Preeclampsia Foundation (F-87701-41-01) . The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, the Department of Health. We furthermore acknowledge all collaborators in the YCR-BCIP and FOxTROT studies, as well as all other collaborators for all other cohortsincluded in this study.	Abnar S., 2020, PREPRINT; [Anonymous], 2015, MCO Study Whole Slide Image Collection; Benson AB III, 2018, J NATL COMPR CANC NE, V16, P359, DOI 10.6004/jnccn.2018.0021; Bera K, 2019, NAT REV CLIN ONCOL, V16, P703, DOI 10.1038/s41571-019-0252-y; Bilal M., 2021, Novel Deep Learning Algorithm Predicts the Status of Molecular Pathways and Key Mutations in Colorectal Cancer from Routine Histology Images, DOI [10.1101/2021.01.19.21250122, DOI 10.1101/2021.01.19.21250122]; Bilal M., 2022, PREPRINT; Bilal M., 2021, The Lancet Digital; Brenner H, 2011, J CLIN ONCOL, V29, P3761, DOI 10.1200/JCO.2011.35.9307; Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Cao R, 2020, THERANOSTICS, V10, P11080, DOI 10.7150/thno.49864; Cercek A, 2022, NEW ENGL J MED, V386, P2363, DOI 10.1056/NEJMoa2201445; Chalabi M, 2022, ANN ONCOL, V33, pS1389, DOI 10.1016/j.annonc.2022.08.016; Chalabi M, 2020, NAT MED, V26, P566, DOI 10.1038/s41591-020-0805-8; Chen RJ, 2022, PROC CVPR IEEE, P16123, DOI 10.1109/CVPR52688.2022.01567; Coudray N, 2018, NAT MED, V24, P1559, DOI 10.1038/s41591-018-0177-5; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Echle A, 2020, GASTROENTEROLOGY, V159, P1406, DOI 10.1053/j.gastro.2020.06.021; Edwards NJ, 2015, J PROTEOME RES, V14, P2707, DOI 10.1021/pr501254j; Fu Y, 2020, NAT CANCER, V1, P800, DOI 10.1038/s43018-020-0085-8; Laleh NG, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-33266-0; Grabsch H, 2006, CLIN CANCER RES, V12, P1494, DOI 10.1158/1078-0432.CCR-05-2105; Gray R, 2007, LANCET, V370, P2020, DOI 10.1016/s0140-6736(07)61866-2; Gray RT, 2017, BRIT J CANCER, V116, P1652, DOI 10.1038/bjc.2017.139; Gray RT, 2017, CLIN TRANSL GASTROEN, V8, DOI 10.1038/ctg.2017.18; Hawkins N., 2011, MCO study tumour collection; Hendricks A, 2020, CANCERS, V12, DOI 10.3390/cancers12020393; Hoffmeister M, 2015, JNCI-J NATL CANCER I, V107, DOI 10.1093/jnci/djv045; Ilse M, 2018, PR MACH LEARN RES, V80; Isella Claudio, 2014, TCGA CRC 450 dataset; Iveson TJ, 2018, LANCET ONCOL, V19, P562, DOI 10.1016/S1470-2045(18)30093-7; Jang HJ, 2020, WORLD J GASTROENTERO, V26, P6207, DOI 10.3748/wjg.v26.i40.6207; Jonnagaddala J, 2016, STUD HEALTH TECHNOL, V225, P387, DOI 10.3233/978-1-61499-658-3-387; Kather JN, 2020, NAT CANCER, V1, P789, DOI 10.1038/s43018-020-0087-6; Kather JN, 2019, NAT MED, V25, P1054, DOI 10.1038/s41591-019-0462-y; Kim YJ, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101854; Kingma D. P., 2017, arXiv; Kleppe A, 2022, LANCET ONCOL, V23, P1221, DOI 10.1016/S1470-2045(22)00391-6; Laleh NG, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102474; Lee SH, 2021, INT J CANCER, V149, P728, DOI 10.1002/ijc.33599; Lim C, 2015, ANN ONCOL, V26, P1415, DOI 10.1093/annonc/mdv208; Liu Y, 2018, CANCER CELL, V33, P721, DOI 10.1016/j.ccell.2018.03.010; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Loshchilov I., 2017, PREPRINT; Morton D, 2023, J CLIN ONCOL, V41, P1541, DOI 10.1200/JCO.22.00046; Muzny DM, 2012, NATURE, V487, P330, DOI 10.1038/nature11252; National Institute for Health and Care Excellence, 2020, Colorectal cancer [NICE Guideline No. 151]; National Institute for Health and Care Excellence (NICE), 2020, Molecular Testing Strategies for Lynch Syndrome in People with Colorectal Cancer (NICE Guidance); Niehues JM, 2023, CELL REP MED, V4, DOI 10.1016/j.xcrm.2023.100980; Offermans K, 2022, J PATHOL CLIN RES, V8, P169, DOI 10.1002/cjp2.250; Quirke P, 2007, HISTOPATHOLOGY, V50, P103, DOI 10.1111/j.1365-2559.2006.02543.x; Reisenbuchler D., 2022, PREPRINT; Saillard C., 2022, PREPRINT; Saldanha OL, 2023, NPJ PRECIS ONCOL, V7, DOI 10.1038/s41698-023-00365-0; Schirris Y, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102464; Schmoll HJ, 2012, ANN ONCOL, V23, P2479, DOI 10.1093/annonc/mds236; Schrammen PL, 2022, J PATHOL, V256, P50, DOI 10.1002/path.5800; Shao ZC, 2021, ADV NEUR IN; Shmatko A, 2022, NAT CANCER, V3, P1026, DOI 10.1038/s43018-022-00436-4; Shulman K, 2018, JCO PRECIS ONCOL, V2, P1, DOI 10.1200/PO.17.00253; Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589; Svrcek M, 2022, ANN ONCOL, V33, pS967, DOI 10.1016/j.annonc.2022.07.1045; Taylor J, 2019, BMJ OPEN, V9, DOI 10.1136/bmjopen-2019-030618; Vacante M, 2018, WORLD J CLIN CASES, V6, P869, DOI 10.12998/wjcc.v6.i15.869; VANDENBRANDT PA, 1990, J CLIN EPIDEMIOL, V43, P285, DOI 10.1016/0895-4356(90)90009-E; Vasaikar S, 2019, CELL, V177, P1035, DOI 10.1016/j.cell.2019.03.030; Wagner SJ, 2021, LECT NOTES COMPUT SC, V12908, P257, DOI 10.1007/978-3-030-87237-3_25; Wang XY, 2022, MED IMAGE ANAL, V81, DOI 10.1016/j.media.2022.102559; Ward R., Molecular and Cellular Oncology (MCO) Study Data; West NP, 2021, HISTOPATHOLOGY, V79, P690, DOI 10.1111/his.14390; Yamashita R, 2021, LANCET ONCOL, V22, P132, DOI 10.1016/S1470-2045(20)30535-0; Yin Z., 2022, PREPRINT	72	26	29	9	39	CELL PRESS	CAMBRIDGE	50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA	1535-6108	1878-3686		CANCER CELL	Cancer Cell	SEP 11	2023	41	9					1650	+		10.1016/j.ccell.2023.08.002	http://dx.doi.org/10.1016/j.ccell.2023.08.002		SEP 2023	17	Oncology; Cell Biology	Science Citation Index Expanded (SCI-EXPANDED)	Oncology; Cell Biology	U4GA1	37652006	Green Published, hybrid			2024-09-18	WOS:001084385700001
J	Ayana, G; Barki, H; Choe, SW				Ayana, Gelan; Barki, Hika; Choe, Se-woon			Pathological Insights: Enhanced Vision Transformers for the Early Detection of Colorectal Cancer	CANCERS			English	Article						vision transformer; spatial transformer; colorectal cancer; pathological findings; early detection; endoscopy	BARRETTS-ESOPHAGUS; CLASSIFICATION; STATISTICS; NETWORK; RISK	Simple Summary Accounting for 10% of the new cases in 2020, colorectal cancer (CRC) is one of the most prevalent cancers worldwide. Unfortunately, CRC is frequently identified at a later stage, despite the fact that early detection greatly increases survival rates. Diagnostic endoscopy is the gold standard; however, identifying abnormalities at an early stage is challenging. In particular, convolutional neural networks (CNNs) are being used by researchers to improve detection through deep learning. But prior approaches were primarily concerned with polyp detection. This work provided a novel method for polyp segmentation and endoscopic pathological finding categorization using vision transformers and spatial transformers for the early identification of colorectal cancer (CRC). These approaches perform noticeably better than the current CNN-based algorithms. This work opens up exciting possibilities for improving on early CRC detection beyond just identifying polyps.Abstract Endoscopic pathological findings of the gastrointestinal tract are crucial for the early diagnosis of colorectal cancer (CRC). Previous deep learning works, aimed at improving CRC detection performance and reducing subjective analysis errors, are limited to polyp segmentation. Pathological findings were not considered and only convolutional neural networks (CNNs), which are not able to handle global image feature information, were utilized. This work introduces a novel vision transformer (ViT)-based approach for early CRC detection. The core components of the proposed approach are ViTCol, a boosted vision transformer for classifying endoscopic pathological findings, and PUTS, a vision transformer-based model for polyp segmentation. Results demonstrate the superiority of this vision transformer-based CRC detection method over existing CNN and vision transformer models. ViTCol exhibited an outstanding performance in classifying pathological findings, with an area under the receiver operating curve (AUC) value of 0.9999 +/- 0.001 on the Kvasir dataset. PUTS provided outstanding results in segmenting polyp images, with mean intersection over union (mIoU) of 0.8673 and 0.9092 on the Kvasir-SEG and CVC-Clinic datasets, respectively. This work underscores the value of spatial transformers in localizing input images, which can seamlessly integrate into the main vision transformer network, enhancing the automated identification of critical image features for early CRC detection.	[Ayana, Gelan; Choe, Se-woon] Kumoh Natl Inst Technol, Dept Med IT Convergence Engn, Gumi 39253, South Korea; [Ayana, Gelan] Jimma Univ, Sch Biomed Engn, Jimma 378, Ethiopia; [Barki, Hika] Pukyong Natl Univ, Dept Artificial Intelligence Convergence, Busan 48513, South Korea; [Choe, Se-woon] Kumoh Natl Inst Technol, Dept IT Convergence Engn, Gumi 39253, South Korea; [Choe, Se-woon] Univ Florida, Emerging Pathogens Inst, Gainesville, FL 32608 USA	Kumoh National University Technology; Jimma University; Pukyong National University; Kumoh National University Technology; State University System of Florida; University of Florida	Choe, SW (corresponding author), Kumoh Natl Inst Technol, Dept Med IT Convergence Engn, Gumi 39253, South Korea.; Choe, SW (corresponding author), Kumoh Natl Inst Technol, Dept IT Convergence Engn, Gumi 39253, South Korea.; Choe, SW (corresponding author), Univ Florida, Emerging Pathogens Inst, Gainesville, FL 32608 USA.	gelan@kumoh.ac.kr; daljuhika@pukyong.ac.kr; musej2s8@ufl.edu	Ayana, Gelan/GPX-3670-2022	Choe, Se-woon/0000-0002-7298-4147; Ayana, Gelan/0000-0002-5219-6098; Dalju, Hika Barki/0000-0002-7540-6334	National Research Foundation of Korea	National Research Foundation of Korea(National Research Foundation of Korea)	No Statement Available	Alboaneen D, 2023, BIG DATA COGN COMPUT, V7, DOI 10.3390/bdcc7020074; Ali S, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.102002; Ayana G, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13020178; Ayana G, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12112654; Ayana G, 2022, MICROMACHINES-BASEL, V13, DOI 10.3390/mi13091508; Ayana G, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12040862; Ayana G, 2022, CANCERS, V14, DOI 10.3390/cancers14051280; Baeg MK, 2016, GUT LIVER, V10, P76, DOI 10.5009/gnl14381; Cao H., 2021, SWIN UNET UNET LIKE, P1; Dalju HB, 2021, 2021 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE (IEEE BIOCAS 2021), DOI 10.1109/BIOCAS49922.2021.9645002; del Amor R, 2022, COMPUT METH PROG BIO, V224, DOI 10.1016/j.cmpb.2022.107012; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dese K, 2022, HARDWAREX, V11, DOI 10.1016/j.ohx.2022.e00276; Dese K, 2021, CL LYMPH MYELOM LEUK, V21, pE903, DOI 10.1016/j.clml.2021.06.025; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Hao YZ, 2020, GUT LIVER, V14, P399, DOI 10.5009/gnl19097; Helsingen Lise M, 2022, NEJM Evid, V1, pEVIDra2100035, DOI 10.1056/EVIDra2100035; Hu JQ, 2022, COMPUT MED IMAG GRAP, V101, DOI 10.1016/j.compmedimag.2022.102124; Huang C-H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.07172; Huang XD, 2022, COMPUT MED IMAG GRAP, V98, DOI 10.1016/j.compmedimag.2022.102072; Jaderberg M., 2016, P ACM INT C PROCEEDI, V2, P45; Jha D, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.102007; Jung KU, 2022, J ANUS RECTUM COLON, V6, P231, DOI 10.23922/jarc.2022-050; Karthikeyan A., 2024, Meas. Sens, V31, P100976, DOI [10.1016/j.measen.2023.100976, DOI 10.1016/J.MEASEN.2023.100976]; Keshtkar K, 2021, Preprints, DOI [10.20944/preprints202110.0135.v1, 10.20944/preprints202110.0135.v1, DOI 10.20944/PREPRINTS202110.0135.V1]; Khan TM, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.106023; Khil H, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81877-2; Kovári B, 2022, ADV ANAT PATHOL, V29, P2, DOI 10.1097/PAP.0000000000000311; Li WS, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108824; Li YF, 2023, IEEE SENS J, V23, P23629, DOI 10.1109/JSEN.2023.3308394; Liu FJ, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106213; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Logan RFA, 1999, GUT, V44, P775, DOI 10.1136/gut.44.6.775; Lucafò M, 2021, FRONT PHARMACOL, V12, DOI 10.3389/fphar.2021.772101; Marabotto E, 2022, CANCERS, V14, DOI 10.3390/cancers14174254; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Nogueira-Rodríguez A, 2022, NEURAL COMPUT APPL, V34, P10375, DOI 10.1007/s00521-021-06496-4; Pacal I, 2021, COMPUT BIOL MED, V134, DOI 10.1016/j.compbiomed.2021.104519; Pan SM, 2023, BMC BIOINFORMATICS, V24, DOI 10.1186/s12859-023-05196-1; Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018; Patel K., 2022, P 36 C NEURAL INFORM, P1; Patel K, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0236452; Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P164, DOI 10.1145/3083187.3083212; Rawla P, 2019, GASTROENTEROL REV, V14, P89, DOI 10.5114/pg.2018.81072; Roy S, 2020, IEEE T MED IMAGING, V39, P2676, DOI 10.1109/TMI.2020.2994459; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Shahmoradi MK, 2020, ANN MED SURG, V57, P7, DOI 10.1016/j.amsu.2020.07.010; Sharma A, 2023, INT J MED INFORM, V177, DOI 10.1016/j.ijmedinf.2023.105142; Siegel RL, 2022, CA-CANCER J CLIN, V72, P7, DOI 10.3322/caac.21708; Siegel RL, 2020, CA-CANCER J CLIN, V70, P7, DOI [10.3322/caac.21590, 10.3322/caac.21601]; Solaymani-Dodaran M, 2004, SCAND J GASTROENTERO, V39, P680, DOI 10.1080/00365520410004802; Song PF, 2023, ENG APPL ARTIF INTEL, V124, DOI 10.1016/j.engappai.2023.106634; Stoffel EM, 2020, GASTROENTEROLOGY, V158, P341, DOI 10.1053/j.gastro.2019.07.055; Sutton RT, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-06726-2; Nguyen TC, 2021, LECT NOTES COMPUT SC, V12901, P633, DOI 10.1007/978-3-030-87193-2_60; Toyoshima O, 2022, J GASTROEN HEPATOL, V37, P291, DOI 10.1111/jgh.15693; Vaswani A, 2017, ADV NEUR IN, V30; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Xi Y, 2021, TRANSL ONCOL, V14, DOI 10.1016/j.tranon.2021.101174; Xie E., 2021, Advances in Neural Information Processing Systems, V34, P12077, DOI DOI 10.48550/ARXIV.2105.15203; Xie YH, 2020, SIGNAL TRANSDUCT TAR, V5, DOI 10.1038/s41392-020-0116-z; Yang HK, 2022, COMPUT MED IMAG GRAP, V101, DOI 10.1016/j.compmedimag.2022.102110; Younas F, 2023, APPL INTELL, V53, P2410, DOI 10.1007/s10489-022-03689-9; Zhang RK, 2017, IEEE J BIOMED HEALTH, V21, P41, DOI 10.1109/JBHI.2016.2635662	65	0	0	9	9	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		2072-6694		CANCERS	Cancers	APR	2024	16	7							1441	10.3390/cancers16071441	http://dx.doi.org/10.3390/cancers16071441			20	Oncology	Science Citation Index Expanded (SCI-EXPANDED)	Oncology	NN1G0	38611117	gold, Green Published			2024-09-18	WOS:001201032500001
J	Kim, H; Yuh, T; Choi, JE; Baek, E; Kim, HS; Beom, SH; Ahn, JB; Shin, SJ				Kim, Hyunwook; Yuh, Taeho; Choi, Jeong Eun; Baek, Eunsil; Kim, Han Sang; Beom, Seung-Hoon; Ahn, Joong Bae; Shin, Sang Joon			A rapid assessment tool for systemic treatment outcomes in colorectal cancer with deep bidirectional transformers	JOURNAL OF CLINICAL ONCOLOGY			English	Meeting Abstract	Special Clinical Science Symposia	MAY 29-29, 2024	ELECTR NETWORK						Yonsei Univ, Coll Med, Yonsei Canc Ctr, Div Med Oncol,Dept Internal Med, Seoul, South Korea; Yonsei Univ, Coll Med, Seoul, South Korea; Yonsei Univ Hlth Syst, Div Digital Hlth, Off Data Serv, Seoul, South Korea; Yonsei Univ, Coll Med, Songdang Inst Canc Res, Seoul, South Korea	Yonsei University; Yonsei University Health System; Yonsei University; Yonsei University Health System; Yonsei University; Yonsei University Health System; Yonsei University; Yonsei University Health System									0	0	0	0	0	LIPPINCOTT WILLIAMS & WILKINS	PHILADELPHIA	TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA	0732-183X	1527-7755		J CLIN ONCOL	J. Clin. Oncol.	JUN 1	2024	42	16		S		e15567								1	Oncology	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Oncology	ZL8X7					2024-09-18	WOS:001275557404059
J	El Nahhas, OS; Bonner, JD; Greenson, JK; Schmolze, DB; Shaktah, L; Salazar, J; Reynaga, L; Lindsey, S; Lu, J; Moreno, V; Schmit, SL; Tsai, YY; Hamilton, SR; Rennert, G; Kather, JN; Gruber, SB				El Nahhas, Omar S.; Bonner, Joseph D.; Greenson, Joel K.; Schmolze, Daniel B.; Shaktah, Lawrence; Salazar, Jonathan; Reynaga, Lorena; Lindsey, Sidney; Lu, Jenny; Moreno, Victor; Schmit, Stephanie L.; Tsai, Ya-Yu; Hamilton, Stanley R.; Rennert, Gad; Kather, Jakob N.; Gruber, Stephen B.			Weakly-supervised prediction of tumor infiltrating lymphocytes per high power field from colorectal cancer histopathology slides using regression transformers	CANCER RESEARCH			English	Meeting Abstract	Annual Meeting of the American-Association-for-Cancer-Research (AACR)	APR 05-10, 2024	San Diego, CA	Amer Assoc Cancer Res									Moreno, Victor/GNN-0449-2022						0	0	0	0	0	AMER ASSOC CANCER RESEARCH	PHILADELPHIA	615 CHESTNUT ST, 17TH FLOOR, PHILADELPHIA, PA 19106-4404 USA	0008-5472	1538-7445		CANCER RES	Cancer Res.	APR 1	2024	84	7		S		LB386				10.1158/1538-7445.AM2024-LB386	http://dx.doi.org/10.1158/1538-7445.AM2024-LB386			2	Oncology	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Oncology	NV3E4					2024-09-18	WOS:001203184200576
J	Lijin, P; Ullah, M; Vats, A; Cheikh, FA; Kumar, GS; Nair, MS				Lijin, P.; Ullah, Mohib; Vats, Anuja; Cheikh, Faouzi Alaya; Kumar, G. Santhosh; Nair, Madhu S.			PolySegNet: improving polyp segmentation through swin transformer and vision transformer fusion	BIOMEDICAL ENGINEERING LETTERS			English	Article; Early Access						Swin transformer; Vision transformer; Convolutional neural network; Colorectal cancer; Segmentation		Colorectal cancer ranks as the second most prevalent cancer worldwide, with a high mortality rate. Colonoscopy stands as the preferred procedure for diagnosing colorectal cancer. Detecting polyps at an early stage is critical for effective prevention and diagnosis. However, challenges in colonoscopic procedures often lead medical practitioners to seek support from alternative techniques for timely polyp identification. Polyp segmentation emerges as a promising approach to identify polyps in colonoscopy images. In this paper, we propose an advanced method, PolySegNet, that leverages both Vision Transformer and Swin Transformer, coupled with a Convolutional Neural Network (CNN) decoder. The fusion of these models facilitates a comprehensive analysis of various modules in our proposed architecture.To assess the performance of PolySegNet, we evaluate it on three colonoscopy datasets, a combined dataset, and their augmented versions. The experimental results demonstrate that PolySegNet achieves competitive results in terms of polyp segmentation accuracy and efficacy, achieving a mean Dice score of 0.92 and a mean Intersection over Union (IoU) of 0.86. These metrics highlight the superior performance of PolySegNet in accurately delineating polyp boundaries compared to existing methods. PolySegNet has shown great promise in accurately and efficiently segmenting polyps in medical images. The proposed method could be the foundation for a new class of transformer-based segmentation models in medical image analysis.	[Lijin, P.; Kumar, G. Santhosh; Nair, Madhu S.] Cochin Univ Sci & Technol, Dept Comp Sci, Artificial Intelligence & Comp Vis Lab, Kochi 682022, Kerala, India; [Ullah, Mohib; Vats, Anuja] Norwegian Univ Sci & Technol, Teknol Vegen 22, N-2815 Gjovik, Norway; [Cheikh, Faouzi Alaya] Norwegian Univ Sci & Technol, Norwegian Colour & Visual Comp Lab, Teknol Vegen 22, N-2815 Gjovik, Norway	Cochin University Science & Technology; Norwegian University of Science & Technology (NTNU); Norwegian University of Science & Technology (NTNU)	Lijin, P (corresponding author), Cochin Univ Sci & Technol, Dept Comp Sci, Artificial Intelligence & Comp Vis Lab, Kochi 682022, Kerala, India.	lijinp43@gmail.com; mohib.ullah@ntnu.no; anuja.vats@ntnu.no; faouzi.cheikh@ntnu.no; san@cusat.ac.in; madhu_s_nair2001@yahoo.com	Nair, Madhu S./B-7069-2013		Cochin University of Science and Technology (CUSAT) [CUSAT/AC(B).B3/2867/2021]; Research Council of Norway [ES652214/309857]	Cochin University of Science and Technology (CUSAT); Research Council of Norway(Research Council of Norway)	The research work was supported by the Cochin University of Science and Technology (CUSAT) through the JRF fellowship vide File No. CUSAT/AC(B).B3/2867/2021, and the Research Council of Norway under project number: ES652214/309857.	Ali R, 2022, AUTOMAT CONSTR, V141, DOI 10.1016/j.autcon.2022.104412; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chen L.C., 2018, P EUR C COMP VIS ECC, V833, P801, DOI DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2017, Arxiv, DOI arXiv:1706.05587; Choi W, 2020, IEEE T IND ELECTRON, V67, P8016, DOI 10.1109/TIE.2019.2945265; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181; Heidari M, 2023, IEEE WINT CONF APPL, P6191, DOI 10.1109/WACV56688.2023.00614; Hu J., 2018, IEEE C COMP VIS PATT, P7132, DOI DOI 10.1109/CVPR.2018.00745; Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304; Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Kang DH, 2022, STRUCT HEALTH MONIT, V21, P2190, DOI 10.1177/14759217211053776; Lewis J, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-28530-2; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Oktay O, 2018, Arxiv, DOI arXiv:1804.03999; Rahman MM, 2023, IEEE WINT CONF APPL, P6211, DOI 10.1109/WACV56688.2023.00616; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Song PF, 2022, COMPUT BIOL MED, V146, DOI 10.1016/j.compbiomed.2022.105476; Srivastava A, 2022, IEEE J BIOMED HEALTH, V26, P2252, DOI 10.1109/JBHI.2021.3138024; Tan MX, 2019, PR MACH LEARN RES, V97; Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23; Tomar NK, 2022, COMP MED SY, P317, DOI 10.1109/CBMS55023.2022.00063; Tomar NK, 2023, IEEE T NEUR NET LEAR, V34, P9375, DOI 10.1109/TNNLS.2022.3159394; Tragakis A, 2023, IEEE WINT CONF APPL, P3649, DOI 10.1109/WACV56688.2023.00365; Trinh QH, 2023, Arxiv, DOI arXiv:2305.07848; Yu W, 2022, METAFORMER BASELINES; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944; Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	35	0	0	2	2	SPRINGERNATURE	LONDON	CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND	2093-9868	2093-985X		BIOMED ENG LETT	Biomed. Eng. Lett.	2024 AUG 20	2024										10.1007/s13534-024-00415-x	http://dx.doi.org/10.1007/s13534-024-00415-x		AUG 2024	11	Engineering, Biomedical	Science Citation Index Expanded (SCI-EXPANDED)	Engineering	D2B9W					2024-09-18	WOS:001294302000001
J	Gustav, M; van Treeck, M; Carrero, ZI; Loeffler, CM; Reitsam, NG; Märkl, B; Meneghetti, AR; Boardman, LA; French, AJ; Goode, EL; Gsur, A; Brezina, S; Gunter, MJ; Murphy, N; Limburg, P; Thibodeau, S; Foersch, S; Steinfelder, R; Harrison, T; Peters, U; Phipps, A; Kather, JN				Gustav, Marco; van Treeck, Marko; Carrero, Zunamys I.; Loeffler, Chiara M.; Reitsam, Nic G.; Markl, Bruno; Meneghetti, Asier Rabasco; Boardman, Lisa A.; French, Amy J.; Goode, Ellen L.; Gsur, Andrea; Brezina, Stefanie; Gunter, Marc J.; Murphy, Neil; Limburg, Paul; Thibodeau, Stephen; Foersch, Sebastian; Steinfelder, Robert; Harrison, Tabitha; Peters, Ulrike; Phipps, Amanda; Kather, Jakob N.			Assessing microsatellite instability dominance in colorectal cancer phenotype: A multi-study initiative using multi-target transformers for genomic biomarker prediction	CANCER RESEARCH			English	Meeting Abstract	Annual Meeting of the American-Association-for-Cancer-Research (AACR)	APR 05-10, 2024	San Diego, CA	Amer Assoc Cancer Res															0	0	0	0	0	AMER ASSOC CANCER RESEARCH	PHILADELPHIA	615 CHESTNUT ST, 17TH FLOOR, PHILADELPHIA, PA 19106-4404 USA	0008-5472	1538-7445		CANCER RES	Cancer Res.	MAR 15	2023	84	6		S		4924				10.1158/1538-7445.AM2024-4924	http://dx.doi.org/10.1158/1538-7445.AM2024-4924			2	Oncology	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Oncology	WD7Q4					2024-09-18	WOS:001253000902012
J	Cai, HB; Feng, XB; Yin, RM; Zhao, YC; Guo, LC; Fan, XS; Liao, J				Cai, Hongbin; Feng, Xiaobing; Yin, Ruomeng; Zhao, Youcai; Guo, Lingchuan; Fan, Xiangshan; Liao, Jun			MIST: multiple instance learning network based on Swin Transformer for whole slide image classification of colorectal adenomas	JOURNAL OF PATHOLOGY			English	Article						colorectal adenoma; pathology; whole slide image; classification; Swin Transformer	SERRATED PATHWAY; CANCER; POLYPS	Colorectal adenoma is a recognized precancerous lesion of colorectal cancer (CRC), and at least 80% of colorectal cancers are malignantly transformed from it. Therefore, it is essential to distinguish benign from malignant adenomas in the early screening of colorectal cancer. Many deep learning computational pathology studies based on whole slide images (WSIs) have been proposed. Most approaches require manual annotation of lesion regions on WSIs, which is time-consuming and labor-intensive. This study proposes a new approach, MIST - Multiple Instance learning network based on the Swin Transformer, which can accurately classify colorectal adenoma WSIs only with slide-level labels. MIST uses the Swin Transformer as the backbone to extract features of images through self-supervised contrastive learning and uses a dual-stream multiple instance learning network to predict the class of slides. We trained and validated MIST on 666 WSIs collected from 480 colorectal adenoma patients in the Department of Pathology, The Affiliated Drum Tower Hospital of Nanjing University Medical School. These slides contained six common types of colorectal adenomas. The accuracy of external validation on 273 newly collected WSIs from Nanjing First Hospital was 0.784, which was superior to the existing methods and reached a level comparable to that of the local pathologist's accuracy of 0.806. Finally, we analyzed the interpretability of MIST and observed that the lesion areas of interest in MIST were generally consistent with those of interest to local pathologists. In conclusion, MIST is a low-burden, interpretable, and effective approach that can be used in colorectal cancer screening and may lead to a potential reduction in the mortality of CRC patients by assisting clinicians in the decision-making process. (c) 2022 The Pathological Society of Great Britain and Ireland.	[Cai, Hongbin; Yin, Ruomeng; Liao, Jun] China Pharmaceut Univ, Sch Sci, Nanjing, Peoples R China; [Feng, Xiaobing] Hunan Univ, Coll Elect & Informat Engn, Changsha, Peoples R China; [Zhao, Youcai] Nanjing First Hosp, Dept Pathol, Nanjing, Peoples R China; [Guo, Lingchuan] Soochow Univ, Dept Pathol, Affiliated Hosp 1, Suzhou, Peoples R China; [Fan, Xiangshan] Nanjing Univ, Dept Pathol, Affiliated Drum Tower Hosp, Med Sch, Nanjing, Peoples R China; [Liao, Jun] China Pharmaceut Univ, Sch Sci, 639 Longmian Ave, Nanjing 211198, Peoples R China; [Fan, Xiangshan] Nanjing Drum Tower Hosp, Dept Pathol, 321 Zhongshan Rd, Nanjing 210008, Peoples R China	China Pharmaceutical University; Hunan University; Soochow University - China; Nanjing University; China Pharmaceutical University; Nanjing University	Liao, J (corresponding author), China Pharmaceut Univ, Sch Sci, 639 Longmian Ave, Nanjing 211198, Peoples R China.; Fan, XS (corresponding author), Nanjing Drum Tower Hosp, Dept Pathol, 321 Zhongshan Rd, Nanjing 210008, Peoples R China.	fxs23@163.com; liaojun@cpu.edu.cn	Liao, Jun/JUF-2784-2023	fan, xiangshan/0000-0003-4552-5858	High Performance Computing Center, China Pharmaceutical University; National Natural Science Foundation of China [81473274]; Postgraduate Research & Practice Innovation Program of Jiangsu Province [KYCX_19_0667]	High Performance Computing Center, China Pharmaceutical University; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Postgraduate Research & Practice Innovation Program of Jiangsu Province	Tissue samples were kindly provided by the Drum Tower Hospital Affiliated to Nanjing University School of Medicine. This work was supported by the High Performance Computing Center, China Pharmaceutical University, and was financially supported by the National Natural Science Foundation of China (81473274) and the Postgraduate Research & Practice Innovation Program of Jiangsu Province (KYCX_19_0667).	Bejnordi BE, 2017, JAMA-J AM MED ASSOC, V318, P2199, DOI 10.1001/jama.2017.14585; Bera K, 2019, NAT REV CLIN ONCOL, V16, P703, DOI 10.1038/s41571-019-0252-y; Bettington M, 2013, HISTOPATHOLOGY, V62, P367, DOI 10.1111/his.12055; Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1; Cancer Genome Atlas Network, 2012, Nature, V487, P330, DOI 10.1038/nature11252; Chen RJ., PREPRINT; Chen T, 2020, PR MACH LEARN RES, V119; De Palma FDE, 2019, CANCERS, V11, DOI 10.3390/cancers11071017; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Feng RW, 2021, IEEE J BIOMED HEALTH, V25, P3700, DOI 10.1109/JBHI.2020.3040269; Fraggetta Filippo, 2018, J Pathol Inform, V9, P46, DOI 10.4103/jpi.jpi_70_18; Garau N, 2022, PROC CVPR IEEE, P13679, DOI 10.1109/CVPR52688.2022.01332; Hou L, 2016, PROC CVPR IEEE, P2424, DOI 10.1109/CVPR.2016.266; Ito N, 2019, ONCOLOGY-BASEL, V96, P44, DOI 10.1159/000491636; Jass JR, 2007, HISTOPATHOLOGY, V50, P113, DOI 10.1111/j.1365-2559.2006.02549.x; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Kieffer B., 2017, 2017 7 INT C IMAGE P, P1, DOI [10.1109/IPTA.2017.8310149, DOI 10.1109/IPTA.2017.8310149]; Koohbanani NA, 2021, IEEE T MED IMAGING, V40, P2845, DOI 10.1109/TMI.2021.3056023; Korbar Bruno, 2017, J Pathol Inform, V8, P30, DOI 10.4103/jpi.jpi_34_17; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Kumar N, 2020, J DIGIT IMAGING, V33, P1034, DOI 10.1007/s10278-020-00351-z; Lee J, 2019, PR MACH LEARN RES, V97; Leggett B, 2010, GASTROENTEROLOGY, V138, P2088, DOI 10.1053/j.gastro.2009.12.066; Li B, 2021, PROC CVPR IEEE, P14313, DOI 10.1109/CVPR46437.2021.01409; Liu Y, 2019, ARCH PATHOL LAB MED, V143, P859, DOI 10.5858/arpa.2018-0147-OA; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Lu MY, 2021, NAT BIOMED ENG, V5, P555, DOI 10.1038/s41551-020-00682-w; Maron O, 1998, ADV NEUR IN, V10, P570; Mori Y, 2017, ENDOSCOPY, V49, P813, DOI 10.1055/s-0043-109430; Murakami T, 2019, J GASTROEN HEPATOL, V34, P1685, DOI 10.1111/jgh.14752; Nagtegaal ID, 2020, HISTOPATHOLOGY, V76, P182, DOI 10.1111/his.13975; Nguyen LH, 2020, GASTROENTEROLOGY, V158, P291, DOI 10.1053/j.gastro.2019.08.059; Odze RD., 2014, ODZE GOLDBLUM SURG P; Pai RK, 2019, MODERN PATHOL, V32, P1390, DOI 10.1038/s41379-019-0280-2; Pan Bowen, 2021, Advances in Neural Information Processing Systems, V34, P24898; Phipps AI, 2015, GASTROENTEROLOGY, V148, P77, DOI 10.1053/j.gastro.2014.09.038; Raghu M, 2021, ADV NEUR IN, V34; Rymarczyk D, 2021, IEEE WINT CONF APPL, P1720, DOI 10.1109/WACV48630.2021.00176; Satorres C, 2021, GUT LIVER, V15, P31, DOI 10.5009/gnl19402; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]; Shaukat A, 2021, AM J GASTROENTEROL, V116, P458, DOI 10.14309/ajg.0000000000001122; Song ZG, 2020, BMJ OPEN, V10, DOI 10.1136/bmjopen-2019-036423; Sudharshan PJ, 2019, EXPERT SYST APPL, V117, P103, DOI 10.1016/j.eswa.2018.09.049; Tang YC, 2022, PROC CVPR IEEE, P20698, DOI 10.1109/CVPR52688.2022.02007; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Wei JW, 2020, JAMA NETW OPEN, V3, DOI 10.1001/jamanetworkopen.2020.3398; Xu G, 2019, IEEE I CONF COMP VIS, P10681, DOI 10.1109/ICCV.2019.01078; Xu J., 2021, Int. J. Image Graph. Signal Process. (IJIGSP), V13, P33, DOI [DOI 10.5815/IJIGSP.2021.04.03, 10.5815/ijigsp.2021.04.03]; Yao JW, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101789; Zhang PY, 2017, I S BIOMED IMAGING, P578, DOI 10.1109/ISBI.2017.7950587; Zhou CJ, 2021, COMPUT MED IMAG GRAP, V88, DOI 10.1016/j.compmedimag.2021.101861; Zhou HY, 2021, IEEE INT CONF COMP V, P2230, DOI 10.1109/ICCVW54120.2021.00252	53	19	20	7	52	WILEY	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0022-3417	1096-9896		J PATHOL	J. Pathol.	FEB	2023	259	2					125	135		10.1002/path.6027	http://dx.doi.org/10.1002/path.6027		DEC 2022	11	Oncology; Pathology	Science Citation Index Expanded (SCI-EXPANDED)	Oncology; Pathology	7R1CT	36318158				2024-09-18	WOS:000893601400001
C	Tomar, NK; Shergill, A; Rieders, B; Bagci, U; Jha, D			IEEE	Tomar, Nikhil Kumar; Shergill, Annie; Rieders, Brandon; Bagci, Ulas; Jha, Debesh			TransResU-Net: A Transformer based ResU-Net for Real-Time Colon Polyp Segmentation	2023 45TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE & BIOLOGY SOCIETY, EMBC	IEEE Engineering in Medicine and Biology Society Conference Proceedings		English	Proceedings Paper	45th Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC)	JUL 24-27, 2023	Sydney, AUSTRALIA	IEEE, IEEE Engn Med & Biol Soc			COLORECTAL-CANCER	Colorectal cancer (CRC) is one of the most common causes of cancer and cancer-related mortality worldwide. Performing colon cancer screening in a timely fashion is the key to early detection. Colonoscopy is the primary modality used to diagnose colon cancer. However, the miss rate of polyps, adenomas and advanced adenomas remains significantly high. Early detection of polyps at the precancerous stage can help reduce the mortality rate and the economic burden associated with colorectal cancer. Deep learning-based computer-aided diagnosis (CADx) system may help gastroenterologists to identify polyps that may otherwise be missed, thereby improving the polyp detection rate. Additionally, CADx system could prove to be a cost-effective system that improves long-term colorectal cancer prevention. In this study, we proposed a deep learning-based architecture for automatic polyp segmentation called Transformer ResU-Net (TransResU-Net). Our proposed architecture is built upon residual blocks with ResNet-50 as the backbone and takes advantage of the transformer self-attention mechanism as well as dilated convolution(s). Our experimental results on two publicly available polyp segmentation benchmark datasets showed that TransResU-Net obtained a highly promising dice score and a real-time speed. With high efficacy in our performance metrics, we concluded that TransResU-Net could be a strong benchmark for building a real-time polyp detection system for the early diagnosis, treatment, and prevention of colorectal cancer. The source code of the proposed TransResU-Net is publicly available at https://github.com/nikhilroxtomar/TransResUNet.	[Tomar, Nikhil Kumar; Bagci, Ulas; Jha, Debesh] Northwestern Univ, Dept Radiol, Machine Hybrid Intelligence Lab, Evanston, IL 60208 USA; [Shergill, Annie] Larkin Community Hosp, Palm Springs Campus, Hialeah, FL USA; [Rieders, Brandon] Yea Long Isl Jewish Valley Stream, Valley Stream, NY USA	Northwestern University	Tomar, NK (corresponding author), Northwestern Univ, Dept Radiol, Machine Hybrid Intelligence Lab, Evanston, IL 60208 USA.		Bagci, Ulas/A-4225-2012; Jha, Debesh/M-2526-2019	Bagci, Ulas/0000-0001-7379-6829	NIH [R01-CA246704, R01-CA240639]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This project is partially supported by the NIH funding: R01-CA246704 and R01-CA240639.	Abu Dayyeh BK, 2015, GASTROINTEST ENDOSC, V81, P502, DOI 10.1016/j.gie.2014.12.022; Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067; Demir U., 2022, P INT C PAR ART INT; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsiang Huang C., 2021, ARXIV210107172; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Kronborg O, 1996, LANCET, V348, P1467, DOI 10.1016/S0140-6736(96)03430-7; Lan PN, 2021, LECT NOTES COMPUT SC, V13018, P15, DOI 10.1007/978-3-030-90436-4_2; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Seeff LC, 2004, GASTROENTEROLOGY, V127, P1670, DOI 10.1053/j.gastro.2004.09.051; Sung H, 2022, JNCI-J NATL CANCER I, V114, P1095, DOI 10.1093/jnci/djac091; Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23; Tomar N. K., 2022, P INT C MED IM COMP; Tomar NK, 2023, IEEE T NEUR NET LEAR, V34, P9375, DOI 10.1109/TNNLS.2022.3159394; Vaswani A, 2017, ADV NEUR IN, V30; Wadhwa V, 2020, ENDOSC INT OPEN, V08, pE1379, DOI 10.1055/a-1223-1926; Wilson ML, 2018, LANCET, V391, P1927, DOI [10.1016/S0140-6736(18)30458-6, 10.1016/S0140-6736]; Yu F., 2015, arXiv preprint arXiv: 1506. 03365; Zauber AG, 2012, NEW ENGL J MED, V366, P687, DOI 10.1056/NEJMoa1100370; Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587; Zhao SB, 2019, GASTROENTEROLOGY, V156, P1661, DOI 10.1053/j.gastro.2019.01.260; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]	27	0	0	3	7	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1557-170X	1558-4615	979-8-3503-2447-1	IEEE ENG MED BIO			2023										10.1109/EMBC40787.2023.10340572	http://dx.doi.org/10.1109/EMBC40787.2023.10340572			4	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BW2ZJ	38083589				2024-09-18	WOS:001133788302150
J	Zidan, U; Gaber, MM; Abdelsamea, MM				Zidan, Usama; Gaber, Mohamed Medhat; Abdelsamea, Mohammed M.			SwinCup: Cascaded swin transformer for histopathological structures segmentation in colorectal cancer	EXPERT SYSTEMS WITH APPLICATIONS			English	Article						Transformers; Histology image analysis; Gland segmentation; Deep learning; Self-supervision		Transformer models have recently become the dominant architecture in many computer vision tasks, including image classification, object detection, and image segmentation. The main reason behind their success is the ability to incorporate global context information into the learning process. By utilising self-attention, recent advancements in the Transformer architecture design enable models to consider long-range dependencies. In this paper, we propose a novel transformer, named Swin Transformer with Cascaded UPsampling (SwinCup) model for the segmentation of histopathology images. We use a hierarchical Swin Transformer with shifted windows as an encoder to extract global context features. The multi-scale feature extraction in a Swin transformer enables the model to attend to different areas in the image at different scales. A cascaded up sampling decoder is used with an encoder to improve its feature aggregation. Experiments on GLAS and CRAG histopathology colorectal cancer datasets were used to validate the model, achieving an average 0.90 (F1 score) and surpassing the state-of-the-art by (23%).	[Zidan, Usama; Gaber, Mohamed Medhat; Abdelsamea, Mohammed M.] Birmingham City Univ, Sch Comp & Digital Technol, Birmingham B4 7XG, England; [Gaber, Mohamed Medhat] Galala Univ, Fac Comp Sci & Engn, Al Galala, Egypt; [Abdelsamea, Mohammed M.] Univ Assiut, Fac Comp & Informat, Dept Comp Sci, Asyut, Egypt	Birmingham City University; Galala University	Abdelsamea, MM (corresponding author), Birmingham City Univ, Sch Comp & Digital Technol, Birmingham B4 7XG, England.	usama.zidan@bcu.ac.uk; mohamed.gaber@bcu.ac.uk; mohammed.abdelsamea@bcu.ac.uk	Abdelsamea, Mohammed/AAF-1031-2019	Zidan, Usama/0000-0001-8309-0432; Abdelsamea, Mohammed M./0000-0002-2728-1127				Abdelsamea MM, 2022, WIRES DATA MIN KNOWL, V12, DOI 10.1002/widm.1474; Awan R, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16516-w; Bulten W, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-37257-4; Cao H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2105.05537; Chen J., 2021, PREPRINT, DOI [DOI 10.48550/ARXIV.2102.04306, 10.48550/arxiv.2102.04306]; Chen LC, 2017, Arxiv, DOI arXiv:1706.05587; Chen XL, 2020, Arxiv, DOI [arXiv:2003.04297, DOI 10.48550/ARXIV.2003.04297]; Dash S, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14020194; Dosovitskiy A., 2021, P ICLR 2021, DOI 10.48550/arXiv.2010.11929; Hussain R, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13234941; Iizuka O, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-58467-9; Kushnure DT, 2021, COMPUT MED IMAG GRAP, V89, DOI 10.1016/j.compmedimag.2021.101885; Landman B., 2015, 2015 MICCAI MULTIATL, V5, P12; Lin AL, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3178991; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Oktay O, 2018, Arxiv, DOI arXiv:1804.03999; Paszke A, 2019, ADV NEUR IN, V32; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sirinukunwattana K, 2016, Arxiv, DOI arXiv:1603.00275; Song ZG, 2020, BMJ OPEN, V10, DOI 10.1136/bmjopen-2019-036423; Subhan F, 2022, CANCERS, V14, DOI 10.3390/cancers14174191; Taher M. R. H., 2021, A systematic benchmarking analysis of transfer learning for medical image analysis; Valanarasu JMJ, 2021, LECT NOTES COMPUT SC, V12901, P36, DOI 10.1007/978-3-030-87193-2_4; Vaswani A, 2017, ADV NEUR IN, V30; Veta M, 2019, MED IMAGE ANAL, V54, P111, DOI 10.1016/j.media.2019.02.012; Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11; Wen Y, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103145; Yan XY, 2022, IEEE WINT CONF APPL, P3270, DOI 10.1109/WACV51458.2022.00333; Yang HN, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.119024; Yang JC, 2022, Arxiv, DOI arXiv:2110.14795; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou H.-Y., 2021, arXiv, DOI DOI 10.48550/ARXIV.2109.03201; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	34	20	20	12	55	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0957-4174	1873-6793		EXPERT SYST APPL	Expert Syst. Appl.	APR 15	2023	216								119452	10.1016/j.eswa.2022.119452	http://dx.doi.org/10.1016/j.eswa.2022.119452		JAN 2023	11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Operations Research & Management Science	8D4XZ		hybrid, Green Accepted			2024-09-18	WOS:000918299000001
C	Amadeus, S; Cenggoro, TW; Budiarto, A; Pardamean, B		Budiharto, W; Kurniawan, A; Suhartono, D; Chowanda, A; Gunawan, AAS; Udjaja, Y		Amadeus, Steven; Cenggoro, Tjeng Wawan; Budiarto, Arif; Pardamean, Bens			A Design of Polygenic Risk Model with Deep Learning for Colorectal Cancer in Multiethnic Indonesians	5TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND COMPUTATIONAL INTELLIGENCE 2020	Procedia Computer Science		English	Proceedings Paper	5th International Conference on Computer Science and Computational Intelligence (ICCSCI)	NOV 19-20, 2020	ELECTR NETWORK			Prognostication; Polygenic Risk Model; Transformer Model; DeepLIFT; Colorectal Cancer		Recently, health management is emerging and attract attention to how to provide better prognostication and health management systems. The challenges in the prognostication are how to develop a model that can self-learn the prognostication features and how to get a high accuracy prediction. Prognostication in health disease involves SNPs which is a genetic marker. In this paper, we propose a polygenic risk model using deep learning: Transformer with self-attention mechanism and DeepLIFT. The use of these deep learning model allows us to predict the risk of colorectal cancer and see the correlation between SNPs. (C) 2021 The Authors. Published by Elsevier B.V.	[Amadeus, Steven; Pardamean, Bens] Bina Nusantara Univ, Comp Sci Dept, BINUS Grad Program, Master Comp Sci Program, Jakarta 11480, Indonesia; [Cenggoro, Tjeng Wawan; Budiarto, Arif] Bina Nusantara Univ, Sch Comp Sci, Comp Sci Dept, Jakarta 11480, Indonesia; [Cenggoro, Tjeng Wawan; Budiarto, Arif; Pardamean, Bens] Bina Nusantara Univ, Bioinformat & Data Sci Res Ctr, Jakarta 11480, Indonesia	Universitas Bina Nusantara; Universitas Bina Nusantara; Universitas Bina Nusantara	Amadeus, S (corresponding author), Bina Nusantara Univ, Comp Sci Dept, BINUS Grad Program, Master Comp Sci Program, Jakarta 11480, Indonesia.	steven.amadeus@binus.ac.id	Cenggoro, Tjeng/AAZ-3355-2020; Pardamean, Bens/GRJ-5921-2022	Pardamean, Bens/0000-0002-7404-9005				Abdullah Murdani, 2012, Gastroenterol Hepatol Bed Bench, V5, P71; Abiyev RH, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/4168538; Baurley JW, 2018, TRENDS MOL MED, V24, P221, DOI 10.1016/j.molmed.2017.12.008; Baurley JW, 2012, ADV INTEL SOFT COMPU, V144, P377; Bychkov D, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21758-3; Cenggoro TW, 2019, PROCEDIA COMPUT SCI, V157, P313, DOI 10.1016/j.procs.2019.08.172; Chen R.C., 2020, SYLWAN, V164; Chen T., 2016, P 22 KNOWL DISC DAT, P785; Choi SW., 2018, GUIDE PERFORMING POL, V2, DOI DOI 10.1101/416545; Dara S., 2018, INT J PURE APPL MATH, V120, P305; Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171; Haiman CA, 2010, CURR OPIN GENET DEV, V20, P330, DOI 10.1016/j.gde.2010.02.007; Joyner C, 2020, BIOMETRICAL J, V62, P191, DOI 10.1002/bimj.201900050; Kourou K, 2015, COMPUT STRUCT BIOTEC, V13, P8, DOI 10.1016/j.csbj.2014.11.005; Levine ME, 2017, METHODS MOL BIOL, V1613, P277, DOI 10.1007/978-1-4939-7027-8_10; Li YK, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62922-y; Lumbanraja FR, 2019, PROCEDIA COMPUT SCI, V157, P25, DOI 10.1016/j.procs.2019.08.137; Mahesworo B., 2020, COMMUN MATH BIOL NEU, V2020; McMahan C, 2017, STAT APPL GENET MOL, V16, P407, DOI 10.1515/sagmb-2017-0044; Pardamean B, 2016, INT J COLORECTAL DIS, V31, P1537, DOI 10.1007/s00384-016-2564-z; Peters U, 2015, GUT, V64, P1623, DOI 10.1136/gutjnl-2013-306705; Qin DH, 2019, CANCER BIOL MED, V16, P4, DOI 10.20892/j.issn.2095-3941.2018.0055; Schmit SL, 2016, CARCINOGENESIS, V37, P547, DOI 10.1093/carcin/bgw046; Shrikumar A, 2017, PR MACH LEARN RES, V70; Siegel R, 2014, CA-CANCER J CLIN, V64, P9, DOI 10.3322/caac.21208; Vaswani A, 2017, ADV NEUR IN, V30; Wan JJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-54031-2; Yang ZL, 2019, ADV NEUR IN, V32; Yusuf I., 2019, GENETIC RISK FACTORS	29	2	2	0	7	ELSEVIER SCIENCE BV	AMSTERDAM	SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	1877-0509			PROCEDIA COMPUT SCI			2021	179						632	639		10.1016/j.procs.2021.01.049	http://dx.doi.org/10.1016/j.procs.2021.01.049			8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR5DK		gold			2024-09-18	WOS:000654256300079
J	Sui, D; Zhang, K; Liu, WF; Chen, J; Ma, XX; Tian, ZF				Sui, Dong; Zhang, Kang; Liu, Weifeng; Chen, Jing; Ma, Xiaoxuan; Tian, Zhaofeng			CST: A Multitask Learning Framework for Colorectal Cancer Region Mining Based on Transformer	BIOMED RESEARCH INTERNATIONAL			English	Article							CONVOLUTIONAL NEURAL-NETWORKS	Colorectal cancer is a high death rate cancer until now; from the clinical view, the diagnosis of the tumour region is critical for the doctors. But with data accumulation, this task takes lots of time and labor with large variances between different doctors. With the development of computer vision, detection and segmentation of the colorectal cancer region from CT or MRI image series are a great challenge in the past decades, and there still have great demands on automatic diagnosis. In this paper, we proposed a novel transfer learning protocol, called CST, that is, a union framework for colorectal cancer region detection and segmentation task based on the transformer model, which effectively constructs the cancer region detection and its segmentation jointly. To make a higher detection accuracy, we incorporate an autoencoder-based image-level decision approach that leverages the image-level decision of a cancer slice. We also compared our framework with one-stage and two-stage object detection methods; the results show that our proposed method achieves better results on detection and segmentation tasks. And this proposed framework will give another pathway for colorectal cancer screen by way of artificial intelligence.	[Sui, Dong; Zhang, Kang; Liu, Weifeng; Ma, Xiaoxuan] Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Beijing 100044, Peoples R China; [Chen, Jing; Tian, Zhaofeng] Navy Med Univ, Changhai Hosp, Dept Lab & Diag, Shanghai 200433, Peoples R China	Beijing University of Civil Engineering & Architecture; Naval Medical University	Sui, D (corresponding author), Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.; Tian, ZF (corresponding author), Navy Med Univ, Changhai Hosp, Dept Lab & Diag, Shanghai 200433, Peoples R China.	suidong@bucea.edu.cn; tian_zhao_feng@163.com	liu, weifeng/F-6250-2010	Sui, Dong/0000-0002-7887-2111; Liu, WeiFeng/0000-0002-7798-797X	National Natural Science Foundation of China [61702026, 62031003]; Pyramid Talent Training Project of Beijing University of Civil Engineering and Architecture [JDYC20200318]; National Key Research and Development Program of China [2020YFF0305504]; Doctoral Research Initiation Fund of Beijing University of Civil Engineering and Architecture [X20040]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Pyramid Talent Training Project of Beijing University of Civil Engineering and Architecture; National Key Research and Development Program of China(National Key Research & Development Program of China); Doctoral Research Initiation Fund of Beijing University of Civil Engineering and Architecture	This research is supported by the National Natural Science Foundation of China (Grant Nos. 61702026 and 62031003), the Pyramid Talent Training Project of Beijing University of Civil Engineering and Architecture (Grant No. JDYC20200318), the National Key Research and Development Program of China (Grant No. 2020YFF0305504), and the Doctoral Research Initiation Fund of Beijing University of Civil Engineering and Architecture (No. X20040).	Ahmad HM, 2020, CURR MED IMAGING, V16, P946, DOI 10.2174/1573405615666191219100824; Desai Sudhen B, 2020, Intell Based Med, V3, P100013, DOI 10.1016/j.ibmed.2020.100013; Gao Y., 2019, INT C MED IM COMP CO; Gao YH, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101831; Glorot X., 2010, P 13 INT C ART INT S, P249; Gupta V, 2020, J DIGIT IMAGING, V33, P431, DOI 10.1007/s10278-019-00267-3; Hassanzadeh T, 2021, IEEE T MED IMAGING, V40, P712, DOI 10.1109/TMI.2020.3035555; Jensch S, 2010, EUR RADIOL, V20, P146, DOI 10.1007/s00330-009-1517-0; Jin Jill, 2021, JAMA, V325, P2026, DOI 10.1001/jama.2021.6557; JingdongWang Ke, 2021, T-PAMI; Knudsen AB, 2021, JAMA-J AM MED ASSOC, V325, P1998, DOI 10.1001/jama.2021.5746; Lei M, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11030534; Li XM, 2018, MED IMAGE ANAL, V45, P41, DOI 10.1016/j.media.2018.01.004; Li Zuchao, 2021, IEEE Transactions on Pattern Analysis and Machine Intelligence.; Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37; Lin JS, 2021, JAMA-J AM MED ASSOC, V325, P1978, DOI 10.1001/jama.2021.4417; Liu P., 2021, IEEE T PATTERN ANAL; Loey Mohamed, 2021, Sustain Cities Soc, V65, P102600, DOI 10.1016/j.scs.2020.102600; Loshkarev IY, 2019, J PHYS CONF SER, V1333, DOI 10.1088/1742-6596/1333/4/042019; Lu Q, 2021, MED IMAGE ANAL, V72, DOI 10.1016/j.media.2021.102094; Ma YJ, 2021, BIOMED PHYS ENG EXPR, V7, DOI 10.1088/2057-1976/ac008a; Mehta SJ, 2021, JAMA NETW OPEN, V4, DOI 10.1001/jamanetworkopen.2021.12593; Ng K, 2021, JAMA-J AM MED ASSOC, V325, P1943, DOI 10.1001/jama.2021.4133; Petit O, 2021, COMPUT MED IMAG GRAP, V91, DOI 10.1016/j.compmedimag.2021.101938; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162; Song DP, 2021, IEEE T MED IMAGING, V40, P2392, DOI 10.1109/TMI.2021.3077484; Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Touvron H, 2021, PR MACH LEARN RES, V139, P7358; Vaswani A, 2017, ADV NEUR IN, V30; Wilson S, 2020, RADIOGRAPHY, V26, pE290, DOI 10.1016/j.radi.2020.04.007; Xie HQ, 2021, PHYS MED BIOL, V66, DOI 10.1088/1361-6560/ac0684; Yang L., 2018, ABS180600593 ARXIV; Zhang YD, 2024, COGN COMPUT, V16, P1649, DOI 10.1007/s12559-020-09776-8; Zhang Z., 2020, Interactive Learning Environments, P1, DOI [DOI 10.1080/10494820.2020.1723113, DOI 10.1109/TFUZZ.2020.2967294]; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660	37	5	6	1	21	WILEY	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	2314-6133	2314-6141		BIOMED RES INT	Biomed Res. Int.	OCT 11	2021	2021								6207964	10.1155/2021/6207964	http://dx.doi.org/10.1155/2021/6207964			8	Biotechnology & Applied Microbiology; Medicine, Research & Experimental	Science Citation Index Expanded (SCI-EXPANDED)	Biotechnology & Applied Microbiology; Research & Experimental Medicine	ZU1QX	34671677	Green Published, gold			2024-09-18	WOS:000769620900006
J	Hille, G; Tummala, P; Spitz, L; Saalfeld, S				Hille, Georg; Tummala, Pavan; Spitz, Lena; Saalfeld, Sylvia			Transformers for colorectal cancer segmentation in CT imaging	INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY			English	Article; Early Access						Deep learning; Transformers; Colorectal cancer; Segmentation; Medical segmentation decathlon; CT imaging		PurposeMost recently transformer models became the state of the art in various medical image segmentation tasks and challenges, outperforming most of the conventional deep learning approaches. Picking up on that trend, this study aims at applying various transformer models to the highly challenging task of colorectal cancer (CRC) segmentation in CT imaging and assessing how they hold up to the current state-of-the-art convolutional neural network (CNN), the nnUnet. Furthermore, we wanted to investigate the impact of the network size on the resulting accuracies, since transformer models tend to be significantly larger than conventional network architectures.MethodsFor this purpose, six different transformer models, with specific architectural advancements and network sizes were implemented alongside the aforementioned nnUnet and were applied to the CRC segmentation task of the medical segmentation decathlon.ResultsThe best results were achieved with the Swin-UNETR, D-Former, and VT-Unet, each transformer models, with a Dice similarity coefficient (DSC) of 0.60, 0.59 and 0.59, respectively. Therefore, the current state-of-the-art CNN, the nnUnet could be outperformed by transformer architectures regarding this task. Furthermore, a comparison with the inter-observer variability (IOV) of approx. 0.64 DSC indicates almost expert-level accuracy. The comparatively low IOV emphasizes the complexity and challenge of CRC segmentation, as well as indicating limitations regarding the achievable segmentation accuracy.ConclusionAs a result of this study, transformer models underline their current upward trend in producing state-of-the-art results also for the challenging task of CRC segmentation. However, with ever smaller advances in total accuracies, as demonstrated in this study by the on par performances of multiple network variants, other advantages like efficiency, low computation demands, or ease of adaption to new tasks become more and more relevant.	[Hille, Georg; Tummala, Pavan; Spitz, Lena] Univ Magdeburg, Dept Simulat & Graph, Magdeburg, Germany; [Hille, Georg; Saalfeld, Sylvia] Res Campus STIMULATE, Magdeburg, Germany; [Saalfeld, Sylvia] Tech Univ Ilmenau, Inst Appl Comp Sci, D-98684 Ilmenau, Germany	Otto von Guericke University; Technische Universitat Ilmenau	Hille, G (corresponding author), Univ Magdeburg, Dept Simulat & Graph, Magdeburg, Germany.; Hille, G (corresponding author), Res Campus STIMULATE, Magdeburg, Germany.	georg.hille@ovgu.de	Saalfeld, Sylvia/AHE-1096-2022		Federal Ministry of Education and Research within the research campus STIMULATE [13GW0473A]	Federal Ministry of Education and Research within the research campus STIMULATE	This work is funded by the Federal Ministry of Education and Research within the research campus STIMULATE under the grant number '13GW0473A'.	Ahmad HM, 2020, CURR MED IMAGING, V16, P946, DOI 10.2174/1573405615666191219100824; Antonelli M, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-30695-9; Chen J., 2021, PREPRINT, DOI [DOI 10.48550/ARXIV.2102.04306, 10.48550/arxiv.2102.04306]; Chen JE, 2023, Arxiv, DOI arXiv:2310.07781; Hatamizadeh A, 2022, LECT NOTES COMPUT SC, V12962, P272, DOI 10.1007/978-3-031-08999-2_22; Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181; Hille G, 2023, COMPUT METH PROG BIO, V240, DOI 10.1016/j.cmpb.2023.107647; Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z; Jia X, 2020, IEEE T AUTOM SCI ENG, V17, P1570, DOI 10.1109/TASE.2020.2964827; Liu XM, 2019, MED PHYS, V46, P3532, DOI 10.1002/mp.13584; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Mainenti PP, 2019, WORLD J GASTROENTERO, V25, P5233, DOI 10.3748/wjg.v25.i35.5233; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Pei Y, 2020, IEEE ACCESS, V8, P64131, DOI 10.1109/ACCESS.2020.2982543; Peiris H, 2022, LECT NOTES COMPUT SC, V13435, P162, DOI 10.1007/978-3-031-16443-9_16; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Soomro MH, 2018, MID EAST CONF BIO, P198, DOI 10.1109/MECBME.2018.8402433; Sui D, 2021, BIOMED RES INT, V2021, DOI 10.1155/2021/6207964; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tang Y., 2021, P IEEE CVF C COMP VI; Tummala P., 2023, BVM WORKSH, P165; Wu YX, 2023, NEURAL COMPUT APPL, V35, P1931, DOI [10.1007/s00521-022-07859-1, 10.1007/s11071-022-07935-0]; Yao LS, 2022, LECT NOTES COMPUT SC, V13433, P564, DOI 10.1007/978-3-031-16437-8_54; Zhang RZ, 2023, Arxiv, DOI arXiv:2310.04677; Zhou HY, 2023, IEEE T IMAGE PROCESS, V32, P4036, DOI 10.1109/TIP.2023.3293771	25	0	0	2	2	SPRINGER HEIDELBERG	HEIDELBERG	TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY	1861-6410	1861-6429		INT J COMPUT ASS RAD	Int. J. Comput. Assist. Radiol. Surg.	2024 JUL 4	2024										10.1007/s11548-024-03217-9	http://dx.doi.org/10.1007/s11548-024-03217-9		JUL 2024	9	Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging; Surgery	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Radiology, Nuclear Medicine & Medical Imaging; Surgery	XM8T4	38965166				2024-09-18	WOS:001262200600001
J	Lu, N; Guan, X; Zhu, JG; Li, Y; Zhang, JP				Lu, Na; Guan, Xiao; Zhu, Jianguo; Li, Yuan; Zhang, Jianping			A Contrast-Enhanced CT-Based Deep Learning System for Preoperative Prediction of Colorectal Cancer Staging and RAS Mutation	CANCERS			English	Article						colorectal cancer; stage; RAS status; deep learning; convolutional neural networks; transformer	LYMPH-NODE METASTASIS; RADIOMIC FEATURES; ARTIFICIAL-INTELLIGENCE; TARGETED THERAPY; PROGNOSIS; DIAGNOSIS; ADENOCARCINOMA; IDENTIFICATION; NOMOGRAM; FOLFIRI	Simple Summary This study explored the role of CT-based deep learning in detecting colorectal cancer tumor location and preoperatively predicting the stage and RAS gene mutation status of colorectal cancer patients. The deep learning model we built achieved excellent performance. The detection network based on Yolov7 realized the detection and preoperative staging of colorectal cancer with an average mean accuracy of 0.98 in the validation cohort. The vision transformer-based prediction network achieved accurate prediction of preoperative RAS in colorectal cancer patients, achieving an area under the receiver operating characteristic curve (AUC) of 0.9591 and 0.9554 in the test cohort and the validation cohort, respectively. This study also explored the clinical applications of deep learning models. Based on the proposed detection network and prediction network, we built a deep learning system for clinicians who do not understand deep learning.Abstract Purpose: This study aimed to build a deep learning system using enhanced computed tomography (CT) portal-phase images for predicting colorectal cancer patients' preoperative staging and RAS gene mutation status. Methods: The contrast-enhanced CT image dataset comprises the CT portal-phase images from a retrospective cohort of 231 colorectal cancer patients. The deep learning system was developed via migration learning for colorectal cancer detection, staging, and RAS gene mutation status prediction. This study used pre-trained Yolov7, vision transformer (VIT), swin transformer (SWT), EfficientNetV2, and ConvNeXt. 4620, and contrast-enhanced CT images and annotated tumor bounding boxes were included in the tumor identification and staging dataset. A total of 19,700 contrast-enhanced CT images comprise the RAS gene mutation status prediction dataset. Results: In the validation cohort, the Yolov7-based detection model detected and staged tumors with a mean accuracy precision (IoU = 0.5) (mAP_0.5) of 0.98. The area under the receiver operating characteristic curve (AUC) in the test set and validation set for the VIT-based prediction model in predicting the mutation status of the RAS genes was 0.9591 and 0.9554, respectively. The detection network and prediction network of the deep learning system demonstrated great performance in explaining contrast-enhanced CT images. Conclusion: In this study, a deep learning system was created based on the foundation of contrast-enhanced CT portal-phase imaging to preoperatively predict the stage and RAS mutation status of colorectal cancer patients. This system will help clinicians choose the best treatment option to increase colorectal cancer patients' chances of survival and quality of life.	[Lu, Na; Guan, Xiao; Zhang, Jianping] Nanjing Med Univ, Dept Gen Surg, Affiliated Hosp 2, 121 Jiangjiayuan Rd, Nanjing 210011, Peoples R China; [Zhu, Jianguo] Nanjing Med Univ, Dept Radiol, Affiliated Hosp 2, Nanjing 210011, Peoples R China; [Li, Yuan] Nanjing Med Univ, Sch Publ Hlth, Key Lab Modern Toxicol, Minist Educ, Nanjing 211166, Peoples R China	Nanjing Medical University; Nanjing Medical University; Nanjing Medical University	Zhang, JP (corresponding author), Nanjing Med Univ, Dept Gen Surg, Affiliated Hosp 2, 121 Jiangjiayuan Rd, Nanjing 210011, Peoples R China.	n2248457306@163.com; xiaoguan@stu.njmu.edu.cn; zhujianguo@njmu.edu.cn; liyuan@njmu.edu.cn; drzhangjp@njmu.edu.cn	Lu, Na/KMA-3229-2024		The authors thanked all colleagues who contributed to this work.	The authors thanked all colleagues who contributed to this work.	The authors thanked all colleagues who contributed to this work.	AAlAbdulsalam Abdulrahman K, 2018, AMIA Jt Summits Transl Sci Proc, V2017, P16; Aerts HJWL, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5006; Amin MB, 2017, CA-CANCER J CLIN, V67, P93, DOI 10.3322/caac.21388; [Anonymous], Guidelines; Balachandran VP, 2015, LANCET ONCOL, V16, pE173, DOI 10.1016/S1470-2045(14)71116-7; Barras D, 2017, CLIN CANCER RES, V23, P104, DOI 10.1158/1078-0432.CCR-16-0140; Bedrikovetski S, 2021, BMC CANCER, V21, DOI 10.1186/s12885-021-08773-w; Bibault JE, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30657-6; Cabitza F, 2017, JAMA-J AM MED ASSOC, V318, P517, DOI 10.1001/jama.2017.7797; Camidge DR, 2019, NAT REV CLIN ONCOL, V16, P341, DOI 10.1038/s41571-019-0173-9; Cellina M, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12112644; Chalkidou A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0124165; Chang X, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.627947; Cremolini C, 2015, LANCET ONCOL, V16, P1306, DOI 10.1016/S1470-2045(15)00122-9; de Boer LL, 2017, LASER MED SCI, V32, P711, DOI 10.1007/s10103-016-2119-0; Deo RC, 2015, CIRCULATION, V132, P1920, DOI 10.1161/CIRCULATIONAHA.115.001593; Dong D, 2020, ANN ONCOL, V31, P912, DOI 10.1016/j.annonc.2020.04.003; Dosovitskiy A., 2021, P IEEE C LEARN REPR; Dou YF, 2022, MEDICINE, V101, DOI 10.1097/MD.0000000000029244; Drozdzal M, 2018, MED IMAGE ANAL, V44, P1, DOI 10.1016/j.media.2017.11.005; ESR, 2010, INSIGHTS IMAGING, V1, P42, DOI 10.1007/s13244-010-0025-8; He P, 2021, J ONCOL, V2021, DOI 10.1155/2021/6687291; Hosny A, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002711; Hou M, 2023, EUR RADIOL, V33, P1, DOI 10.1007/s00330-022-08952-8; Huang SG, 2020, CANCER LETT, V471, P61, DOI 10.1016/j.canlet.2019.12.007; Huang YQ, 2016, J CLIN ONCOL, V34, P2157, DOI 10.1200/JCO.2015.65.9128; Jia LL, 2023, EUR J RADIOL, V158, DOI 10.1016/j.ejrad.2022.110640; Joo I, 2015, J MAGN RESON IMAGING, V41, P814, DOI 10.1002/jmri.24586; Jiang KL, 2021, FRONT MED-LAUSANNE, V8, DOI 10.3389/fmed.2021.629080; Kalligosfyri PM, 2022, BIOSENSORS-BASEL, V12, DOI 10.3390/bios12020097; Kermany DS, 2018, CELL, V172, P1122, DOI 10.1016/j.cell.2018.02.010; Kim K, 2021, KOREAN J RADIOL, V22, P912, DOI 10.3348/kjr.2020.0447; Kubota K, 2017, GASTROINTEST TUMORS, V3, P163, DOI 10.1159/000454923; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Levy I, 2016, BEST PRACT RES CL GA, V30, P705, DOI 10.1016/j.bpg.2016.09.005; Li L, 2020, GASTRIC CANCER, V23, P126, DOI 10.1007/s10120-019-00992-2; Li ML, 2020, J TRANSL MED, V18, DOI 10.1186/s12967-020-02215-0; Li Q, 2019, CLIN CANCER RES, V25, P5212, DOI 10.1158/1078-0432.CCR-18-4173; Liang F, 2022, WORLD J GASTRO ONCOL, V14, P124, DOI 10.4251/wjgo.v14.i1.124; Liu SL, 2017, EUR RADIOL, V27, P4951, DOI 10.1007/s00330-017-4881-1; Liu Y, 2016, CLIN LUNG CANCER, V17, P441, DOI 10.1016/j.cllc.2016.02.001; Liu Z, 2021, Arxiv, DOI arXiv:2103.14030; Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167; Lu Y, 2018, CANCER RES, V78, P5135, DOI 10.1158/0008-5472.CAN-18-0494; Minami S, 2022, CANCERS, V14, DOI 10.3390/cancers14215361; Nasseri Y, 2017, SURG CLIN N AM, V97, P503, DOI 10.1016/j.suc.2017.01.002; Obaro AE, 2018, LANCET GASTROENTEROL, V3, P326, DOI 10.1016/S2468-1253(18)30032-3; Pacal I, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104003; Peeters M, 2015, CLIN CANCER RES, V21, P5469, DOI 10.1158/1078-0432.CCR-15-0526; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russo M, 2019, SCIENCE, V366, P1473, DOI 10.1126/science.aav4474; Russo M, 2016, CANCER DISCOV, V6, P147, DOI 10.1158/2159-8290.CD-15-1283; Schmidt DR, 2021, CA-CANCER J CLIN, V71, P333, DOI 10.3322/caac.21670; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Strickler JH, 2017, CANCER TREAT REV, V60, P109, DOI 10.1016/j.ctrv.2017.08.006; Sundar R, 2017, CANCER DISCOV, V7, P558, DOI 10.1158/2159-8290.CD-17-0087; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tan MX, 2020, Arxiv, DOI [arXiv:1905.11946, DOI 10.48550/ARXIV.1905.11946]; Tang YL, 2023, WORLD J GASTROENTERO, V29, P926, DOI 10.3748/wjg.v29.i6.926; Traverso A, 2018, INT J RADIAT ONCOL, V102, P1143, DOI 10.1016/j.ijrobp.2018.05.053; Ueyama H, 2021, J GASTROEN HEPATOL, V36, P482, DOI 10.1111/jgh.15190; Vaswani A, 2017, ADV NEUR IN, V30; Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721; Wang J, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aax3223; Wang N, 2020, CANCER MANAG RES, V12, P1211, DOI 10.2147/CMAR.S230138; Wong D, 2018, NATURE, V555, P446, DOI 10.1038/d41586-018-02881-7; Wu QY, 2021, CHINESE MED J-PEKING, V134, P821, DOI 10.1097/CM9.0000000000001401; Xu YW, 2019, CLIN CANCER RES, V25, P3266, DOI 10.1158/1078-0432.CCR-18-2495; Xue T, 2022, BRIT J RADIOL, V95, DOI 10.1259/bjr.20211014; Yun J, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42276-w; Zheng LB, 2020, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.01238; Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001; Zou L, 2022, CHINESE MED J-PEKING, V135, P26, DOI 10.1097/CM9.0000000000001637	74	1	1	1	8	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		2072-6694		CANCERS	Cancers	SEP	2023	15	18							4497	10.3390/cancers15184497	http://dx.doi.org/10.3390/cancers15184497			14	Oncology	Science Citation Index Expanded (SCI-EXPANDED)	Oncology	S5EI5	37760468	gold, Green Published			2024-09-18	WOS:001071393400001
J	Raju, ASN; Venkatesh, K; Padmaja, B; Reddy, GS				Raju, Akella S. Narasimha; Venkatesh, K.; Padmaja, B.; Reddy, G. Sucharitha			GIEnsemformerCADx: A hybrid ensemble learning approach for enhanced gastrointestinal cancer recognition	MULTIMEDIA TOOLS AND APPLICATIONS			English	Article; Early Access						Colorectal cancer; Deep learning; Hybrid technique; Computer-aided diagnosis (CADx); Vision Transformer; Fusion Convolutional Neural Networks (FCNNs); Bidirectional Long Short-Term Memory (BiLSTM)	DIAGNOSIS	Colorectal cancer, a formidable health hazard, necessitates the development of innovative and accurate diagnostic instruments in light of the rising mortality rates associated with gastrointestinal disorders. The introduction of deep learning algorithms has revolutionised disease detection, but the search for cutting-edge techniques continues to be essential. Enter GIEnsemformerCADx, an innovative hybrid approach poised to revolutionise early colorectal cancer detection. This diagnostic juggernaut offers a comprehensive solution by combining the formidable capabilities of vision transformers, fusion CNNs, and bidirectional LSTM models. Vision transformers derive high-level features from transformed data representations, whereas Fusion CNNs interpret complex spatial correlations within input images. The bidirectional LSTM model complements these advantages by enhancing the understanding of temporal relationships, resulting in an accurate and timely diagnosis of colorectal cancer. The Hyper Kvasir dataset was meticulously calibrated and rebalanced for training purposes, resulting in an optimised training corpus consisting of nine classes extracted from the original 23. The ten-class mixed CKHK-22 dataset was then subjected to rigorous evaluation, confirming the reliability of this method. Using well-known CNN architectures, such as AlexNet, DarkNet-19, ResNet-50, and DenseNet-201, within the CADx system, novel CNN fusion models (ADaDR-22, ADaR-22, and DaRD-22) were created by fusing these pre-trained CNNs. In identifying colorectal cancer, the DaRD-22 model outperformed its competitors, with a remarkable accuracy rate of 93.3% for Hyper Kvasir and 91.67% for the CKHK-22 datasets. GIEnsemformerCADx represents a major advancement in computer-aided colorectal cancer detection. Utilizing hybrid innovation and propelled by the exceptional performance of the DaRD-22 model, it promises to improve patient outcomes and reduce mortality rates through early detection and prompt intervention. In the ever-present battle against colorectal cancer, this innovative system is a beacon of hope and progress.	[Raju, Akella S. Narasimha; Reddy, G. Sucharitha] Inst Aeronaut Engn, Dept Comp Sci & Engn Data Sci, Hyderabad 500043, India; [Venkatesh, K.] SRM Inst Sci & Technol, Sch Comp, Dept Networking & Commun, Chennai 603203, Tamilnadu, India; [Padmaja, B.] Inst Aeronaut Engn, Dept Comp Sci & Engn, AI&ML, Hyderabad 500043, India	SRM Institute of Science & Technology Chennai	Raju, ASN (corresponding author), Inst Aeronaut Engn, Dept Comp Sci & Engn Data Sci, Hyderabad 500043, India.	a.raju@iare.ac.in	G, sucharitha/AAT-5202-2021; S Narasimha Raju, Akella/GYU-4605-2022; K, Venkatesh/ABE-8825-2021; b, padmaja/AAG-9690-2020	G, Sucharitha/0000-0002-3356-350X; S Narasimha Raju, Akella/0000-0002-3237-2859; K, Venkatesh/0000-0002-2895-0839; B, Padmaja/0000-0001-5327-9795	SRM Institute of Science and Technology in Kattanlkalatur	SRM Institute of Science and Technology in Kattanlkalatur	All of the funding for this endeavor comes from inside the organization. The SRM Institute of Science and Technology in Kattanlkalatur has awarded a fellowship to one of their own in order to fund the whole of this endeavor.	"Americal cancer Society,"ACS, ABOUT US; [Anonymous], 2022, ABOUT US; Attallah O, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.423; Banik D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3015607; Cao RH, 2020, IEEE INTERNET THINGS, V7, P1641, DOI 10.1109/JIOT.2019.2946296; Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041; Fonollà R, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10155040; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Igarashi S, 2020, COMPUT BIOL MED, V124, DOI 10.1016/j.compbiomed.2020.103950; Joseph RK, 2016, CRIT POL ECON S ASIA, P1; kaggle, US; Kudo SE, 2021, TRANSL GASTROENT HEP, V6, DOI 10.21037/tgh.2019.12.14; Li KD, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0255809; Liew WS, 2022, Lecture Notes in Electrical Engineering, V758, DOI [10.1007/978-981-16-2183-3_71, DOI 10.1007/978-981-16-2183-3_71]; Mathur P, 2020, JCO GLOB ONCOL, V6, P1063, DOI 10.1200/GO.20.00122; Meng YD, 2022, IEEE T MED IMAGING, V41, P690, DOI 10.1109/TMI.2021.3123567; Mitsala A, 2021, CURR ONCOL, V28, P1581, DOI 10.3390/curroncol28030149; Raju ANS, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/4325412; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Nisha JS, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103465; Oza P, 2022, J IMAGING, V8, DOI 10.3390/jimaging8050141; Öztürk S, 2021, J BIOMED INFORM, V113, DOI 10.1016/j.jbi.2020.103638; Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018; Rahim T, 2021, BIOMED SIGNAL PROCES, V68, DOI 10.1016/j.bspc.2021.102654; Raju ASN, 2023, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-08859-5; Raju Akella S. Narasimha, 2022, Computational and Mathematical Methods in Medicine; Raju ASN, 2023, BIOENGINEERING-BASEL, V10, DOI 10.3390/bioengineering10060738; Raju AS., 2022, Int J Elect Comput Eng, V12, P738; Sharma P, 2022, FRONT GENET, V13, DOI 10.3389/fgene.2022.844391; Simapro, About us; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tarver T, 2012, J CONS HLTH INTERNET, V16, P366, DOI 10.1080/15398285.2012.701177; Tripathi S, 2021, COMPUT MED IMAG GRAP, V87, DOI 10.1016/j.compmedimag.2020.101838; Wang YK, 2021, CANCERS, V13, DOI 10.3390/cancers13020321; Wu HF, 2018, J ELECTROMYOGR KINES, V42, P136, DOI 10.1016/j.jelekin.2018.07.005; Wu WT, 2020, COMPUT MATH METHOD M, V2020, DOI 10.1155/2020/6789306	36	0	0	3	3	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	1380-7501	1573-7721		MULTIMED TOOLS APPL	Multimed. Tools Appl.	2024 FEB 22	2024										10.1007/s11042-024-18521-4	http://dx.doi.org/10.1007/s11042-024-18521-4		FEB 2024	41	Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IT4Y1					2024-09-18	WOS:001168583600012
C	Wang, A; Wu, M; Qi, H; Shi, H; Chen, JH; Chen, YR; Luo, XB			IEEE	Wang, Ao; Wu, Ming; Qi, Hao; Shi, Hong; Chen, Jianhua; Chen, Yinran; Luo, Xiongbiao			PYRAMID TRANSFORMER DRIVEN MULTIBRANCH FUSION FOR POLYP SEGMENTATION IN COLONOSCOPIC VIDEO IMAGES	2023 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP	IEEE International Conference on Image Processing ICIP		English	Proceedings Paper	30th IEEE International Conference on Image Processing (ICIP)	OCT 08-11, 2023	Kuala Lumpur, MALAYSIA	IEEE, Inst Elect & Elect Engineers, Signal Proc Soc		Polyp segmentation; vision transformer; convolutional neural networks; colorectal cancer; colonoscopy		Colonoscopic polyp segmentation is essential and valuable to early diagnosis and treatment of colorectal cancer. It remains challenging to accurately extract these polyps due to their small sizes, irregular shapes, image artifacts, and illumination variations. This work proposes a new encoder-decoder architecture called pyramid transformer driven multibranch fusion to precisely segment different types of colorectal polyps during colonoscopy. Specifically, our architecture employs a simple, convolution-free pyramid transformer as its encoder that is a flexible and powerful feature extractor. Next, a multibranch fusion decoder is employed to reserve the detailed appearance information and fuse semantic global cues, which can deal with blurred polyp edges caused by nonuniform illumination and the shaky colonoscope. Additionally, a hybrid spatial-frequency loss function is introduced for accurate training. We evaluate our proposed architecture on colonoscopic polyp images with four types of polyps with different pathological features, with the experimental results showing that our architecture significantly outperforms other deep learning models. Particularly, our method improves the average dice similarity and intersection over union to 90.7% and 0.848, respectively.	[Wang, Ao; Wu, Ming; Qi, Hao; Chen, Yinran; Luo, Xiongbiao] Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China; [Luo, Xiongbiao] Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen 361102, Peoples R China; [Shi, Hong; Chen, Jianhua] Fujian Med Univ, Fujian Canc Hosp, Canc Hosp, Fuzhou 350014, Peoples R China	Xiamen University; Xiamen University; Fujian Medical University	Luo, XB (corresponding author), Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China.; Luo, XB (corresponding author), Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen 361102, Peoples R China.; Shi, H (corresponding author), Fujian Med Univ, Fujian Canc Hosp, Canc Hosp, Fuzhou 350014, Peoples R China.		Luo, Xiong/P-4343-2016		National Natural Science Foundation of China [61971367]; Natural Science Foundation of Fujian Province of China [2020J01004]; Fujian Provincial Technology Innovation Joint Funds [2019Y9091]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Fujian Province of China(Natural Science Foundation of Fujian Province); Fujian Provincial Technology Innovation Joint Funds	This work was supported partly by the National Natural Science Foundation of China under Grant 61971367, the Natural Science Foundation of Fujian Province of China under Grant 2020J01004, and the Fujian Provincial Technology Innovation Joint Funds under Grant 2019Y9091.	[Anonymous], 2021, P 29 ACM INT C MULT, DOI DOI 10.1109/SNPDWINTER52325.2021.00044; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Guo XQ, 2021, IEEE T MED IMAGING, V40, P1134, DOI 10.1109/TMI.2020.3046843; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsiang Huang C., 2021, ARXIV210107172; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Jiang LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13899, DOI 10.1109/ICCV48922.2021.01366; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Van Leemput K, 2001, IEEE T MED IMAGING, V20, P677, DOI 10.1109/42.938237; Vaswani A, 2017, ADV NEUR IN, V30; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1	15	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-7281-9835-4	IEEE IMAGE PROC			2023							2350	2354		10.1109/ICIP49359.2023.10223054	http://dx.doi.org/10.1109/ICIP49359.2023.10223054			5	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BW1IU					2024-09-18	WOS:001106821002088
C	Pei, ZX; Zhang, DQ; Shao, W			IEEE	Pei, Zongxiang; Zhang, Daoqiang; Shao, Wei			Efficient Metric Learning with Graph Transformer for Accurate Colorectal Cancer Staging	2022 IEEE-EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS (BHI) JOINTLY ORGANISED WITH THE IEEE-EMBS INTERNATIONAL CONFERENCE ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS (BSN'22)			English	Proceedings Paper	4th IEEE-EMBS International Conference on Wearable and Implantable Body Sensor Networks (BSN) / 18th IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)	SEP 27-30, 2022	Ioannina, GREECE	IEEE Engn Med & Biol, IEEE, IEEE Digital Real, IEEE Digital Privacy, Biomed Res Inst, Fdn Res & Technol Hellas, Med Technol & Intelligent Informat Syst, Rizarios Fdn, IEEE Journal Biomed Hlth Informat, IEEE Open Journal Engn Med & Biol, Gen Secretariat Reg Epirus, Municipal Ioannina, Ephorate Antiquities Ioannina, Lyceum Club Greek Women, IEEE Future Direct		Histopathology; Metric learning; Transformer; GCN		Colorectal cancer (CRC) is the third leading cause of cancer death in men and the third leading cause of cancer death in women in United States. So far, the histopathological image remains the golden standard in staging CRC, and accurate staging CRC is important for timely therapy and possible delay of the disease. Existing studies often utilized the pre-trained deep models to extract features from histopathological images, which neglected to take the supervised metric information into consideration. In addition, most of the existing methods did not take advantages of the correlations among different samples for the downstream classification tasks. To address the aforementioned problems, in this paper, we propose an efficient Metric learning with Graph Transformer (MGT), which adopts efficient metric learning to help extract distinguished image features followed by applying graph transformer for CRC staging. The main advantage of the proposed graph transformer is that it can fully exploit the correlations among different patients, which results in better tumor staging performance. To evaluate the effectiveness of the proposed method, we conduct several experiments for CRC staging on public available dataset TCGA-CRC in The Cancer Genome Atlas (TCGA). The experimental results show that our method can consistently achieve superior classification performance than the comparing methods.	[Pei, Zongxiang; Zhang, Daoqiang; Shao, Wei] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China	Nanjing University of Aeronautics & Astronautics	Pei, ZX (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China.	zongxiang.pei@nuaa.edu.cn; dqzhang@nuaa.edu.cn; shaowei20022005@nuaa.edu.cn	Shao, Weishi/HQJ-3627-2023; Zhang, Daoqiang/D-3754-2011		National Natural Science Foundation of China [62136004, 61902183]; National Key RD Program of China [2018YFC2001600, 2018YFC2001602, 2018YFA0701703]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key RD Program of China	This work was supported by National Natural Science Foundation of China (Nos. 62136004, 61902183), and the National Key RD Program of China (Grant Nos.: 2018YFC2001600, 2018YFC2001602 and 2018YFA0701703).	Aatresh AA, 2021, COMPUT MED IMAG GRAP, V93, DOI 10.1016/j.compmedimag.2021.101975; [Anonymous], 2008, RUBINS PATHOLOGY CLI; Barker J, 2016, MED IMAGE ANAL, V30, P60, DOI 10.1016/j.media.2015.12.002; cancer, CANC TYP COL CANC ST; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dif N, 2022, APPL INTELL, V52, P358, DOI 10.1007/s10489-021-02425-z; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang H, 2022, INT J IMAG SYST TECH, V32, P209, DOI 10.1002/ima.22618; Ilse M, 2018, PR MACH LEARN RES, V80; Katzman J, 2017, Arxiv, DOI arXiv:1606.00931; Kausar T, 2021, IEEE ACCESS, V9, P905, DOI 10.1109/ACCESS.2020.3044625; Kingma DP, 2014, ADV NEUR IN, V27; Lafarge MW, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101849; Lee J, 2019, PR MACH LEARN RES, V97; Lee K, 2021, FRONT ARTIF INTELL, V4, DOI 10.3389/frai.2021.754641; Mercan C, 2021, IEEE J BIOMED HEALTH, V25, P2041, DOI 10.1109/JBHI.2020.3036734; Kipf TN, 2017, Arxiv, DOI [arXiv:1609.02907, DOI 10.48550/ARXIV.1609.02907]; Vaswani A, 2017, ADV NEUR IN, V30; Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903; Vinyals O, 2016, Arxiv, DOI arXiv:1511.06391; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810; Wang HY, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-310; Wang P, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103400; Xue Y, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101816; Yao JW, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101789; Yao JW, 2019, LECT NOTES COMPUT SC, V11764, P496, DOI 10.1007/978-3-030-32239-7_55; Yu KH, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12474; Yuan YY, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3004330; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhu XL, 2016, IEEE INT C BIOINFORM, P544, DOI 10.1109/BIBM.2016.7822579; Zhu YT, 2014, NAT METHODS, V11, P599, DOI 10.1038/nmeth.2956	34	0	0	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-6654-8791-7				2022										10.1109/BHI56158.2022.9926858	http://dx.doi.org/10.1109/BHI56158.2022.9926858			6	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Medical Informatics	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Medical Informatics	BU4DA					2024-09-18	WOS:000895865900049
J	Cen, M; Li, XY; Guo, BW; Jonnagaddala, J; Zhang, H; Xu, XS				Cen, Min; Li, Xingyu; Guo, Bangwei; Jonnagaddala, Jitendra; Zhang, Hong; Xu, Xu Steven			A Novel and Efficient Digital Pathology Classifier for Predicting Cancer Biomarkers Using Sequencer Architecture	AMERICAN JOURNAL OF PATHOLOGY			English	Article							PROSTATE-CANCER; BIOPSIES	In digital pathology tasks, transformers have achieved state-of-the-art results, surpassing convolutional neural networks (CNNs). However, transformers are usually complex and resource intensive. This study developed a novel and efficient digital pathology classifier called DPSeq to predict cancer biomarkers through fine-tuning a sequencer architecture integrating horizontal and vertical bidirectional long short-term memory networks. Using hematoxylin and eosin-stained histopathologic images of colorectal cancer from two international data sets (The Cancer Genome Atlas and Molecular and Cellular Oncology), the predictive performance of DPSeq was evaluated in a series of experiments. DPSeq demonstrated exceptional performance for predicting key biomarkers in colorectal cancer (microsat-ellite instability status, hypermutation, CpG island methylator phenotype status, BRAF mutation, TP53 mutation, and chromosomal instability), outperforming most published state-of-the-art classifiers in a within-cohort internal validation and a cross-cohort external validation. In addition, under the same experimental conditions using the same set of training and testing data sets, DPSeq surpassed four CNNs (ResNet18, ResNet50, MobileNetV2, and EfficientNet) and two transformer (Vision Transformer and Swin Transformer) models, achieving the highest area under the receiver operating characteristic curve and area under the precision-recall curve values in predicting microsatellite instability status, BRAF mutation, and CpG island methylator phenotype status. Furthermore, DPSeq required less time for both training and prediction because of its simple architecture. Therefore, DPSeq appears to be the preferred choice over transformer and CNN models for predicting cancer biomarkers. (Am J Pathol 2023, 193: 2122-2132; https://doi.org/10.1016/j.ajpath.2023.09.006)	[Cen, Min; Guo, Bangwei] Univ Sci & Technol China, Sch Data Sci, Hefei, Peoples R China; [Li, Xingyu; Zhang, Hong] Univ Sci & Technol China, Sch Management, Dept Stat & Finance, Hefei, Peoples R China; [Jonnagaddala, Jitendra] Univ New South Wales, Sch Populat Hlth, Sydney, NSW, Australia; [Xu, Xu Steven] Genmab Inc, Clin Pharmacol & Quantitat Sci, Princeton, NJ USA; [Zhang, Hong] Univ Sci & Technol China, Sch Management, Dept Stat & Finance, Bldg Management Acad,East Campus USTC,96 Jinzhai R, Hefei 230026, Anhui, Peoples R China; [Xu, Xu Steven] Genmab Inc, Clin Pharmacol & Quantitat Sci, 777 Scudders Mill Rd,Bldg 2,4th Fl, Plainsboro, NJ 08536 USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Chinese Academy of Sciences; University of Science & Technology of China, CAS; University of New South Wales Sydney; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Zhang, H (corresponding author), Univ Sci & Technol China, Sch Management, Dept Stat & Finance, Bldg Management Acad,East Campus USTC,96 Jinzhai R, Hefei 230026, Anhui, Peoples R China.; Xu, XS (corresponding author), Genmab Inc, Clin Pharmacol & Quantitat Sci, 777 Scudders Mill Rd,Bldg 2,4th Fl, Plainsboro, NJ 08536 USA.	zhangh@ustc.edu.cn; sxu@genmab.com	Jonnagaddala, Jitendra/F-7372-2015; xingyu, li/HSG-5924-2023; xu, xu/KCL-3979-2024	Xu, Xu Steven/0000-0001-6997-5533; li, xingyu/0000-0001-7813-2511	National Natural Science Foundation of China [72091212, 12171451]; Anhui Center for Applied Mathematics; Australian National Health and Medical Research Council [GNT1192469]; Google Cloud Research [GCP19980904]; NVIDIA Academic Hardware grant programs; Research Technology Services at University of New South Wales	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Anhui Center for Applied Mathematics; Australian National Health and Medical Research Council(National Health & Medical Research Council (NHMRC) of Australia); Google Cloud Research; NVIDIA Academic Hardware grant programs; Research Technology Services at University of New South Wales	Supported in part by the National Natural Science Foundation of China numbers 72091212 and 12171451 (M.C., X.L., B.G., and H.Z.) ; Anhui Center for Applied Mathematics (M.C., X.L., B.G., and H.Z.) ; the Australian National Health and Medical Research Council number GNT1192469 (J.J.) ; the Research Technology Services at University of New South Wales (J.J.) ; Google Cloud Research award number GCP19980904 (J.J.) ; and NVIDIA Academic Hardware grant programs (J.J.) .	Aslan MF, 2023, COMPUT ELECTR ENG, V105, DOI 10.1016/j.compeleceng.2022.108562; BenTaieb A, 2019, Arxiv, DOI arXiv:1910.12329; Bilal M, 2021, LANCET DIGIT HEALTH, V3, pE763, DOI 10.1016/S2589-7500(21)00180-1; Bulten W, 2020, LANCET ONCOL, V21, P233, DOI 10.1016/S1470-2045(19)30739-9; Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Dubey K, 2019, Arxiv, DOI arXiv:1907.10370; Echle A, 2022, ESMO OPEN, V7, DOI 10.1016/j.esmoop.2022.100400; Echle A, 2020, GASTROENTEROLOGY, V159, P1406, DOI 10.1053/j.gastro.2020.06.021; Guo BW, 2023, J PATHOL CLIN RES, V9, P223, DOI 10.1002/cjp2.312; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KL, 2023, INTEL MED, V3, P59, DOI 10.1016/j.imed.2022.07.002; Jonnagaddala J, 2016, STUD HEALTH TECHNOL, V225, P387, DOI 10.3233/978-1-61499-658-3-387; Kather JN, 2020, NAT CANCER, V1, P789, DOI 10.1038/s43018-020-0087-6; Laleh NG, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102474; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Lu MY, 2021, NAT BIOMED ENG, V5, P555, DOI 10.1038/s41551-020-00682-w; Macenko M, 2009, I S BIOMED IMAGING, P1107, DOI 10.1109/ISBI.2009.5193250; Mehta S, 2022, Arxiv, DOI arXiv:2110.02178; Pinckaers H, 2021, IEEE T MED IMAGING, V40, P1817, DOI 10.1109/TMI.2021.3066295; Reisenbüchler D, 2022, LECT NOTES COMPUT SC, V13432, P377, DOI 10.1007/978-3-031-16434-7_37; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shaban M, 2020, IEEE T MED IMAGING, V39, P2395, DOI 10.1109/TMI.2020.2971006; Shao ZC, 2021, ADV NEUR IN; Strom P, 2020, LANCET ONCOL, V21, P222, DOI 10.1016/S1470-2045(19)30738-7; Tan MX, 2019, PR MACH LEARN RES, V97; Tatsunami Y, 2022, Arxiv, DOI [arXiv:2205.01972, DOI 10.48550/ARXIV.2205.01972]; Tripathi S, 2021, COMPUT MED IMAG GRAP, V87, DOI 10.1016/j.compmedimag.2020.101838; Vaswani A, 2017, ADV NEUR IN, V30; Wang XL, 2019, INT CONF MEASURE, P1, DOI [10.1109/ICMIC48233.2019.9068567, 10.1109/TCYB.2019.2935141]; Ward RL., 2015, Molecular and cellular oncology (mco) study tumour collection; Yao HD, 2019, CANCERS, V11, DOI 10.3390/cancers11121901; Zhai X., 2022, PROC CVPR IEEE, P12104, DOI [DOI 10.1109/CVPR52688.2022.01179, 10.1109/CVPR52688.2022.01179]; Zheng YS, 2023, IEEE T MED IMAGING, V42, P2726, DOI 10.1109/TMI.2023.3264781; Zhu MZ, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79715-y	35	0	0	1	5	ELSEVIER SCIENCE INC	NEW YORK	STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA	0002-9440	1525-2191		AM J PATHOL	Am. J. Pathol.	DEC	2023	193	12					2122	2132		10.1016/j.ajpath.2023.09.006	http://dx.doi.org/10.1016/j.ajpath.2023.09.006		NOV 2023	11	Pathology	Science Citation Index Expanded (SCI-EXPANDED)	Pathology	CH2B2	37775043	Bronze			2024-09-18	WOS:001124286100001
C	Lee, JC; Kwak, JT			IEEE	Lee, Ju Cheon; Kwak, Jin Tae			Order-ViT: Order Learning Vision Transformer for Cancer Classification in Pathology Images	2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS, ICCVW	IEEE International Conference on Computer Vision Workshops		English	Proceedings Paper	IEEE/CVF International Conference on Computer Vision (ICCV)	OCT 02-06, 2023	Paris, FRANCE	IEEE, IEEE Comp Soc, CVF			NETWORK	In computational pathology, cancer classification is one of the most widely studied tasks. There exist numerous tools for cancer classification, which are mainly built based upon convolutional neural networks or Transformers. These tools, by and large, formulate cancer classification as a categorical classification problem, which ignores the intrinsic relationship among cancer grades. Herein, we propose an order learning vision transformer for cancer classification that can not only learn the histopathological patterns of individual cancer grades but also utilize the ordering relationship among cancer grades. Built based upon vision transformer, the proposed method simultaneously conducts categorical classification per input sample and order classification for a pair of input and reference samples. Moreover, it introduces a voting scheme to identify less confident samples and to improve the accuracy of the decision on such samples. The proposed method is evaluated on two types of cancer datasets including colorectal and gastric cancers. Experimental results show that the proposed method outperforms other classification models and can facilitate improved cancer diagnosis in clinics.	[Lee, Ju Cheon; Kwak, Jin Tae] Korea Univ, Sch Elect Engn, Seoul, South Korea	Korea University	Lee, JC (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.	dlwncjs0618@korea.ac.kr; jkwak@korea.ac.kr			National Research Foundation of Korea (NRF) [2021R1A2C2014557]; Institute of Information & communications Technology Planning & evaluation (IITP) [RS-2022-00167143]; Korea government (MSIT)	National Research Foundation of Korea (NRF)(National Research Foundation of Korea); Institute of Information & communications Technology Planning & evaluation (IITP)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); Korea government (MSIT)(Ministry of Science & ICT (MSIT), Republic of Korea)	This work was supported by the grant of the National Research Foundation of Korea (NRF) (No. 2021R1A2C2014557) and Institute of Information & communications Technology Planning & evaluation (IITP) (No. RS-2022-00167143), funded by the Korea government (MSIT).	Bandara WGC, 2022, INT GEOSCI REMOTE SE, P207, DOI 10.1109/IGARSS46834.2022.9883686; Bejnordi BE, 2018, MODERN PATHOL, V31, P1502, DOI 10.1038/s41379-018-0073-z; Bejnordi BE, 2017, JAMA-J AM MED ASSOC, V318, P2199, DOI 10.1001/jama.2017.14585; Bino Sebastian V, 2012, ARXIV12054831; Bray F, 2021, CANCER-AM CANCER SOC, V127, P3029, DOI 10.1002/cncr.33587; Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1; Carion N., 2020, EUR C COMP VIS, P213; Chun-Lin L., 2010, NTUEE, Taiwan, V21, P22; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Z, 2022, INFORM SCIENCES, V608, P1093, DOI 10.1016/j.ins.2022.06.091; Howard A, 2017, ARXIV, P1; Huang G., 2017, ARXIV160806993V5, P4700, DOI [DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243]; Ikromjanov K, 2022, 2022 INT C ART INT I, P399, DOI [DOI 10.1109/ICAIIC54071.2022.9722635, 10.1109/ICAIIC54071.2022.9722635]; Irshad Humayun, 2014, IEEE Rev Biomed Eng, V7, P97, DOI 10.1109/RBME.2013.2295804; Jain J, 2023, PROC CVPR IEEE, P2989, DOI 10.1109/CVPR52729.2023.00292; Kassani SH, 2022, INT J MED INFORM, V159, DOI 10.1016/j.ijmedinf.2021.104669; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li H, 2022, COMPUT BIOL MED, V151, DOI 10.1016/j.compbiomed.2022.106265; Lim Kyungsun, 2020, INT C LEARN REPR; Liu WL, 2022, PATTERN RECOGN, V130, DOI [10.1016/j.patcog.2020.108829, 10.1016/j.patcog.2022.108829]; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Movellan J. R., 2002, Open Source Document, V49, P1; Pietikainen M., 2010, Scholarpedia, V5, P9775, DOI DOI 10.4249/SCHOLARPEDIA.9775.REVISION#188481; Sarwinda D, 2021, PROCEDIA COMPUT SCI, V179, P423, DOI 10.1016/j.procs.2021.01.025; Shin NH, 2022, PROC CVPR IEEE, P18739, DOI 10.1109/CVPR52688.2022.01820; Sirinukunwattana K, 2017, MED IMAGE ANAL, V35, P489, DOI 10.1016/j.media.2016.08.008; Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717; Tan MX, 2019, PR MACH LEARN RES, V97; Touvron H, 2022, LECT NOTES COMPUT SC, V13684, P516, DOI 10.1007/978-3-031-20053-3_30; Vuong TTL, 2022, IEEE J BIOMED HEALTH, V26, P1152, DOI 10.1109/JBHI.2021.3099817; Vuong Trinh Thi Le, 2021, MED IMAGE ANAL, V73; Wang XY, 2022, MED IMAGE ANAL, V81, DOI 10.1016/j.media.2022.102559; Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379; Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102; ZHANG HW, 2021, MED IM COMP COMP A 8, V24, P181; Zhang QQ, 2018, IEEE INT CONF HEALT, P199, DOI 10.1109/ICHI.2018.00030; Zhao YX, 2022, MULTIMED TOOLS APPL, V81, P11717, DOI 10.1007/s11042-022-12258-8; Zhilong Lv, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P491, DOI 10.1109/BIBM52615.2021.9669445; Zhou B., 2017, PROC CVPR IEEE, P633, DOI DOI 10.1109/CVPR.2017.544; Zidan U, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119452; Zou Y, 2022, INT J IMAG SYST TECH, V32, P266, DOI 10.1002/ima.22628; Zuluaga-Gomez J, 2021, COMP M BIO BIO E-IV, V9, P131, DOI 10.1080/21681163.2020.1824685	43	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	2473-9936		979-8-3503-0744-3	IEEE INT CONF COMP V			2023							2485	2494		10.1109/ICCVW60793.2023.00263	http://dx.doi.org/10.1109/ICCVW60793.2023.00263			10	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Imaging Science & Photographic Technology	BW4XE					2024-09-18	WOS:001156680302057
C	Mandujano-Cornejo, V; Montoya-Zegarra, JA		Yang, G; Aviles-Rivero, A; Roberts, M; Schonlieb, CB		Mandujano-Cornejo, Vittorino; Montoya-Zegarra, Javier A.			Polyp2Seg: Improved Polyp Segmentation with Vision Transformer	MEDICAL IMAGE UNDERSTANDING AND ANALYSIS, MIUA 2022	Lecture Notes in Computer Science		English	Proceedings Paper	26th Annual Conference on Medical Image Understanding and Analysis (MIUA)	JUL 27-29, 2022	Univ Cambridge, Cambridge, ENGLAND		Univ Cambridge	Colorectal cancer; Colonoscopy; Automatic polyp segmentation		Colorectal cancer (CRC) is the third most common type of cancer worldwide. It can be prevented by screening the colon and detecting polyps which might become malign. Therefore, an accurate detection/segmentation of polyps in colonoscopy images is crucial for CRC prevention. In this paper, we propose a novel transformer-based architecture for polyp image segmentation named Polyp2Seg. The model adopts a transformer architecture as its encoder to extract multi-hierarchical features. Additionally, a novel Feature Aggregation Module (FAM) merges progressively the multi-level features from the encoder to better localise polyps by adding semantic information. Next, a Multi-Context Attention Module (MCAM) removes noise and other artifacts, while incorporating a multi-scale attention mechanism to further improve polyp detections. Quantitative and qualitative experiments on five challenging datasets and over 5 different SOTAs demonstrate that our method significantly improves the segmentation accuracy of Polyps under different evaluation metrics. Our model achieves a new state-of-the-art over most of the datasets.	[Mandujano-Cornejo, Vittorino; Montoya-Zegarra, Javier A.] UCSP, Dept Comp Sci, Arequipa, Peru; [Montoya-Zegarra, Javier A.] Lucerne Univ Appl Sci & Arts, Luzern, Switzerland	Universidad Catolica San Pablo	Mandujano-Cornejo, V (corresponding author), UCSP, Dept Comp Sci, Arequipa, Peru.	vittorino.mandujano@ucsp.edu.pe; javier.montoya@hslu.ch		Montoya, Javier/0000-0002-3652-1954				Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197; Baheti B, 2020, IEEE COMPUT SOC CONF, P1473, DOI 10.1109/CVPRW50498.2020.00187; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Bray F, 2020, CA-CANCER J CLIN, V70, P313, DOI 10.3322/caac.21609; Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9; Chao P, 2019, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2019.00365; Chen X., 2021, arXiv; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Dosovitskiy A., 2021, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Ganz M, 2012, IEEE T BIO-MED ENG, V59, P2144, DOI 10.1109/TBME.2012.2195314; Glorot X., 2011, 14 INT C ARTIF INTEL, P315; Gross S., 2009, MANUEL KENNEL; Hatamizadeh A., 2021, Unetr: Transformers for 3d medical image segmentation, P574; He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]; Huang C-H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.07172; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Jing JF, 2022, TEXT RES J, V92, P30, DOI 10.1177/0040517520928604; Jorge Bernal J.S., 2012, AUTOMATIC POLYP DETE; Liu Z., 2022, arXiv, DOI DOI 10.48550/ARXIV.2201.03545; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long JAT, 2015, Arxiv, DOI arXiv:1411.4038; Loshchilov I., 2019, 7 INT C LEARN REPR N; Maghsoudi OH, 2017, IEEE SIGNAL PROCESSI; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Oktay O., 2018, Medical Imaging with Deep Learning, DOI DOI 10.48550/ARXIV; Pan Z., 2021, arXiv; Patel K., 2021, ENHANCED U NET FEATU; Petit O., 2021, U-net transformer: Self and cross attention for medical image segmentation; Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Sun X., 2019, Colorectal Polyp Segmentation by U-Net with Dilation Convolution; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tan MX, 2020, Arxiv, DOI [arXiv:1905.11946, DOI 10.48550/ARXIV.1905.11946]; Vaswani A, 2023, Arxiv, DOI [arXiv:1706.03762, DOI 10.48550/ARXIV.1706.03762]; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang WH, 2023, Arxiv, DOI arXiv:2106.13797; Wang WH, 2021, Arxiv, DOI [arXiv:2102.12122, DOI 10.48550/ARXIV.2102.12122]; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wei J, 2021, Arxiv, DOI arXiv:2108.00882; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xie SN, 2017, Arxiv, DOI arXiv:1611.05431; Yin ZJ, 2022, Arxiv, DOI arXiv:2103.06725; Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055; Zhang P., 2021, PROC IEEE INT C COMP, P2998; Zhao XQ, 2021, Arxiv, DOI arXiv:2108.05082; Zhou ZW, 2018, Arxiv, DOI arXiv:1807.10165; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	57	4	4	0	6	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	0302-9743	1611-3349	978-3-031-12053-4; 978-3-031-12052-7	LECT NOTES COMPUT SC			2022	13413						519	534		10.1007/978-3-031-12053-4_39	http://dx.doi.org/10.1007/978-3-031-12053-4_39			16	Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Radiology, Nuclear Medicine & Medical Imaging	BU1YK					2024-09-18	WOS:000883331000039
J	Lo, CM; Yang, YW; Lin, JK; Lin, TC; Chen, WS; Yang, SH; Chang, SC; Wang, HS; Lan, YT; Lin, HH; Huang, SC; Cheng, HH; Jiang, JK; Lin, CC				Lo, Chung-Ming; Yang, Yi-Wen; Lin, Jen-Kou; Lin, Tzu-Chen; Chen, Wei-Shone; Yang, Shung-Haur; Chang, Shih-Ching; Wang, Huann-Sheng; Lan, Yuan-Tzu; Lin, Hung-Hsin; Huang, Sheng-Chieh; Cheng, Hou-Hsuan; Jiang, Jeng-Kai; Lin, Chun -Chi			Modeling the survival of colorectal cancer patients based on colonoscopic features in a feature ensemble vision transformer	COMPUTERIZED MEDICAL IMAGING AND GRAPHICS			English	Article						Colon cancer; Colonoscopy; Vision transformer; Prognosis	COLON-CANCER; FAMILY-HISTORY; TUMOR SIZE; SYSTEM; STAGE; CLASSIFICATION; PREDICTION; PROGNOSIS; DIAGNOSIS; VALIDATION	The prognosis of patients with colorectal cancer (CRC) mostly relies on the classic tumor node metastasis (TNM) staging classification. A more accurate and convenient prediction model would provide a better prognosis and assist in treatment. From May 2014 to December 2017, patients who underwent an operation for CRC were enrolled. The proposed feature ensemble vision transformer (FEViT) used ensemble classifiers to benefit the combinations of relevant colonoscopy features from the pretrained vision transformer and clinical features, including sex, age, family history of CRC, and tumor location, to establish the prognostic model. A total of 1729 colonoscopy images were enrolled in the current retrospective study. For the prediction of patient survival, FEViT achieved an accuracy of 94 % with an area under the receiver operating characteristic curve of 0.93, which was better than the TNM staging classification (90 %, 0.83) in the experiment. FEViT reduced the limited receptive field and gradient disappearance in the conventional convolutional neural network and was a relatively effective and efficient procedure. The promising accuracy of FEViT in modeling survival makes the prognosis of CRC patients more predictable and practical.	[Lo, Chung-Ming] Natl Chengchi Univ, Grad Inst Lib Informat & Archival Studies, Taipei, Taiwan; [Yang, Yi-Wen; Lin, Jen-Kou; Lin, Tzu-Chen; Chen, Wei-Shone; Yang, Shung-Haur; Chang, Shih-Ching; Wang, Huann-Sheng; Lan, Yuan-Tzu; Lin, Hung-Hsin; Huang, Sheng-Chieh; Cheng, Hou-Hsuan; Jiang, Jeng-Kai; Lin, Chun -Chi] Taipei Vet Gen Hosp, Dept Surg, Div Colon & Rectal Surg, Taipei, Taiwan; [Yang, Yi-Wen; Lin, Jen-Kou; Lin, Tzu-Chen; Chen, Wei-Shone; Yang, Shung-Haur; Chang, Shih-Ching; Wang, Huann-Sheng; Lan, Yuan-Tzu; Lin, Hung-Hsin; Huang, Sheng-Chieh; Cheng, Hou-Hsuan; Jiang, Jeng-Kai; Lin, Chun -Chi] Natl Yang Ming Chiao Tung Univ, Sch Med, Dept Surg, Taipei, Taiwan; [Yang, Shung-Haur] Natl Yang Ming Chiao Tung Univ Hosp, Dept Surg, Yilan, Taiwan; [Lin, Chun -Chi] Natl Yang Ming Chiao Tung Univ, Sch Med, Dept Surg, 201,Sec 2,Shipai Rd, Taipei 11217, Taiwan	National Chengchi University; Taipei Veterans General Hospital; National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung University	Lin, CC (corresponding author), Taipei Vet Gen Hosp, Dept Surg, Div Colon & Rectal Surg, Taipei, Taiwan.; Lin, CC (corresponding author), Natl Yang Ming Chiao Tung Univ, Sch Med, Dept Surg, Taipei, Taiwan.; Lin, CC (corresponding author), Natl Yang Ming Chiao Tung Univ, Sch Med, Dept Surg, 201,Sec 2,Shipai Rd, Taipei 11217, Taiwan.	cclin15@vghtpe.gov.tw	LI, LIXIN/KFS-0074-2024; wang, wenjing/KEH-0575-2024; Zhang, yuxuan/JXM-9935-2024; Yang, Yiwen/AAS-9985-2021; Liu, Zhenyu/G-8444-2018	Lo, Chung-Ming/0000-0001-5068-6002	Ministry of Science and Technology of Taiwan [MOST 111-2221-E-004-012]; VGHUST Joint Research Program [VGHUST112-G1-4-2]	Ministry of Science and Technology of Taiwan(Ministry of Science and Technology, Taiwan); VGHUST Joint Research Program	Chung-Ming Lo received funding from the Ministry of Science and Technology of Taiwan (MOST 111-2221-E-004-012) and from the VGHUST Joint Research Program (VGHUST112-G1-4-2) for financially supporting this study.	Alkadri S, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104770; Berzin TM, 2020, GASTROINTEST ENDOSC, V92, P951, DOI 10.1016/j.gie.2020.06.035; Brierley JD, 2016, TNM Classification of Malignant Tumours, V8th; Byrne MF, 2019, GUT, V68, P94, DOI 10.1136/gutjnl-2017-314547; Chan JA, 2008, JAMA-J AM MED ASSOC, V299, P2515, DOI 10.1001/jama.299.21.2515; Chen J, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105878; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Echle A, 2022, ESMO OPEN, V7, DOI 10.1016/j.esmoop.2022.100400; Galon J, 2014, J PATHOL, V232, P199, DOI 10.1002/path.4287; Han SL, 2003, J SURG ONCOL, V82, P241, DOI 10.1002/jso.10228; Hassan C, 2021, GASTROINTEST ENDOSC, V93, P77, DOI 10.1016/j.gie.2020.06.059; He K, 2020, BMC MED IMAGING, V20, DOI 10.1186/s12880-020-00457-4; Henrikson NB, 2015, GENET MED, V17, P702, DOI 10.1038/gim.2014.188; Hewett DG, 2012, GASTROENTEROLOGY, V143, P599, DOI 10.1053/j.gastro.2012.05.006; Hildebrand LA, 2021, CANCERS, V13, DOI 10.3390/cancers13030391; Hsu YL, 2019, INT J BIOL MARKER, V34, P47, DOI 10.1177/1724600818807164; Hu JF, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.848798; Hu KL, 2022, COMPUT BIOL MED, V147, DOI 10.1016/j.compbiomed.2022.105760; Hu S., J DIGIT IMAGING, P1; Huang B, 2016, WORLD J GASTROENTERO, V22, P6726, DOI 10.3748/wjg.v22.i29.6726; Inoue F, 2022, WORLD J GASTRO ENDOS, V14, P495, DOI 10.4253/wjge.v14.i8.495; Kanth P, 2021, BMJ-BRIT MED J, V374, DOI 10.1136/bmj.n1855; Kourou K, 2015, COMPUT STRUCT BIOTEC, V13, P8, DOI 10.1016/j.csbj.2014.11.005; Kudo SE, 1996, GASTROINTEST ENDOSC, V44, P8, DOI 10.1016/S0016-5107(96)70222-5; Lam CS, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0253890; Lea D, 2014, SCAND J GASTROENTERO, V49, P1153, DOI 10.3109/00365521.2014.950692; Lee HS, 2015, INTEST RES, V13, P332, DOI 10.5217/ir.2015.13.4.332; Lee SH, 2021, INT J CANCER, V149, P728, DOI 10.1002/ijc.33599; Li H, 2020, IEEE ACCESS, V8, P62448, DOI 10.1109/ACCESS.2020.2981496; Li KD, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0255809; Liang CW, 2021, CANCERS, V13, DOI 10.3390/cancers13225787; Liang YX, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-95828-4; Liew WS, 2021, COMPUT METH PROG BIO, V206, DOI 10.1016/j.cmpb.2021.106114; Liu Q, 2018, CANCER MANAG RES, V10, P2303, DOI 10.2147/CMAR.S165188; Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167; Lo C.-M., 2023, COMPUT METHODS PROG; Lo CM, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10081494; Lo CM, 2022, COMPUT BIOL MED, V147, DOI 10.1016/j.compbiomed.2022.105779; Lo Chung-Ming, 2021, Ultrasound Med Biol, V47, P2266, DOI 10.1016/j.ultrasmedbio.2021.03.038; Malakorn S, 2021, CLIN COLORECTAL CANC, V20, pE53, DOI 10.1016/j.clcc.2020.08.007; Matsuda T, 2015, DIGEST ENDOSC, V27, P25, DOI 10.1111/den.12451; Mori Y, 2018, ANN INTERN MED, V169, P357, DOI 10.7326/M18-0249; Namieno T, 1996, ONCOL REP, V3, P527; Nosrati V, 2022, COMPUT BIOL MED, V148, DOI 10.1016/j.compbiomed.2022.105820; Pacal I, 2021, COMPUT BIOL MED, V134, DOI 10.1016/j.compbiomed.2021.104519; Pei Q, 2022, EUR RADIOL, V32, P714, DOI 10.1007/s00330-021-08167-3; Poritz LS, 2011, SURGERY, V150, P649, DOI 10.1016/j.surg.2011.07.049; Rath T, 2016, ENDOSCOPY, V48, P557, DOI 10.1055/s-0042-102251; Saha S, 2015, AM J SURG, V209, P570, DOI 10.1016/j.amjsurg.2014.12.008; Samala RK, 2017, PHYS MED BIOL, V62, P8894, DOI 10.1088/1361-6560/aa93d4; Sano Y, 2016, DIGEST ENDOSC, V28, P526, DOI 10.1111/den.12644; Skrede OJ, 2020, LANCET, V395, P350, DOI 10.1016/S0140-6736(19)32998-8; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Turkki R, 2019, BREAST CANCER RES TR, V177, P41, DOI 10.1007/s10549-019-05281-1; van Eeghen EE, 2015, J GASTROINTEST ONCOL, V6, P605, DOI 10.3978/j.issn.2078-6891.2015.070; van Griethuysen JJM, 2017, CANCER RES, V77, pE104, DOI 10.1158/0008-5472.CAN-17-0339; Weiser MR, 2021, J CLIN ONCOL, V39, P911, DOI 10.1200/JCO.20.02553; Weiser MR, 2018, ANN SURG ONCOL, V25, P1454, DOI 10.1245/s10434-018-6462-1; Weiser MR, 2011, J CLIN ONCOL, V29, P4796, DOI 10.1200/JCO.2011.36.5080; Wu X., 2020, ACAD RADIOL, V27, P254; Xia JF, 2022, COMPUT BIOL MED, V143, DOI 10.1016/j.compbiomed.2021.105206; Yahagi M, 2016, J GASTROINTEST SURG, V20, P648, DOI 10.1007/s11605-015-3026-6; Yamashita R, 2021, LANCET ONCOL, V22, P132, DOI 10.1016/S1470-2045(20)30535-0; Yang L, 2018, EUR RADIOL, V28, P2058, DOI 10.1007/s00330-017-5146-8; Yang YF, 2017, INT J CANCER, V141, P1942, DOI 10.1002/ijc.30827; Ying ML, 2022, BMC CANCER, V22, DOI 10.1186/s12885-022-09584-3; Zauber AG, 2012, NEW ENGL J MED, V366, P687, DOI 10.1056/NEJMoa1100370	68	12	13	3	7	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0895-6111	1879-0771		COMPUT MED IMAG GRAP	Comput. Med. Imaging Graph.	JUL	2023	107								102242	10.1016/j.compmedimag.2023.102242	http://dx.doi.org/10.1016/j.compmedimag.2023.102242		MAY 2023	8	Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Radiology, Nuclear Medicine & Medical Imaging	I6DT4	37172354				2024-09-18	WOS:001003676600001
J	Tan, YC; Chen, L; Zheng, CD; Ling, H; Lai, XS				Tan, Yicai; Chen, Lei; Zheng, Chudong; Ling, Hui; Lai, Xinshan			SAEFormer: stepwise attention emphasis transformer for polyp segmentation	MULTIMEDIA TOOLS AND APPLICATIONS			English	Article; Early Access						Polyp segmentation; Deep learning; Medical image processing; Transformer	VALIDATION	Polyp segmentation in colorectal images is the most effective and necessary tool for the early detection of colorectal cancer, and deep learning has become popular for efficiently segmenting polyps. The complex morphological characteristics of polyps, such as the unclear boundary between polyps and mucosa, and the lack of training data could cause great difficulties in network fitting. Transformer-based semantic segmentation networks have achieved more promising performance than traditional convolutional neural networks. However, the dispersion of self-attention and the less accurate local feature recognition limit the further development and applications of Transformer-based networks. This paper proposes a novel Stepwise Attention Emphasis module to refocus self-attention for Transformer-based polyp segmentation in colorectal images, where a reverse fuse module is used to better fuse different levels of features. Furthermore, a new decoder network, called the densely smooth fusion decoder, is also proposed to enhance local details and provide more useful information from deep features to shallow features. Experimental comparisons are conducted, and result analysis shows that the proposed network achieves promising performance in both learning and generalization ability on public datasets.	[Tan, Yicai; Chen, Lei; Zheng, Chudong; Ling, Hui; Lai, Xinshan] Guangdong Univ Technol, Guangzhou, Peoples R China	Guangdong University of Technology	Chen, L (corresponding author), Guangdong Univ Technol, Guangzhou, Peoples R China.	yicaitan6@gmail.com; chenlei3@gdut.edu.cn; zcdlll2333@gmail.com; Hesper-L@outlook.com; laixinshan@126.com		Chen, Lei/0000-0003-1423-3481	National Natural Science Foundation of China [62006044]; National Natural Science Foundation of China [202201010377]; Programme of Science and Technology of Guangdong Province	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Programme of Science and Technology of Guangdong Province	This work was supported in part by the National Natural Science Foundation of China (62006044) and in part by the Programme of Science and Technology of Guangdong Province (202201010377).	Ahmad P, 2021, IEEE ACCESS, V9, P148384, DOI 10.1109/ACCESS.2021.3122543; AR B RS V.K SS K, 2023, Multimed Tools Appl, pp1-20; Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chan SX, 2022, MULTIMED TOOLS APPL, V81, P13275, DOI 10.1007/s11042-021-10536-5; Chang Q, 2023, SPIE, V12468; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Contributors M., 2020, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark; Dai Z, 2021, ADV NEUR IN, V34; Dawwas MF, 2014, NEW ENGL J MED, V370, P2539, DOI [10.1056/NEJMc1405329, 10.1056/NEJMoa1309086]; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Elmeslimany EM, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-16416-4; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Hansen S, 2023, MED IMAGE ANAL, V89, DOI 10.1016/j.media.2023.102870; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]; Huang SQ, 2023, PROC CVPR IEEE, P3072, DOI 10.1109/CVPR52729.2023.00300; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Kim NH, 2017, INTEST RES, V15, P411, DOI 10.5217/ir.2017.15.3.411; Lee J, 2017, MEDICINE, V96, DOI 10.1097/MD.0000000000007468; Li YT, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S0218001421570020; Liu JH, 2023, MULTIMED TOOLS APPL, V82, P19901, DOI 10.1007/s11042-022-14218-8; Liu Z, 2021, MULTIMEDIA SYST, V27, P111, DOI 10.1007/s00530-020-00709-x; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lou AE, 2022, Arxiv, DOI arXiv:2108.07368; Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959; Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/embc.2019.8857339, 10.1109/EMBC.2019.8857339]; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Oktay O, 2018, Arxiv, DOI arXiv:1804.03999; Owen CG, 2009, INVEST OPHTH VIS SCI, V50, P2004, DOI 10.1167/iovs.08-3018; Raghu M, 2021, ADV NEUR IN, V34; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Safarov S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041441; Sanderson E, 2022, LECT NOTES COMPUT SC, V13413, P892, DOI 10.1007/978-3-031-12053-4_65; Shao H-C, 2023, IEEE J Biomed Health Inform; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tang FL, 2022, Arxiv, DOI arXiv:2212.11677; Vaswani A, 2017, ADV NEUR IN, V30; Wang JF, 2022, LECT NOTES COMPUT SC, V13433, P110, DOI 10.1007/978-3-031-16437-8_11; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wei Wang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P796, DOI 10.1007/978-3-030-59722-1_77; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Xie EZ, 2021, ADV NEUR IN, V34; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Zhang SH, 2019, LECT NOTES COMPUT SC, V11764, P797, DOI 10.1007/978-3-030-32239-7_88; Zhang X, 2021, PROC CVPR IEEE, P13951, DOI 10.1109/CVPR46437.2021.01374; Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681; Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609	55	0	0	12	19	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	1380-7501	1573-7721		MULTIMED TOOLS APPL	Multimed. Tools Appl.	2024 FEB 13	2024										10.1007/s11042-024-18515-2	http://dx.doi.org/10.1007/s11042-024-18515-2		FEB 2024	21	Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HR1K7					2024-09-18	WOS:001161140100005
J	Lima, ACD; de Paiva, LF; Bráz, G Jr; de Almeida, JDS; Silva, AC; Coimbra, MT; de Paiva, AC				de Moura Lima, Alan Carlos; de Paiva, Lisle Faray; Braz Jr, Geraldo; de Almeida, Joao Dallyson S.; Silva, Aristofanes Correa; Coimbra, Miguel Tavares; de Paiva, Anselmo Cardoso			A Two-Stage Method for Polyp Detection in Colonoscopy Images Based on Saliency Object Extraction and Transformers	IEEE ACCESS			English	Article						Colonoscopy images; deep learning; depth maps; polyp detection; saliency objects; transformers	VALIDATION	The gastrointestinal tract is responsible for the entire digestive process. Several diseases, including colorectal cancer, can affect this pathway. Among the deadliest cancers, colorectal cancer is the second most common. It arises from benign tumors in the colon, rectum, and anus. These benign tumors, known as colorectal polyps, can be diagnosed and removed during colonoscopy. Early detection is essential to reduce the risk of cancer. However, approximately 28% of polyps are lost during this examination, mainly because of limitations in diagnostic techniques and image analysis methods. In recent years, computer-aided detection techniques for these lesions have been developed to improve detection quality during periodic examinations. We proposed an automatic method for polyp detection using colonoscopy images. This study presents a two-stage polyp detection method for colonoscopy images using transformers. In the first stage, a saliency map extraction model is supported by the extracted depth maps to identify possible polyp areas. The second stage of the method consists of detecting polyps in the extracted images resulting from the first stage, combined with the green and blue channels. Several experiments were performed using four public colonoscopy datasets. The best results obtained for the polyp detection task were satisfactory, reaching 91% Average Precision in the CVC-ClinicDB dataset, 92% Average Precision in the Kvasir-SEG dataset, and 84% Average Precision in the CVC-ColonDB dataset. This study demonstrates that polyp detection in colonoscopy images can be efficiently performed using a combination of depth maps, salient object-extracted maps, and transformers.	[de Moura Lima, Alan Carlos; Braz Jr, Geraldo; de Almeida, Joao Dallyson S.; Silva, Aristofanes Correa; de Paiva, Anselmo Cardoso] Univ Fed Maranhao, NCA, BR-65085580 Sao Luis, Brazil; [de Paiva, Lisle Faray] Univ Burgundy, UFR SC, F-71200 Le Creusot, France; [Coimbra, Miguel Tavares] Univ Porto, Fac Sci, INESC TEC, P-4200465 Porto, Portugal	Universidade Federal do Maranhao; INESC TEC; Universidade do Porto	Lima, ACD (corresponding author), Univ Fed Maranhao, NCA, BR-65085580 Sao Luis, Brazil.	alanlima@nca.ufma.br	Lima Filho, Abel/KLZ-4296-2024; Braz, Geraldo/AAW-1827-2021; Silva, Aristofanes Correa/D-7957-2013; Paiva, Anselmo/L-2358-2013; Braz Junior, Geraldo/P-3851-2014	Silva, Aristofanes Correa/0000-0003-0423-2514; Faray de Paiva, Lisle/0000-0002-4594-5628; Almeida, Joao Dallyson Sousa de Almeida/0000-0001-7013-9700; Paiva, Anselmo/0000-0003-4921-0626; Coimbra, Miguel/0000-0001-7501-6523; Braz Junior, Geraldo/0000-0003-3731-6431	Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES), Brazil [001]; Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq), Brazil; Fundacao de Amparo a Pesquisa e ao Desenvolvimento Cientifico e Tecnologico do Maranhao (FAPEMA), Brazil; Portuguese funding agency, FCT-Fundacao para a Ciencia e a Tecnologia [PTDC/EEI-EEE/5557/2020]	Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES), Brazil(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq), Brazil(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)); Fundacao de Amparo a Pesquisa e ao Desenvolvimento Cientifico e Tecnologico do Maranhao (FAPEMA), Brazil(Fundacao de Amparo a Pesquisa e Desenvolvimento Cientifico do Maranhao (FAPEMA)); Portuguese funding agency, FCT-Fundacao para a Ciencia e a Tecnologia(Fundacao para a Ciencia e a Tecnologia (FCT))	& nbsp;This work was supported in part by Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES), Brazil, under Grant 001; in part by Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq), Brazil; in part by Fundacao de Amparo a Pesquisa e ao Desenvolvimento Cientifico e Tecnologico do Maranhao (FAPEMA), Brazil; and in part by National Funds through the Portuguese funding agency, FCT-Fundacao para a Ciencia e a Tecnologia, within project PTDC/EEI-EEE/5557/2020.	Ali HA, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101897; Badrinarayanan V, 2015, Arxiv, DOI arXiv:1505.07293; Bagheri M, 2019, IEEE ENG MED BIO, P6742, DOI [10.1109/EMBC.2019.8856793, 10.1109/embc.2019.8856793]; Bai Y, 2021, NEURIPS, P1; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Bochkovskiy A., 2020, P IEEE CVF C COMP VI, P722; Branch M., 2021, arXiv, DOI DOI 10.48550/ARXIV.2103.15715; Cai L., 2021, P IEEE EMBS INT C BI, P1; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13; Cheng DC, 2011, BIOMED ENG-APP BAS C, V23, P357, DOI 10.4015/S1016237211002761; COX DR, 1982, BRIT J CLIN PHARMACO, V14, P325, DOI 10.1111/j.1365-2125.1982.tb01987.x; Deeba F, 2020, BIOMED SIGNAL PROCES, V55, DOI 10.1016/j.bspc.2019.04.007; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Fang YQ, 2021, IEEE SENS J, V21, P11799, DOI 10.1109/JSEN.2020.3015831; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KL, 2023, INTEL MED, V3, P59, DOI 10.1016/j.imed.2022.07.002; Hsu H., 2014, Wiley StatsRef: Statistics, DOI [10.1002/9781118445112.stat05929, DOI 10.1002/9781118445112.STAT05929]; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Lafraxo S, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13040733; Lee JY, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-65387-1; Lin MTU, 2021, Arxiv, DOI arXiv:2012.06785; Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468; Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101; Lou A., 2023, J MED IMAG, V10, P81; Neri E., 2015, INFORM PROCESSING ME, P327; Organizacao Mundial da Saude, 2020, COLORECTAL CANCER; Qian ZQ, 2021, IEEE SENS J, V21, P11374, DOI 10.1109/JSEN.2020.3036005; Rahman MM, 2023, IEEE WINT CONF APPL, P6211, DOI 10.1109/WACV56688.2023.00616; Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196; Ratuapli Shiva K, 2014, Clin Liver Dis (Hoboken), V4, P109, DOI 10.1002/cld.433; Redmon J., 2018, arXiv; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Ronneberger O., 2015, MICCAL; Sánchez-Peralta LF, 2020, INT J COMPUT ASS RAD, V15, P1975, DOI 10.1007/s11548-020-02262-4; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tajbakhsh Nima, 2015, Inf Process Med Imaging, V24, P327, DOI 10.1007/978-3-319-19992-4_25; Taud H., 2018, Geomatic Approaches for Modeling Land Change Scenarios, P451; Thu Hong L. T., 2020, P RIVF INT C COMP CO, P1; Vaswani A, 2017, ADV NEUR IN, V30; Wan JJ, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11122264; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yuan L, 2021, Arxiv, DOI [arXiv:2101.11986, DOI 10.48550/ARXIV.2101.11986]; Zhiqiang Shen, 2021, 2021 7th International Conference on Computer and Communications (ICCC), P1757, DOI 10.1109/ICCC54389.2021.9674267	51	6	6	3	12	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2169-3536			IEEE ACCESS	IEEE Access		2023	11						76108	76119		10.1109/ACCESS.2023.3297097	http://dx.doi.org/10.1109/ACCESS.2023.3297097			12	Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Telecommunications	O0GD5		gold			2024-09-18	WOS:001040682200001
J	Fitzgerald, K; Bernal, J; Histace, A; Matuszewski, BJ				Fitzgerald, Kerr; Bernal, Jorge; Histace, Aymeric; Matuszewski, Bogdan J.			Polyp Segmentation With the FCB-SwinV2 Transformer	IEEE ACCESS			English	Article						Transformers; Feature extraction; Decoding; Convolutional neural networks; Colonoscopy; Deep learning; Training; Image segmentation; Biomedical image processing; Tumors; Colorectal cancer; Medical image processing; polyp segmentation; deep learning; SwinV2; transformer	MISS RATE; COLONOSCOPY; VALIDATION	Polyp segmentation within colonoscopy video frames using deep learning models has the potential to automate colonoscopy screening procedures. This could help improve the early lesion detection rate and in vivo characterization of polyps which could develop into colorectal cancer. Recent state-of-the-art deep learning polyp segmentation models have combined Convolutional Neural Network (CNN) architectures and Transformer Network (TN) architectures. Motivated by the aim of improving the performance of polyp segmentation models and their robustness to data variations beyond those covered during training, we propose a new CNN-TN hybrid model named the FCB-SwinV2 Transformer. This model was created by making extensive modifications to the recent state-of-the-art FCN-Transformer, including replacing the TN branch architecture with a SwinV2 U-Net. The performance of the FCB-SwinV2 Transformer is evaluated on the popular colonoscopy segmentation benchmarking datasets Kvasir-SEG, CVC-ClinicDB and ETIS-LaribPolypDB. Generalizability tests are also conducted to determine if models can maintain accuracy when evaluated on data outside of the training distribution. The FCB-SwinV2 Transformer consistently achieves higher mean Dice and mean IoU scores when compared to other models reported in literature and therefore represents new state-of-the-art performance. The importance of understanding subtleties in evaluation metrics and dataset partitioning are also demonstrated and discussed. Code available: https://github.com/KerrFitzgerald/Polyp_FCB-SwinV2Transformer	[Fitzgerald, Kerr; Matuszewski, Bogdan J.] Univ Cent Lancashire, Comp Vis & Machine Learning CVML Grp, Preston PR1 2HE, England; [Bernal, Jorge] Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona 08193, Spain; [Bernal, Jorge] Univ Autonoma Barcelona, Comp Sci Dept, Barcelona 08193, Spain; [Histace, Aymeric] CY Paris Cergy Univ, ETIS UMR 8051, ENSEA, CNRS, Cergy, France	University of Central Lancashire; Centre de Visio per Computador (CVC); Autonomous University of Barcelona; Autonomous University of Barcelona; CY Cergy Paris Universite; Centre National de la Recherche Scientifique (CNRS)	Fitzgerald, K (corresponding author), Univ Cent Lancashire, Comp Vis & Machine Learning CVML Grp, Preston PR1 2HE, England.	kffitzgerald@uclan.ac.uk	Bernal, Jorge/H-4647-2015		Science and Technology Facilities Council	Science and Technology Facilities Council(UK Research & Innovation (UKRI)Science & Technology Facilities Council (STFC))	No Statement Available	Abe M., 2022, Swin V2 Unet/Upernet; Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chang Qi, 2023, Proceedings of SPIE - Progress in Biomedical Optics and Imaging, DOI 10.1117/12.2647897; Combalia M, 2020, IEEE COMPUT SOC CONF, P3211, DOI 10.1109/CVPRW50498.2020.00380; Dawwas MF, 2014, NEW ENGL J MED, V370, P2539, DOI [10.1056/NEJMc1405329, 10.1056/NEJMoa1309086]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Dumitru RG, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-36940-5; Gal Y, 2016, PR MACH LEARN RES, V48; Guo QQ, 2022, IEEE ACCESS, V10, P52971, DOI 10.1109/ACCESS.2022.3175858; Guo YB, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070069; Hendrycks D, 2020, Arxiv, DOI [arXiv:1606.08415, 10.48550/arXiv.1606.08415]; Huang Y., 2020, 2022 IEEE INT C BIOI, P1558, DOI [DOI 10.1109/BIBM55620.2022.9995247, 10.1109/BIBM55620.2022.9995247]; Hwang M, 2020, COMPUT MED IMAG GRAP, V84, DOI 10.1016/j.compmedimag.2020.101763; Ige AO, 2023, IEEE ACCESS, V11, P16142, DOI 10.1109/ACCESS.2023.3244789; Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Kim NH, 2017, INTEST RES, V15, P411, DOI 10.5217/ir.2017.15.3.411; Lai HL, 2023, VISUAL COMPUT, V39, P1453, DOI 10.1007/s00371-022-02422-4; lakubovskii P., 2019, GitHub Repository; Lee J, 2017, MEDICINE, V96, DOI 10.1097/MD.0000000000007468; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Loshchilov I., 2019, P INT C LEARN REPR; Mandujano-Cornejo V, 2022, LECT NOTES COMPUT SC, V13413, P519, DOI 10.1007/978-3-031-12053-4_39; An NS, 2022, IEEE ACCESS, V10, P43669, DOI 10.1109/ACCESS.2022.3168693; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Qadir H. A., 2019, PROC 13 INT S MED IN, P1; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48; Sanderson E, 2022, LECT NOTES COMPUT SC, V13413, P892, DOI 10.1007/978-3-031-12053-4_65; Srivastava A, 2022, IEEE J BIOMED HEALTH, V26, P2252, DOI 10.1109/JBHI.2021.3138024; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23; Trinh QH, 2023, COMP MED SY, P742, DOI 10.1109/CBMS58004.2023.00312; Wang B. Dong. W., 2023, CAAI Artif. Intell.Res., V2; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Whitson MJ, 2012, GASTROINTEST ENDOSC, V75, P641, DOI 10.1016/j.gie.2011.10.032; who, 2023, Colorectal cancer; Wightman R., 2019, GitHub Repository; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xie E., 2021, Advances in Neural Information Processing Systems, V34, P12077, DOI DOI 10.48550/ARXIV.2105.15203; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou L, 2023, PROC INT JOINT C NEU, P1, DOI [10.1109/JCNN54540.2023.10191607, DOI 10.1109/JCNN54540.2023.10191607]	48	0	0	13	13	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2169-3536			IEEE ACCESS	IEEE Access		2024	12						38927	38943		10.1109/ACCESS.2024.3376228	http://dx.doi.org/10.1109/ACCESS.2024.3376228			17	Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Telecommunications	LW4Y8		gold			2024-09-18	WOS:001189837500001
J	Guo, BW; Li, XY; Yang, MM; Jonnagaddala, J; Zhang, H; Xu, XS				Guo, Bangwei; Li, Xingyu; Yang, Miaomiao; Jonnagaddala, Jitendra; Zhang, Hong; Xu, Xu Steven			Predicting microsatellite instability and key biomarkers in colorectal cancer from H&E-stained images: achieving state-of-the-art predictive performance with fewer data using Swin Transformer	JOURNAL OF PATHOLOGY CLINICAL RESEARCH			English	Article						digital pathology; deep learning; colorectal cancer; biomarkers; Swin Transformer	LYNCH SYNDROME; GUIDELINES; ADENOMAS; MODEL	Many artificial intelligence models have been developed to predict clinically relevant biomarkers for colorectal cancer (CRC), including microsatellite instability (MSI). However, existing deep learning networks require large training datasets, which are often hard to obtain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin Transformer [Swin-T]), we developed an efficient workflow to predict biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, and BRAF and TP53 mutation) that required relatively small datasets. Our Swin-T workflow substantially achieved the state-of-the-art (SOTA) predictive performance in an intra-study cross-validation experiment on the Cancer Genome Atlas colon and rectal cancer dataset (TCGA-CRC-DX). It also demonstrated excellent generalizability in cross-study external validation and delivered a SOTA area under the receiver operating characteristic curve (AUROC) of 0.90 for MSI, using the Molecular and Cellular Oncology dataset for training (N = 1,065) and the TCGA-CRC-DX (N = 462) for testing. A similar performance (AUROC = 0.91) was reported in a recent study, using similar to 8,000 training samples (ResNet18) on the same testing dataset. Swin-T was extremely efficient when using small training datasets and exhibited robust predictive performance with 200-500 training samples. Our findings indicate that Swin-T could be 5-10 times more efficient than existing algorithms for MSI prediction based on ResNet18 and ShuffleNet. Furthermore, the Swin-T models demonstrated their capability in accurately predicting MSI and BRAF mutation status, which could exclude and therefore reduce samples before subsequent standard testing in a cascading diagnostic workflow, in turn reducing turnaround time and costs.	[Guo, Bangwei] Univ Sci & Technol China, Sch Data Sci, Hefei, Peoples R China; [Li, Xingyu; Zhang, Hong] Univ Sci & Technol China, Sch Management, Dept Stat & Finance, Hefei, Peoples R China; [Yang, Miaomiao] Anhui Med Univ, Affiliated Hosp 1, Clin Pathol Ctr, Hefei, Peoples R China; [Jonnagaddala, Jitendra] UNSW Sydney, Sch Populat Hlth, Kensington, NSW, Australia; [Xu, Xu Steven] Genmab Inc, Clin Pharmacol & Quantitat Sci, Princeton, NJ 08540 USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Anhui Medical University; University of New South Wales Sydney	Zhang, H (corresponding author), Univ Sci & Technol China, Sch Management, Dept Stat & Finance, Hefei, Peoples R China.; Jonnagaddala, J (corresponding author), UNSW Sydney, Sch Populat Hlth, Kensington, NSW, Australia.; Xu, XS (corresponding author), Genmab Inc, Clin Pharmacol & Quantitat Sci, Princeton, NJ 08540 USA.	jitendra.jonnagaddala@unsw.edu.au; zhangh@ustc.edu.cn; sxu@genmab.com	Jonnagaddala, Jitendra/F-7372-2015; xu, xu/KCL-3979-2024; xingyu, li/HSG-5924-2023	Xu, Xu Steven/0000-0001-6997-5533; li, xingyu/0000-0001-7813-2511	National Natural Science Foundation of China [12171451, 72091212]; Anhui Center for Applied Mathematics; Google Cloud Research [GNT1192469]; NVIDIA Academic Hardware grant programs; Australian National Health and Medical Research Council; Research Technology Services at UNSW Sydney;  [GCP19980904]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Anhui Center for Applied Mathematics; Google Cloud Research; NVIDIA Academic Hardware grant programs; Australian National Health and Medical Research Council(National Health & Medical Research Council (NHMRC) of Australia); Research Technology Services at UNSW Sydney; 	The research of BG, XL, and HZ was partially supported by National Natural Science Foundation of China (No. 12171451 and No. 72091212) and Anhui Center for Applied Mathematics. JJ is funded by the Australian National Health and Medical Research Council (No. GNT1192469). JJ also acknowledges the funding support received through the Research Technology Services at UNSW Sydney, Google Cloud Research (award# GCP19980904) and NVIDIA Academic Hardware grant programs. We also would like to thank the SREDH Consortium's (, accessed on 15 November 2022) Translational Cancer Bioinformatics working group for the access to the MCO CRC dataset.	André T, 2020, NEW ENGL J MED, V383, P2207, DOI 10.1056/NEJMoa2017699; Bilal M, 2021, LANCET DIGIT HEALTH, V3, pE763, DOI 10.1016/S2589-7500(21)00180-1; Cao R, 2020, THERANOSTICS, V10, P11080, DOI 10.7150/thno.49864; Courtiol P., 2018, ARXIV; Courtiol P, 2019, NAT MED, V25, P1519, DOI 10.1038/s41591-019-0583-3; Dabir PD, 2020, EUR J HUM GENET, V28, P277, DOI 10.1038/s41431-019-0538-7; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Echle A, 2022, ESMO OPEN, V7, DOI 10.1016/j.esmoop.2022.100400; Echle A, 2020, GASTROENTEROLOGY, V159, P1406, DOI 10.1053/j.gastro.2020.06.021; Fu Y, 2020, NAT CANCER, V1, P800, DOI 10.1038/s43018-020-0085-8; Greenson JK, 2009, AM J SURG PATHOL, V33, P126, DOI 10.1097/PAS.0b013e31817ec2b1; Guo B, 2022, ARXIV; Hampel H, 2008, J CLIN ONCOL, V26, P5783, DOI 10.1200/JCO.2008.17.5950; Iino H, 2000, GUT, V47, P37, DOI 10.1136/gut.47.1.37; Jenkins MA, 2007, GASTROENTEROLOGY, V133, P48, DOI 10.1053/j.gastro.2007.04.044; Jonnagaddala J, 2016, STUD HEALTH TECHNOL, V225, P387, DOI 10.3233/978-1-61499-658-3-387; Kather JN, 2020, NAT CANCER, V1, P789, DOI 10.1038/s43018-020-0087-6; Kather JN, 2019, NAT MED, V25, P1054, DOI 10.1038/s41591-019-0462-y; Kather JN, 2019, PLOS MED, V16, DOI 10.1371/journal.pmed.1002730; Laleh, 2021, IMMUNOINFORMATICS, V3-4:; Laleh NG, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102474; Lee SH, 2021, INT J CANCER, V149, P728, DOI 10.1002/ijc.33599; Liu A.H., 2022, arXiv; Liu Y, 2018, CANCER CELL, V33, P721, DOI 10.1016/j.ccell.2018.03.010; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Macenko M, 2009, I S BIOMED IMAGING, P1107, DOI 10.1109/ISBI.2009.5193250; Marcus L, 2019, CLIN CANCER RES, V25, P3753, DOI 10.1158/1078-0432.CCR-18-4070; Schirris Y, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102464; Schmauch B, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17678-4; Schrammen PL, 2022, J PATHOL, V256, P50, DOI 10.1002/path.5800; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]; Sirinukunwattana K, 2021, GUT, V70, P544, DOI 10.1136/gutjnl-2019-319866; Stjepanovic N, 2019, ANN ONCOL, V30, P1558, DOI 10.1093/annonc/mdz233; Ward RL., 2015, Molecular and cellular oncology (mco) study tumour collection; Yamashita R, 2021, LANCET ONCOL, V22, P132, DOI 10.1016/S1470-2045(20)30535-0	35	6	7	1	16	WILEY	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA		2056-4538		J PATHOL CLIN RES	J. Pathol. Clin. Res.	MAY	2023	9	3					223	235		10.1002/cjp2.312	http://dx.doi.org/10.1002/cjp2.312		FEB 2023	13	Pathology	Science Citation Index Expanded (SCI-EXPANDED)	Pathology	C2LG1	36723384	gold, Green Published			2024-09-18	WOS:000926917400001
J	Singh, VK; Makhlouf, Y; Sarker, MMK; Craig, S; Baena, J; Greene, C; Mason, L; James, JA; Salto-Tellez, M; O'Reilly, P; Maxwell, P				Singh, Vivek Kumar; Makhlouf, Yasmine; Sarker, M. Mostafa Kamal; Craig, Stephanie; Baena, Juvenal; Greene, Christine; Mason, Lee; James, Jacqueline A.; Salto-Tellez, Manuel; O'Reilly, Paul; Maxwell, Perry			KRASFormer: a fully vision transformer-based framework for predicting <i>KRAS</i> gene mutations in histopathological images of colorectal cancer	BIOMEDICAL PHYSICS & ENGINEERING EXPRESS			English	Article						colorectal cancer; vision-transformer; KRAS gene; whole slide image; mutation classification; next generation sequencing		Detecting the Kirsten Rat Sarcoma Virus (KRAS) gene mutation is significant for colorectal cancer (CRC) patients. The KRAS gene encodes a protein involved in the epidermal growth factor receptor (EGFR) signaling pathway, and mutations in this gene can negatively impact the use of monoclonal antibodies in anti-EGFR therapy and affect treatment decisions. Currently, commonly used methods like next-generation sequencing (NGS) identify KRAS mutations but are expensive, time-consuming, and may not be suitable for every cancer patient sample. To address these challenges, we have developed KRASFormer, a novel framework that predicts KRAS gene mutations from Haematoxylin and Eosin (H & E) stained WSIs that are widely available for most CRC patients. KRASFormer consists of two stages: the first stage filters out non-tumor regions and selects only tumour cells using a quality screening mechanism, and the second stage predicts the KRAS gene either wildtype' or mutant' using a Vision Transformer-based XCiT method. The XCiT employs cross-covariance attention to capture clinically meaningful long-range representations of textural patterns in tumour tissue and KRAS mutant cells. We evaluated the performance of the first stage using an independent CRC-5000 dataset, and the second stage included both The Cancer Genome Atlas colon and rectal cancer (TCGA-CRC-DX) and in-house cohorts. The results of our experiments showed that the XCiT outperformed existing state-of-the-art methods, achieving AUCs for ROC curves of 0.691 and 0.653 on TCGA-CRC-DX and in-house datasets, respectively. Our findings emphasize three key consequences: the potential of using H & E-stained tissue slide images for predicting KRAS gene mutations as a cost-effective and time-efficient means for guiding treatment choice with CRC patients; the increase in performance metrics of a Transformer-based model; and the value of the collaboration between pathologists and data scientists in deriving a morphologically meaningful model.	[Singh, Vivek Kumar; Makhlouf, Yasmine; Craig, Stephanie; Baena, Juvenal; Greene, Christine; Mason, Lee; James, Jacqueline A.; Salto-Tellez, Manuel; Maxwell, Perry] Queens Univ Belfast, Precis Med Ctr Excellence, Patrick G Johnston Ctr Canc Res, Hlth Sci Bldg, Belfast BT9 7AE, North Ireland; [Singh, Vivek Kumar] Queen Mary Univ London, Barts Canc Inst, Ctr Biomarkers & Biotherapeut, London EC1M 6BQ, England; [Sarker, M. Mostafa Kamal] Univ Oxford, Inst Biomed Engn, Oxford OX37DQ, England; [James, Jacqueline A.; Salto-Tellez, Manuel] Belfast Hlth & Social Care Trust, Reg Mol Diagnost Serv, Belfast BT9 7AE, North Ireland; [Salto-Tellez, Manuel; O'Reilly, Paul] Sonrai Analyt, Belfast BT9 7AE, North Ireland; [Salto-Tellez, Manuel] Belfast City Hosp, Belfast Hlth & Social Care Trust, Cellular Pathol, Lisburn Rd, Belfast BT9 7AB, North Ireland	Queens University Belfast; University of London; Queen Mary University London; University of Oxford; Belfast City Hospital	Maxwell, P (corresponding author), Queens Univ Belfast, Precis Med Ctr Excellence, Patrick G Johnston Ctr Canc Res, Hlth Sci Bldg, Belfast BT9 7AE, North Ireland.	p.maxwell@qub.ac.uk	SINGH, VIVEK/AAE-2347-2022; Mason, Lee/L-4834-2018; Craig, Stephanie/AAX-2060-2020	Craig, Stephanie/0000-0002-5476-751X; Mason, Lee/0000-0002-7814-099X; Singh, Vivek Kumar/0000-0002-8259-7087	UK Research and Innovationhttps://doi.org/10.13039/100014013; HSC Research and Development Division of the Public Health Agency in Northern Ireland; Cancer Research UK; Friends of the Cancer Centre; Invest Northern Ireland; Health and Social Care Research and Development Division of the Public Health Agency in Northern Ireland; Sean Crummey Memorial Fund; Tom Simms Memorial Fund	UK Research and Innovationhttps://doi.org/10.13039/100014013; HSC Research and Development Division of the Public Health Agency in Northern Ireland; Cancer Research UK(Cancer Research UK); Friends of the Cancer Centre; Invest Northern Ireland; Health and Social Care Research and Development Division of the Public Health Agency in Northern Ireland; Sean Crummey Memorial Fund; Tom Simms Memorial Fund	The Northern Ireland Biobank has received funds from the HSC Research and Development Division of the Public Health Agency in Northern Ireland, Cancer Research UK, and the Friends of the Cancer Centre. The Precision Medicine Centre of Excellence has received funding from Invest Northern Ireland, Cancer Research UK, the Health and Social Care Research and Development Division of the Public Health Agency in Northern Ireland, the Sean Crummey Memorial Fund, and the Tom Simms Memorial Fund.	Alam MR, 2023, BRIEF BIOINFORM, V24, DOI 10.1093/bib/bbad151; Ali A., 2021, Advances in Neural Information Processing Systems, V34; Bankhead P, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-17204-5; Bao H., 2021, PREPRINT; Bilal M, 2021, LANCET DIGIT HEALTH, V3, pE763, DOI 10.1016/S2589-7500(21)00180-1; Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7; Deininger L, 2022, arXiv, DOI DOI 10.48550/ARXIV.2206.00389; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hossin M., 2015, Int. J. Data Min. Knowl. Manag. Process, V5, P1, DOI [10.5281/zenodo.3557376, 10.5121/ijdkp.2015.5201, DOI 10.5121/IJDKP.2015.5201]; Jang HJ, 2020, WORLD J GASTROENTERO, V26, P6207, DOI 10.3748/wjg.v26.i40.6207; Jiang YQ, 2022, IEEE OPEN J ENG MED, V3, P115, DOI 10.1109/OJEMB.2022.3192103; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244; Lièvre A, 2006, CANCER RES, V66, P3992, DOI 10.1158/0008-5472.CAN-06-0191; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Mao LC, 2019, IEEE ACCESS, V7, P172231, DOI 10.1109/ACCESS.2019.2956508; Megía PJ, 2021, CATAL TODAY, V367, P145, DOI 10.1016/j.cattod.2020.04.069; Morgan E, 2023, GUT, V72, P338, DOI 10.1136/gutjnl-2022-327736; Moutik O, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23020734; Ohata EF, 2021, J SUPERCOMPUT, V77, P9494, DOI 10.1007/s11227-020-03575-6; Raczkowski L, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50587-1; Salto-Tellez M., 2012, Principles of Molecular Diagnostics and Personalized Cancer Medicine, P196; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shamshad F, 2023, MED IMAGE ANAL, V88, DOI 10.1016/j.media.2023.102802; Southwood M, 2020, J PATHOL CLIN RES, V6, P40, DOI 10.1002/cjp2.145; Tan MX, 2021, PR MACH LEARN RES, V139, P7102; Tan MX, 2019, PR MACH LEARN RES, V97; Touvron H, 2023, IEEE T PATTERN ANAL, V45, P5314, DOI 10.1109/TPAMI.2022.3206148; Wagner SJ, 2023, CANCER CELL, V41, P1650, DOI 10.1016/j.ccell.2023.08.002; Wu LJ, 2022, Arxiv, DOI arXiv:2211.08543; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yao XJ, 2022, MULTIMED TOOLS APPL, V81, P41361, DOI 10.1007/s11042-020-09634-7; Zeid M A E., 2021, 10 INT C INT COMP IN, V224, P224	36	0	0	2	2	IOP Publishing Ltd	BRISTOL	TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND	2057-1976			BIOMED PHYS ENG EXPR	Biomed. Phys. Eng. Express	SEP 1	2024	10	5							055012	10.1088/2057-1976/ad5bed	http://dx.doi.org/10.1088/2057-1976/ad5bed			14	Radiology, Nuclear Medicine & Medical Imaging	Emerging Sources Citation Index (ESCI)	Radiology, Nuclear Medicine & Medical Imaging	YW7S3	38925106	hybrid			2024-09-18	WOS:001271593900001
C	Wei, XF; Zhou, X		Luo, B; Cheng, L; Wu, ZG; Li, H; Li, C		Wei, Xuefeng; Zhou, Xuan			FLDNet: A Foreground-Aware Network for Polyp Segmentation Leveraging Long-Distance Dependencies	NEURAL INFORMATION PROCESSING, ICONIP 2023, PT III	Lecture Notes in Computer Science		English	Proceedings Paper	30th International Conference on Neural Information Processing (ICONIP) of the Asia-Pacific-Neural-Network-Society (APNNS)	NOV 20-23, 2023	Changsha, PEOPLES R CHINA	Asia Pacific Neural Network Soc		Deep Learning; Polyp Segmentation; Colorectal Cancer		Given the close association between colorectal cancer and polyps, the diagnosis and identification of colorectal polyps play a critical role in the detection and surgical intervention of colorectal cancer. In this context, the automatic detection and segmentation of polyps from various colonoscopy images has emerged as a significant problem that has attracted broad attention. Current polyp segmentation techniques face several challenges: firstly, polyps vary in size, texture, color, and pattern; secondly, the boundaries between polyps and mucosa are usually blurred, existing studies have focused on learning the local features of polyps while ignoring the long-range dependencies of the features, and also ignoring the local context and global contextual information of the combined features. To address these challenges, we propose FLD-Net (Foreground-Long-Distance Network), a Transformer-based neural network that captures long-distance dependencies for accurate polyp segmentation. Specifically, the proposed model consists of three main modules: a pyramid-based Transformer encoder, a local context module, and a foreground-Aware module. Multilevel features with long-distance dependency information are first captured by the pyramid-based transformer encoder. On the high-level features, the local context module obtains the local characteristics related to the polyps by constructing different local context information. The coarse map obtained by decoding the reconstructed highest-level features guides the feature fusion process in the foreground-Aware module of the high-level features to achieve foreground enhancement of the polyps. Our proposed method, FLDNet, was evaluated using seven metrics on common datasets and demonstrated superiority over state-of-the-art methods on widely-used evaluation measures.	[Wei, Xuefeng; Zhou, Xuan] Inst Polytech Paris, F-91120 Palaiseau, France	Institut Polytechnique de Paris	Wei, XF; Zhou, X (corresponding author), Inst Polytech Paris, F-91120 Palaiseau, France.	xuefeng.wei@ip-paris.fr; xuan.zhou@ip-paris.fr						Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361; Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Huang C-H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.07172; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Kim T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2167, DOI 10.1145/3474085.3475375; Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959; Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/embc.2019.8857339, 10.1109/EMBC.2019.8857339]; Naseer M, 2021, ADV NEUR IN, V34; Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/crv52889.2021.00032, 10.1109/CRV52889.2021.00032]; Raghu M, 2021, ADV NEUR IN, V34; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Shamshad F., 2022, arXiv; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Vaswani A, 2017, ADV NEUR IN, V30; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Yin ZJ, 2022, I S BIOMED IMAGING, DOI 10.1109/ISBI52829.2022.9761402; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	25	1	1	3	5	SPRINGER-VERLAG SINGAPORE PTE LTD	SINGAPORE	152 BEACH ROAD, #21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE	0302-9743	1611-3349	978-981-99-8066-6; 978-981-99-8067-3	LECT NOTES COMPUT SC			2024	14449						477	487		10.1007/978-981-99-8067-3_35	http://dx.doi.org/10.1007/978-981-99-8067-3_35			11	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Radiology, Nuclear Medicine & Medical Imaging	BW4HN					2024-09-18	WOS:001148053300035
J	Liu, XG; Song, S				Liu, Xiaogang; Song, Shuang			Attention combined pyramid vision transformer for polyp segmentation	BIOMEDICAL SIGNAL PROCESSING AND CONTROL			English	Article						Polyp segmentation; Convolutional neural network; Pyramid vision transformer; Endoscope	COLORECTAL-CANCER; NETWORK; IMAGES; ENDOSCOPY; COLON	Colorectal cancer (CRC) has become one of the most frequent cancers in the world. To prevent CRC, proper polyp localization in endoscopy images plays a vital role for detecting and removing colorectal polyps. Most polyp segmentation methods use convolutional neural networks (CNN) as their backbone, and have achieved promising results to effectively assist clinicians in their diagnosis. However, those CNN-based approaches have limitations in modeling accurate location and shape of polyps, due to the intrinsic locality property of convolutional operations. To address these limitations, this study proposes a novel network, namely AttPVT, that combines CNN and Pyramid Vision Transformer (PVT) together for poly segmentation. The main challenge lies in maintaining long-range semantic information without sacrificing low-level features. Att-PVT applies multidimensional information extraction (MIE) to generate refined feature maps extracted from PVT for better feature representation. Cascaded context integration (CCI) is designed to adaptively aggregating the three highest layers of refined polyp features for learning semantic and location information. To accurately segment colorectal polyps, Att-PVT introduces multilevel feature fusion (MFF) module that explores the boundary information in the shallower layer based on the global map. The proposed workflow has undergone comparative experiments on three public datasets, namely Kvasir, ColonDB, and ETIS. The results show that the proposed approach achieves impressive mDice scores of 0.926, 0.817, and 0.794 for polyp segmentation tasks on these datasets, surpassing other state-of-the-art methods. This indicates the superior generalization and scalability of the proposed approach.	[Liu, Xiaogang; Song, Shuang] Nanjing Tech Univ, Coll Comp & Informat Engn, Nanjing 211800, Peoples R China	Nanjing Tech University	Liu, XG (corresponding author), Nanjing Tech Univ, Coll Comp & Informat Engn, Nanjing 211800, Peoples R China.	liuxg0201@njtech.edu.cn						Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197; Alexandre LA, 2008, INT CONF BIOMED, P38, DOI 10.1109/BMEI.2008.246; Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042; Bisschops R, 2019, ENDOSCOPY, V51, P1155, DOI 10.1055/a-1031-7657; Bokhovkin A, 2019, LECT NOTES COMPUT SC, V11555, P388, DOI 10.1007/978-3-030-22808-8_38; Bretthauer M, 2016, JAMA INTERN MED, V176, P894, DOI 10.1001/jamainternmed.2016.0960; Chen PJ, 2018, GASTROENTEROLOGY, V154, P568, DOI 10.1053/j.gastro.2017.10.010; Dawwas MF, 2014, NEW ENGL J MED, V370, P2539, DOI [10.1056/NEJMc1405329, 10.1056/NEJMoa1309086]; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698; [范登平 Fan Dengping], 2021, [中国科学. 信息科学, Scientia Sinica Informationis], V51, P1475; Gelly S., 2021, ICLR, DOI DOI 10.48550/ARXIV.2010.11929; He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Huang C-H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.07172; Hwang S, 2007, IEEE IMAGE PROC, P1029; Iakovidis DK, 2005, COMP MED SY, P575, DOI 10.1109/CBMS.2005.6; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Jia X, 2020, P IEEE, V108, P178, DOI 10.1109/JPROC.2019.2950506; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Kim T., 2021, ACM MM; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018; Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/crv52889.2021.00032, 10.1109/CRV52889.2021.00032]; Qadir HA, 2019, INT SYM MED INFORM, P181, DOI 10.1109/ismict.2019.8743694; Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196; Rex DK, 2017, GASTROENTEROLOGY, V153, P307, DOI 10.1053/j.gastro.2017.05.013; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Song PF, 2022, COMPUT BIOL MED, V146, DOI 10.1016/j.compbiomed.2022.105476; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Vaswani A, 2017, ADV NEUR IN, V30; Wang S, 2021, IEEE J BIOMED HEALTH, V25, P514, DOI 10.1109/JBHI.2020.2997760; Wang WH, 2023, Arxiv, DOI arXiv:2106.13797; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xie EZ, 2021, ADV NEUR IN, V34; Yue GH, 2022, IEEE J BIOMED HEALTH, V26, P4090, DOI 10.1109/JBHI.2022.3173948; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	51	3	3	14	25	ELSEVIER SCI LTD	London	125 London Wall, London, ENGLAND	1746-8094	1746-8108		BIOMED SIGNAL PROCES	Biomed. Signal Process. Control	MAR	2024	89								105792	10.1016/j.bspc.2023.105792	http://dx.doi.org/10.1016/j.bspc.2023.105792		NOV 2023	10	Engineering, Biomedical	Science Citation Index Expanded (SCI-EXPANDED)	Engineering	CW4M8					2024-09-18	WOS:001128258900001
J	Zhang, CS; Chen, C; Chen, C; Lv, XY				Zhang, Chengsheng; Chen, Cheng; Chen, Chen; Lv, Xiaoyi			SMiT: symmetric mask transformer for disease severity detection	JOURNAL OF CANCER RESEARCH AND CLINICAL ONCOLOGY			English	Article						Deep learning; Intelligent diagnosis; Visual transformer; Symmetric mask; Colorectal cancer; Diabetic retinopathy		PurposeThe application of deep learning methods to the intelligent diagnosis of diseases has been the focus of intelligent medical research. When dealing with image classification tasks, if the lesion area is small and uneven, the background image involved in the training will affect the ultimate accuracy in determining the extent of the lesion. We did not follow the traditional approach of building an intelligent system to assist physicians in diagnosis from the perspective of CNN models, but instead proposed a pure transformer framework that can be used for diagnostic grading of pathological images.MethodsWe propose a Symmetric Mask Pre-Training vision Transformer SMiT model for grading pathological cancer images. SMiT performs a symmetrically identical high probability sparsification of the input image token sequence at the first and last encoder layer positions to pre-train visual transformers, and the parameters of the baseline model are fine-tuned after loading the pre-training weights, allowing the model to concentrate more on extracting detailed features in the lesion region, effectively getting rid of the potential feature dependency problem.ResultsSMiT achieved 92.8% classification accuracy on 4500 histopathological images of colorectal cancer processed by Gaussian filter denoising. We validated the effectiveness and generalizability of this study's methodology on the publicly available diabetic retinopathy dataset APTOS2019 from Kaggle and achieved quadratic Cohen Kappa, accuracy and F1-score of 91.9%, 86.91% and 72.85%, respectively, which were 1-2% better than previous studies based on CNN models.ConclusionSMiT uses a simpler strategy to achieve better results to assist physicians in making accurate clinical decisions, which can be an inspiration for making good use of the visual transformers in the field of medical imaging.	[Zhang, Chengsheng; Chen, Cheng; Lv, Xiaoyi] Xinjiang Univ, Coll Software, Urumqi 830046, Peoples R China; [Chen, Chen] Xinjiang Univ, Coll Informat Sci & Engn, Urumqi 830046, Peoples R China	Xinjiang University; Xinjiang University	Chen, C (corresponding author), Xinjiang Univ, Coll Software, Urumqi 830046, Peoples R China.	chenchengoptics@gmail.com	xiao, ming/KHT-1774-2024; xiang, wei/JXL-3308-2024; Wang, Ling/KBA-9814-2024	Zhang, Chengsheng/0000-0003-1516-6321; Chen, Cheng/0000-0002-6739-1937; Chen, Chen/0000-0003-1406-5721; Wang, Ling/0000-0003-0272-2974				Ashish V., 2017, Advances in neural information processing systems, V30, P5998, DOI 10.48550/arXiv.1706.03762; Bao H., 2021, arXiv; Canziani A., 2016, ARXIV160507678; Chen C, 2022, BMC MED INFORM DECIS, V22, DOI 10.1186/s12911-022-01919-1; Dascalu A, 2022, J CANCER RES CLIN, V148, P2497, DOI 10.1007/s00432-021-03809-x; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2018, ARXIV; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Hassani A., 2021, ARXIV; He K., 2016, PROC IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553; Ho Danliang, 2021, P C HLTH INF LEARN, P204, DOI [10.1145/3450439.3451868, DOI 10.1145/3450439.3451868]; Ibrahim Nur, 2021, Proceedings of the 1st International Conference on Electronics, Biomedical Engineering, and Health Informatics. ICEBEHI 2020. Lecture Notes in Electrical Engineering (LNEE 746), P509, DOI 10.1007/978-981-33-6926-9_44; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Kavitha M, 2022, J PHARM NEGAT RESULT, V13, P94, DOI 10.47750/pnr.2022.13.S04.011; Kim SH, 2021, MULTIMED TOOLS APPL, V80, P35941, DOI 10.1007/s11042-021-10551-6; Lan Z., 2019, Albert: A lite bert for self-supervised learning of language representations; Laxmisagar H. S., 2022, Critical Reviews in Biomedical Engineering, V50, P1, DOI 10.1615/CritRevBiomedEng.2022043417; Lee S., ARXIV; Lee Y., 2021, arXiv; Li M, 2023, MED PHYS, V50, P1507, DOI 10.1002/mp.16067; Li X., 2019, ARXIV; Li Y., 2022, ARXIV; Li Y, 2021, ARXIV; Liu Y., 2019, CORR; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Majumder S, 2021, ARXIV; McHugh ML, 2012, BIOCHEM MEDICA, V22, P276, DOI 10.11613/bm.2012.031; Mnassri K, 2022, INT C CONTROL DECISI, P740, DOI [10.1109/CoDIT55151.2022.9803921, 10.1109/CODIT55151.2022.9803921]; Su Y, 2021, BRIT MACH VIS C; Thrumurthy SG, 2016, BMJ-BRIT MED J, V354, DOI 10.1136/bmj.i3590; Touvron H, 2021, PR MACH LEARN RES, V139, P7358; Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161; Wang QY, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM); Xie Z., 2021, ARXIV; Xing X, 2021, ARXIV; Yin H, 2021, ARXIV; Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564; Zhou PY, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-18879-1; Zhou Y., 2019, arXiv; Zhu X., 2021, ICLR	41	1	1	3	14	SPRINGER	NEW YORK	ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES	0171-5216	1432-1335		J CANCER RES CLIN	J. Cancer Res. Clin. Oncol.	NOV	2023	149	17					16075	16086		10.1007/s00432-023-05223-x	http://dx.doi.org/10.1007/s00432-023-05223-x		SEP 2023	12	Oncology	Science Citation Index Expanded (SCI-EXPANDED)	Oncology	W2SE4	37698681				2024-09-18	WOS:001067830900004
J	Cai, ML; Zhao, L; Qiang, Y; Wang, L; Zhao, JJ				Cai, Meiling; Zhao, Lin; Qiang, Yan; Wang, Long; Zhao, Juanjuan			CHNet: A multi-task global-local Collaborative Hybrid Network for KRAS mutation status prediction in colorectal cancer	ARTIFICIAL INTELLIGENCE IN MEDICINE			English	Article						Colorectal cancer; KRAS mutation status prediction; Segmentation; Classification; Global; Local	ADAPTIVE SEGMENTATION; NET; ATTENTION; BRAF	Accurate prediction of Kirsten rat sarcoma (KRAS) mutation status is crucial for personalized treatment of advanced colorectal cancer patients. However, despite the excellent performance of deep learning models in certain aspects, they often overlook the synergistic promotion among multiple tasks and the consideration of both global and local information, which can significantly reduce prediction accuracy. To address these issues, this paper proposes an innovative method called the Multi-task Global-Local Collaborative Hybrid Network (CHNet) aimed at more accurately predicting patients' KRAS mutation status. CHNet consists of two branches that can extract global and local features from segmentation and classification tasks, respectively, and exchange complementary information to collaborate in executing these tasks. Within the two branches, we have designed a Channel-wise Hybrid Transformer (CHT) and a Spatial-wise Hybrid Transformer (SHT). These transformers integrate the advantages of both Transformer and CNN, employing cascaded hybrid attention and convolution to capture global and local information from the two tasks. Additionally, we have created an Adaptive Collaborative Attention (ACA) module to facilitate the collaborative fusion of segmentation and classification features through guidance. Furthermore, we introduce a novel Class Activation Map (CAM) loss to encourage CHNet to learn complementary information between the two tasks. We evaluate CHNet on the T2-weighted MRI dataset, and achieve an accuracy of 88.93% in KRAS mutation status prediction, which outperforms the performance of representative KRAS mutation status prediction methods. The results suggest that our CHNet can more accurately predict KRAS mutation status in patients via a multi-task collaborative facilitation and considering global-local information way, which can assist doctors in formulating more personalized treatment strategies for patients.	[Cai, Meiling; Qiang, Yan; Zhao, Juanjuan] Taiyuan Univ Technol, Coll Comp Sci & Technol, Coll Data Sci, Taiyuan 030024, Peoples R China; [Wang, Long] Jinzhong Coll Informat, Jinzhong 030800, Shanxi, Peoples R China; [Zhao, Lin] Southeast Univ, Nanjing 210037, Jiangsu, Peoples R China	Taiyuan University of Technology; Southeast University - China	Zhao, JJ (corresponding author), Taiyuan Univ Technol, Coll Comp Sci & Technol, Coll Data Sci, Taiyuan 030024, Peoples R China.	2822885474@qq.com; zhaojuanjuan@tyut.edu.cn			National Natural Science Foun-dation of China [61972274]; National Natural Science Foundation of China [U21A20469]	National Natural Science Foun-dation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foun-dation of China (Grant No. 61972274) ; the National Natural Science Foundation of China (Grant No. U21A20469) .	Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338; Bhattacharjee D, 2022, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52688.2022.01172; Cao YT, 2023, JPN J RADIOL, V41, P1236, DOI 10.1007/s11604-023-01458-3; Chen J., 2021, PREPRINT, DOI [DOI 10.48550/ARXIV.2102.04306, 10.48550/arxiv.2102.04306]; Cui YF, 2020, EUR RADIOL, V30, P1948, DOI 10.1007/s00330-019-06572-3; Cui YF, 2019, J MAGN RESON IMAGING, V50, P930, DOI 10.1002/jmri.26653; Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; González-Castro V, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186214; González-Díaz I, 2019, IEEE J BIOMED HEALTH, V23, P547, DOI 10.1109/JBHI.2018.2806962; Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186; Guo XF, 2020, CURR MED SCI, V40, P1156, DOI 10.1007/s11596-020-2298-6; He K, 2020, BMC MED IMAGING, V20, DOI 10.1186/s12880-020-00457-4; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025; Jo P, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0239806; Jo SJ, 2021, J Korean Soc Radiol, V82; Kexin Ding, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12262), P294, DOI 10.1007/978-3-030-59713-9_29; Kong ZS, 2021, FRONT MOL BIOSCI, V8, DOI 10.3389/fmolb.2021.614277; Li JP, 2021, Arxiv, DOI arXiv:2107.04735; Li YW, 2021, Arxiv, DOI arXiv:2104.05707; Li Y, 2020, J CANCER RES CLIN, V146, P3165, DOI 10.1007/s00432-020-03354-z; Li ZN, 2020, GASTROENTEROL REP, V8, P192, DOI 10.1093/gastro/goaa022; Lin L, 2021, LECT NOTES COMPUT SC, V12908, P65, DOI 10.1007/978-3-030-87237-3_7; Liu J, 2021, IEEE INT C INT ROBOT, P4800, DOI 10.1109/IROS51168.2021.9636111; Liu J, 2022, IEEE T MED IMAGING, V41, P715, DOI 10.1109/TMI.2021.3121138; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Loupakis F, 2009, BRIT J CANCER, V101, P715, DOI 10.1038/sj.bjc.6605177; Ma YL, 2021, COMPUT METH PROG BIO, V209, DOI 10.1016/j.cmpb.2021.106311; Odena A., 2016, Distill, V1, pe3, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003, 10.23915/distill.00003.-URL]; Oh JE, 2020, CANCER RES TREAT, V52, P51, DOI 10.4143/crt.2019.050; Poudel S, 2020, IEEE ACCESS, V8, P99227, DOI 10.1109/ACCESS.2020.2996770; Cajal SRY, 2020, J MOL MED, V98, P161, DOI 10.1007/s00109-020-01874-2; Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012; Siegel RL, 2023, CA-CANCER J CLIN, V73, P17, DOI 10.3322/caac.21763; Song CY, 2020, CANCER MANAG RES, V12, P10919, DOI 10.2147/CMAR.S270727; Song K, 2022, MED PHYS, V49, P254, DOI 10.1002/mp.15361; Suzuki K, 2017, RADIOL PHYS TECHNOL, V10, P257, DOI 10.1007/s12194-017-0406-5; Vaswani A, 2017, ADV NEUR IN, V30; Wang HN, 2022, AAAI CONF ARTIF INTE, P2441; Wang HY, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101846; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009; Wu XM, 2020, ACAD RADIOL, V27, pE254, DOI 10.1016/j.acra.2019.12.007; Xie YT, 2020, IEEE T MED IMAGING, V39, P2482, DOI 10.1109/TMI.2020.2972964; Xu YY, 2019, BMC CANCER, V19, DOI 10.1186/s12885-019-6341-6; Xue T, 2022, BRIT J RADIOL, V95, DOI 10.1259/bjr.20211014; Yang L, 2018, EUR RADIOL, V28, P2058, DOI 10.1007/s00330-017-5146-8; Yu LQ, 2017, IEEE T MED IMAGING, V36, P994, DOI 10.1109/TMI.2016.2642839; Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062; Zhao L, 2023, APPL INTELL, V53, P10232, DOI 10.1007/s10489-022-04011-3; Zhu GM, 2021, MOL CANCER, V20, DOI 10.1186/s12943-021-01441-4; Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705	54	0	0	2	2	ELSEVIER	AMSTERDAM	RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS	0933-3657	1873-2860		ARTIF INTELL MED	Artif. Intell. Med.	SEP	2024	155								102931	10.1016/j.artmed.2024.102931	http://dx.doi.org/10.1016/j.artmed.2024.102931		AUG 2024	17	Computer Science, Artificial Intelligence; Engineering, Biomedical; Medical Informatics	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Medical Informatics	C2K0J	39094228				2024-09-18	WOS:001287684500001
C	Näppi, JJ; Hironaka, T; Wu, DF; Gupta, R; Tachibana, R; Taguchi, K; Okamoto, M; Yoshida, H		Yoshida, H; Wu, S		Nappi, Janne J.; Hironaka, Toru; Wu, Dufan; Gupta, Rajiv; Tachibana, Rie; Taguchi, Katsuyuki; Okamoto, Masaki; Yoshida, Hiroyuki			Automated segmentation of polyps by 3D deep learning in photon-counting CT colonography	IMAGING INFORMATICS FOR HEALTHCARE, RESEARCH, AND APPLICATIONS, MEDICAL IMAGING 2024	Proceedings of SPIE		English	Proceedings Paper	Conference on Medical Imaging - Imaging Informatics for Healthcare, Research, and Applications	FEB 19-21, 2024	San Diego, CA	SPIE, Amer Assoc Physicists Med, Radiol Soc N Amer, World Mol Imaging Soc, Soc Imaging Informat Med, Int Fdn Comp Assisted Radiol & Surg, Med Image Percept Soc		Generative AI; colorectal cancer; virtual colonoscopy; transformer; denoising diffusion; artificial intelligence	DIAMETER	Colorectal cancer (CRC) is the third most common cancer and second most common cause of cancer deaths. Most CRCs develop from large colorectal polyps, but most polyps remain smaller than 6 mm and will never develop into cancer. Therefore, conservative selective polypectomy based on polyp size would be a much more effective colorectal screening strategy than the current practice of removing all polyps. For this purpose, automated polyp measurement would be more reproducible and perhaps more precise than manual polyp measurement in CT colonography. However, for an accurate and explainable image-based measurement, it is first necessary to determine the 3D region of the polyp. We investigated the polyp segmentation performance of a traditional 3D U-Net, transformer-based U-Net, and denoising diffusion-based U-Net on a photon-counting CT (PCCT) colonography dataset. The networks were trained on 946 polyp volumes of interest (VOIs) collected from conventional clinical CT colonography datasets, and they were tested on 17 polyp VOIs extracted from a PCCT colonography dataset of an anthropomorphic colon phantom. All three segmentation networks yielded satisfactory segmentation accuracies with average Dice scores ranging between 0.73-0.75. These preliminary results and experiences are expected to be useful in guiding the development of a deep-learning tool for reliable estimation of the polyp size for the diagnosis and management of patients in CRC screening.	[Nappi, Janne J.; Hironaka, Toru; Tachibana, Rie; Yoshida, Hiroyuki] Massachusetts Gen Hosp, Dept Radiol, 3D Imaging Res, Boston, MA 02114 USA; [Wu, Dufan] Massachusetts Gen Hosp, Dept Radiol, Ctr Adv Med Comp & Anal, Boston, MA USA; [Gupta, Rajiv] Massachusetts Gen Hosp, Dept Radiol, Neuroimaging, Boston, MA USA; [Nappi, Janne J.; Wu, Dufan; Gupta, Rajiv; Tachibana, Rie; Yoshida, Hiroyuki] Harvard Med Sch, Boston, MA 02115 USA; [Tachibana, Rie] Inst Natl Coll Technol, Oshima, Japan; [Taguchi, Katsuyuki] Johns Hopkins Univ, Dept Radiol & Radiol Sci, Baltimore, MD USA; [Okamoto, Masaki] Boston Med Sci Inc, Tokyo, Japan	Harvard University; Massachusetts General Hospital; Harvard University; Massachusetts General Hospital; Harvard University; Massachusetts General Hospital; Harvard University; Harvard Medical School; Johns Hopkins University	Näppi, JJ; Yoshida, H (corresponding author), Massachusetts Gen Hosp, Dept Radiol, 3D Imaging Res, Boston, MA 02114 USA.; Näppi, JJ; Yoshida, H (corresponding author), Harvard Med Sch, Boston, MA 02115 USA.	janne.nappi@mgh.harvard.edu; yoshida.hiro@mgh.harvard.edu						Burling D, 2007, CLIN RADIOL, V62, P145, DOI 10.1016/j.crad.2006.09.018; Burling D, 2005, J COMPUT ASSIST TOMO, V29, P387, DOI 10.1097/01.rct.0000160985.66259.96; Burling D, 2006, EUR RADIOL, V16, P1737, DOI 10.1007/s00330-006-0189-2; Cardoso MJ., 2022, arXiv; Croitoru FA, 2023, IEEE T PATTERN ANAL, V45, P10850, DOI 10.1109/TPAMI.2023.3261988; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001; Fletcher JG, 2007, AM J ROENTGENOL, V188, P945, DOI 10.2214/AJR.06.1169; Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181; Hesamian MH, 2019, J DIGIT IMAGING, V32, P582, DOI 10.1007/s10278-019-00227-x; Kim DH, 2023, KOREAN J RADIOL, V24, P79, DOI 10.3348/kjr.2022.0969; Muenzel D, 2017, RADIOLOGY, V283, P722, DOI 10.1148/radiol.2016160890; Pooler BD, 2024, RADIOLOGY, V310, DOI 10.1148/radiol.232078; Pooler BD, 2023, GUT, V72, P2321, DOI 10.1136/gutjnl-2022-326970; Rajendran K, 2022, RADIOLOGY, V303, P130, DOI 10.1148/radiol.212579; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Summers RM, 2010, RADIOLOGY, V255, P707, DOI 10.1148/radiol.10090877; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Vaswani A, 2017, ADV NEUR IN, V30; Xing Z., 2022, tech. rep; Yee J, 2024, RADIOLOGY, V310, DOI 10.1148/radiol.232007	21	0	0	0	0	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X	1996-756X	978-1-5106-7167-6; 978-1-5106-7166-9	PROC SPIE			2024	12931								129311C	10.1117/12.3007290	http://dx.doi.org/10.1117/12.3007290			6	Computer Science, Artificial Intelligence; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Radiology, Nuclear Medicine & Medical Imaging	BW9QJ					2024-09-18	WOS:001219280700041
J	Park, KB; Lee, JY				Park, Kyeong-Beom; Lee, Jae Yeol			SwinE-Net: hybrid deep learning approach to novel polyp segmentation using convolutional neural network and Swin Transformer	JOURNAL OF COMPUTATIONAL DESIGN AND ENGINEERING			English	Article						polyp segmentation; convolutional neural networks; multidilation convolutional block; multifeature aggregation block; Swin Transformer; Vision Transformer	COLONOSCOPY	Prevention of colorectal cancer (CRC) by inspecting and removing colorectal polyps has become a global health priority because CRC is one of the most frequent cancers in the world. Although recent U-Net-based convolutional neural networks (CNNs) with deep feature representation and skip connections have shown to segment polyps effectively, U-Net-based approaches still have limitations in modeling explicit global contexts, due to the intrinsic nature locality of convolutional operations. To overcome these problems, this study proposes a novel deep learning model, SwinE-Net, for polyp segmentation that effectively combines a CNN-based EfficientNet and Vision Transformer (ViT)-based Swin Ttransformer. The main challenge is to conduct accurate and robust medical segmentation in maintaining global semantics without sacrificing low-level features of CNNs through Swin Transformer. First, the multidilation convolutional block generates refined feature maps to enhance feature discriminability for multilevel feature maps extracted from CNN and ViT. Then, the multifeature aggregation block creates intermediate side outputs from the refined polyp features for efficient training. Finally, the attentive deconvolutional network-based decoder upsamples the refined and combined feature maps to accurately segment colorectal polyps. We compared the proposed approach with previous state-of-the-art methods by evaluating various metrics using five public datasets (Kvasir, ClinicDB, ColonDB, ETIS, and EndoScene). The comparative evaluation, in particular, proved that the proposed approach showed much better performance in the unseen dataset, which shows the generalization and scalability in conducting polyp segmentation. Furthermore, an ablation study was performed to prove the novelty and advantage of the proposed network. The proposed approach outperformed previous studies.	[Park, Kyeong-Beom; Lee, Jae Yeol] Chonnam Natl Univ, Dept Ind Engn, 77,Yongbong Ro, Gwangju 61186, South Korea	Chonnam National University	Lee, JY (corresponding author), Chonnam Natl Univ, Dept Ind Engn, 77,Yongbong Ro, Gwangju 61186, South Korea.	jaeyeol@chonnam.ac.kr		Park, Kyeong-Beom/0000-0003-4737-730X	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education [2019R1I1A3A01059082]; Korea Health Technology Research and Development Project through the Korea Health Industry Development Institute (KHIDI) - Ministry of Health and Welfare [HI19C0642]	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education(National Research Foundation of KoreaMinistry of Education (MOE), Republic of KoreaNational Research Council for Economics, Humanities & Social Sciences, Republic of Korea); Korea Health Technology Research and Development Project through the Korea Health Industry Development Institute (KHIDI) - Ministry of Health and Welfare	This work was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF), funded by the Ministry of Education (2019R1I1A3A01059082), and the Korea Health Technology Research and Development Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health and Welfare (HI19C0642).	Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P., 2017, P SPIE MED IM, V10134; Cao Hu, 2021, ARXIV210505537; Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645; Fan TL, 2020, IEEE ACCESS, V8, P179656, DOI 10.1109/ACCESS.2020.3025372; Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172; Ferlay J., 2010, Cancer Incidence and Mortality Worldwide: IARC CancerBase No. 10; Guo YB, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070069; Hassan C, 2020, GUT, V69, P799, DOI 10.1136/gutjnl-2019-319914; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsiang Huang C., 2021, ARXIV210107172; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mahmud T, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104119; Mao Yuxin, 2021, ARXIV210410127; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Mori Y, 2018, NAT BIOMED ENG, V2, P713, DOI 10.1038/s41551-018-0308-9; Nguyen NQ, 2020, IEEE ACCESS, V8, P99495, DOI 10.1109/ACCESS.2020.2995630; Park KB, 2020, IEEE ACCESS, V8, P146308, DOI 10.1109/ACCESS.2020.3015108; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Safarov S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041441; Seo H, 2020, IEEE T MED IMAGING, V39, P1316, DOI 10.1109/TMI.2019.2948320; Siegel RL, 2020, CA-CANCER J CLIN, V70, P7, DOI [10.3322/caac.21590, 10.3322/caac.21601]; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tan MX, 2019, PR MACH LEARN RES, V97; Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23; Tomar N. K., 2021, ARXIV210317235; Touvron H, 2021, PR MACH LEARN RES, V139, P7358; van Rijn JC, 2006, AM J GASTROENTEROL, V101, P343, DOI 10.1111/j.1572-0241.2006.00390.x; Vania M, 2021, J COMPUT DES ENG, V8, P1023, DOI 10.1093/jcde/qwab030; Vania M, 2019, J COMPUT DES ENG, V6, P224; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xie Enze, 2021, ARXIV210108461; Xinzi Sun, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P851, DOI 10.1109/ICMLA.2019.00148; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	50	55	59	4	60	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND		2288-5048		J COMPUT DES ENG	J. Comput. Des. Eng.	APR 7	2022	9	2					616	632		10.1093/jcde/qwac018	http://dx.doi.org/10.1093/jcde/qwac018			17	Computer Science, Interdisciplinary Applications; Engineering, Multidisciplinary	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	0I0CF		gold			2024-09-18	WOS:000779095500001
J	Li, WS; Huang, ZP; Li, FY; Zhao, YH; Zhang, HC				Li, Weisheng; Huang, Zhaopeng; Li, Feiyan; Zhao, Yinghui; Zhang, Hongchuan			CIFG-Net: Cross-level information fusion and guidance network for Polyp Segmentation	COMPUTERS IN BIOLOGY AND MEDICINE			English	Article						Colorectal cancer; Polyp segmentation; Transformer; Cross-level information processing	ATTENTION	Colorectal cancer is a common malignant tumor of the digestive tract. Most colorectal cancer is caused by colorectal polyp lesions. Timely detection and removal of colorectal polyps can substantially reduce the incidence of colorectal cancer. Accurate polyp segmentation can provide important polyp information that can aid in the early diagnosis and treatment of colorectal cancer. However, polyps of the same type can vary in texture, color, and even size. Furthermore, some polyps are similar in colour to the surrounding healthy tissue, which makes the boundary between the polyp and the surrounding area unclear. In order to overcome the issues of inaccurate polyp localization and unclear boundary segmentation, we propose a polyp segmentation network based on cross-level information fusion and guidance. We use a Transformer encoder to extract a more robust feature representation. In addition, to refine the processing of feature information from encoders, we propose the edge feature processing module (EFPM) and the cross-level information processing module (CIPM). EFPM is used to focus on the boundary information in polyp features. After processing each feature, EFPM can obtain clear and accurate polyp boundary features, which can mitigate unclear boundary segmentation. CIPM is used to aggregate and process multi-scale features transmitted by various encoder layers and to solve the problem of inaccurate polyp location by using multi-level features to obtain the location information of polyps. In order to better use the processed features to optimise our segmentation effect, we also propose an information guidance module (IGM) to integrate the processed features of EFPM and CIPM to obtain accurate positioning and segmentation of polyps. Through experiments on five public polyp datasets using six metrics, it was demonstrated that the proposed network has better robustness and more accurate segmentation effect. Compared with other advanced algorithms, CIFG-Net has superior performance. Code available at: https://github.com/zspnb/CIFG-Net.	[Li, Weisheng; Huang, Zhaopeng; Li, Feiyan; Zhao, Yinghui; Zhang, Hongchuan] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing, Peoples R China	Chongqing University of Posts & Telecommunications	Li, WS (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing, Peoples R China.	liws@cqupt.edu.cn			National Natural Science Foundation of China [62331008, 62027827, 62221005]; Natural Science Foundation of Chongqing [2023NSCQ-LZX0045, CSTB2022NSCQ-MSX0436]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Chongqing(Natural Science Foundation of Chongqing)	This work was supported by the National Natural Science Foundation of China [Nos. 62331008, 62027827 and 62221005] , Natural Science Foundation of Chongqing (Nos. 2023NSCQ-LZX0045, CSTB2022NSCQ-MSX0436) .	Barua I, 2021, ENDOSCOPY, V53, P277, DOI 10.1055/a-1201-7165; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Ciardiello F, 2022, CA-CANCER J CLIN, V72, P372, DOI 10.3322/caac.21728; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Granados-Romero J.J., 2017, Int. J. Res. Med. Sci, V5, P4667, DOI 10.18203/2320- 6012.ijrms20174914; Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247; Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172; Intisar Rizwan I. Haque, 2020, Informatics in Medicine Unlocked, V18, DOI 10.1016/j.imu.2020.100297; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Kim T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2167, DOI 10.1145/3474085.3475375; Liu Qianying, 2023, ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1, DOI 10.1109/ICASSP49357.2023.10096379; Lou AG, 2023, J MED IMAGING, V10, DOI 10.1117/1.JMI.10.1.014005; Nguyen T.C., Medical Image Computing and Computer Assisted Intervention-MICCAI 2021. MICCAI 2021, V12901; Pan ZZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P367, DOI 10.1109/ICCV48922.2021.00043; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Nguyen TC, 2021, LECT NOTES COMPUT SC, V12901, P633, DOI 10.1007/978-3-030-87193-2_60; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xie EZ, 2021, ADV NEUR IN, V34; Yurtkulu SC, 2019, SIG PROCESS COMMUN, DOI 10.1109/siu.2019.8806244; Zhang RF, 2022, LECT NOTES COMPUT SC, V13433, P99, DOI 10.1007/978-3-031-16437-8_10; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	41	2	2	26	40	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0010-4825	1879-0534		COMPUT BIOL MED	Comput. Biol. Med.	FEB	2024	169								107931	10.1016/j.compbiomed.2024.107931	http://dx.doi.org/10.1016/j.compbiomed.2024.107931		JAN 2024	13	Biology; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Mathematical & Computational Biology	Science Citation Index Expanded (SCI-EXPANDED)	Life Sciences & Biomedicine - Other Topics; Computer Science; Engineering; Mathematical & Computational Biology	GE7A2	38181608				2024-09-18	WOS:001151043100001
J	Erdas, CB				Erdas, Cagatay Berke			Computer-aided colorectal cancer diagnosis: AI-driven image segmentation and classification	PEERJ COMPUTER SCIENCE			English	Article						Colorectal cancer; Computer-aided diagnosis; Histopathology; Image segmentation; Anomaly classi fi cation; Deep learning		Colorectal cancer is an enormous health concern since it is among the most lethal types of malignancy. The manual examination has its limitations, including subjectivity and data overload. To overcome these challenges, computer -aided diagnostic systems focusing on image segmentation and abnormality classi fi cation have been developed. This study presents a two -stage approach for the automatic detection of fi ve types of colorectal abnormalities in addition to a control group: polyp, low-grade intraepithelial neoplasia, high-grade intraepithelial neoplasia, serrated adenoma, adenocarcinoma. In the fi rst stage, UNet3+ was used for image segmentation to locate the anomalies, while in the second stage, the Cross -Attention Multi -Scale Vision Transformer deep learning model was used to predict the type of anomaly after highlighting the anomaly on the raw images. In anomaly segmentation, UNet3+ achieved values of 0.9872, 0.9422, 0.9832, and 0.9560 for Dice Coef fi cient, Jaccard Index, Sensitivity, Speci fi city respectively. In anomaly detection, the Cross -Attention Multi -Scale Vision Transformer model attained a classi fi cation performance of 0.9340, 0.9037, 0.9446, 0.8723, 0.9102, 0.9849 for accuracy, F1 score, precision, recall, Matthews correlation coef fi cient, and speci fi city, respectively. The proposed approach proves its capacity to alleviate the overwhelm of pathologists and enhance the accuracy of colorectal cancer diagnosis by achieving high performance in both the identi fi cation of anomalies and the segmentation of regions.	[Erdas, Cagatay Berke] Baskent Univ, Comp Engn, Ankara, Turkiye	Baskent University	Erdas, CB (corresponding author), Baskent Univ, Comp Engn, Ankara, Turkiye.	berkeerdas@gmail.com						Ben Hamida A, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104730; Bilal M, 2022, medRxiv, DOI [10.1101/2022.02.28.22271565, 10.1101/2022.02.28.22271565, DOI 10.1101/2022.02.28.22271565]; Chan JKC, 2014, INT J SURG PATHOL, V22, P12, DOI 10.1177/1066896913517939; Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041; de Leon MP, 2001, DIGEST LIVER DIS, V33, P372; Erdas CB, 2023, PEERJ COMPUT SCI, V9, P1, DOI 10.7717/peerj-cs.1485; Fischer Andrew H, 2008, CSH Protoc, V2008, DOI 10.1101/pdb.prot4986; Gao HM, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01418-x; Gupta V, 2021, BIOCYBERN BIOMED ENG, V41, P1272, DOI 10.1016/j.bbe.2021.08.011; Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]; Kadri R., 2022, International Journal of Hybrid Intelligent Systems, V17, P163, DOI [10.3233/his-220002, DOI 10.3233/HIS-220002]; Labianca R, 2013, ANN ONCOL, V24, P64, DOI 10.1093/annonc/mdt354; Liu Z, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12189208; Martos O, 2023, PATHOL RES PRACT, V248, DOI 10.1016/j.prp.2023.154694; Mehmood S, 2022, IEEE ACCESS, V10, P25657, DOI 10.1109/ACCESS.2022.3150924; MIaMIA Group, 2022, Figshare, DOI 10.6084/m9.figshare.21540159.v1; Müller D, 2022, BMC RES NOTES, V15, DOI 10.1186/s13104-022-06096-y; Pamudurthy Vijeta, 2020, Proc (Bayl Univ Med Cent), V33, P28, DOI 10.1080/08998280.2019.1686327; Rathore S, 2019, CANCERS, V11, DOI 10.3390/cancers11111700; Ren W, 2013, WORLD J GASTROENTERO, V19, P2092, DOI 10.3748/wjg.v19.i13.2092; Shah N.A., 2021, INT C COMPUTER VISIO, P451, DOI DOI 10.1007/978-981-16-1086-8_40; Shi LY, 2023, FRONT MED-LAUSANNE, V10, DOI 10.3389/fmed.2023.1114673; Sirinukunwattana K, 2016, IEEE T MED IMAGING, V35, P1196, DOI 10.1109/TMI.2016.2525803; Spring KJ, 2006, GASTROENTEROLOGY, V131, P1400, DOI 10.1053/j.gastro.2006.08.038; Sriwastawa A, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-16954-x; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tummala S, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13091594; Wang Y, 2021, 2021 IEEE INT C BIOI, DOI [10.1109/bibm52615.2021.9669780, DOI 10.1109/BIBM52615.2021.9669780]; Yousef R, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13091624	29	0	0	3	3	PEERJ INC	LONDON	341-345 OLD ST, THIRD FLR, LONDON, EC1V 9LL, ENGLAND		2376-5992		PEERJ COMPUT SCI	PeerJ Comput. Sci.	MAY 17	2024	10								e2071	10.7717/peerj-cs.2071	http://dx.doi.org/10.7717/peerj-cs.2071			18	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	RR2F2	38855213	Green Published, gold			2024-09-18	WOS:001229315800003
J	Elkarazle, K; Raman, V; Then, P; Chua, C				Elkarazle, Khaled; Raman, Valliappan; Then, Patrick; Chua, Caslon			Improved Colorectal Polyp Segmentation Using Enhanced MA-NET and Modified Mix-ViT Transformer	IEEE ACCESS			English	Article						~Colorectal polyps; colorectal polyps detection; colorectal polyps segmentation; color space; colonoscopy images	MISSED POLYPS; RISK-FACTORS; DIAGNOSIS; ADENOMA; NETWORK	Colorectal polyps is a prevalent medical condition that could lead to colorectal cancer, a leading cause of cancer-related mortality globally, if left undiagnosed. Colonoscopy remains the gold standard for detection and diagnosis of colorectal neoplasia; however, a significant proportion of neoplastic lesions are missed during routine examinations, particularly diminutive and flat lesions. Deep learning techniques have been employed to improve polyp detection rates in colonoscopy images and have proven successful in reducing the miss rate. However, accurate segmentation of small and flat polyps remains a major challenge to existing models as they struggle to differentiate polypoid and non-polypoid regions apart. To address this issue, we present an enhanced version of the Multi-Scale Attention Network (MA-NET) that incorporates a modified Mix-ViT transformer as the feature extractor. The modified Mix-ViT facilitates ultra-finegrained visual categorization to improve the segmentation accuracy of polypoid and non-polypoid regions. Additionally, we introduce a pre-processing layer that performs histogram equalization on input images in the CIEL*A* B* color space to enhance their features. Our model was trained on a combined dataset comprising Kvasir-SEG and CVC-ClinicDB and cross-validated on CVC-ColonDB and ETIS-LaribDB. The proposed method demonstrates superior performance compared to existing methods, particularly in the detection of small and flat polyps.	[Elkarazle, Khaled; Then, Patrick] Swinburne Univ Technol, Fac Engn Comp & Sci, Sarawak Campus, Kuching 93350, Sarawak, Malaysia; [Raman, Valliappan] Coimbatore Inst Technol, Dept Artificial Intelligence & Data Sci, Coimbatore 641014, Tamil Nadu, India; [Elkarazle, Khaled; Chua, Caslon] Swinburne Univ Technol, Fac Sci Engn & Technol, Melbourne, Vic 3122, Australia	Swinburne University of Technology Sarawak; Swinburne University of Technology; Coimbatore Institute of Technology; Swinburne University of Technology	Elkarazle, K (corresponding author), Swinburne Univ Technol, Fac Engn Comp & Sci, Sarawak Campus, Kuching 93350, Sarawak, Malaysia.	kelkaeazle@swinburne.edu.my	ELKarazle, Khaled/HHC-1556-2022; Chua, Caslon/GPC-6446-2022; Raman, Valliappan/AAX-6431-2021; raman, Valliappan/E-6393-2018	, Caslon/0000-0003-3126-3156; ELKarazle, Khaled Yahya Mohamed Mahmoud/0000-0001-7545-1605; raman, Valliappan/0000-0002-9363-2319; Then, Patrick/0000-0002-6079-2527	Swinburne University of Technology Sarawak Higher Degree by Research (HDR) Support Fund	Swinburne University of Technology Sarawak Higher Degree by Research (HDR) Support Fund	This work was supported in part by the Swinburne University of Technology Sarawak Higher Degree by Research (HDR) Support Fund.	Ahmad M, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/2665283; Ahn SB, 2012, GUT LIVER, V6, P64, DOI 10.5009/gnl.2012.6.1.64; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bazi Y, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030516; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Bond JH, 2000, AM J GASTROENTEROL, V95, P3053; Chang Q., 2022, ESF PNET EFFICIENT D; Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen SJ, 2022, FRONT MED-LAUSANNE, V9, DOI 10.3389/fmed.2022.852553; cie co at, D65 CIE; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; ELKarazle K, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23031225; Ellahyani Ayoub, 2023, Personal and Ubiquitous Computing, P235, DOI 10.1007/s00779-021-01660-y; Eu C.Y., 2022, P INT C ART INT SMAR, DOI [10.1007/978-981-16-2183-3_69, DOI 10.1007/978-981-16-2183-3_69]; Fan TL, 2020, IEEE ACCESS, V8, P179656, DOI 10.1109/ACCESS.2020.3025372; Gelly S., 2021, ICLR, DOI DOI 10.48550/ARXIV.2010.11929; Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559; Hao YZ, 2020, GUT LIVER, V14, P399, DOI 10.5009/gnl19097; Hassan MF, 2022, MULTIMED TOOLS APPL, V81, P26331, DOI 10.1007/s11042-022-12429-7; Helsingen Lise M, 2022, NEJM Evid, V1, pEVIDra2100035, DOI 10.1056/EVIDra2100035; Hu KL, 2022, COMPUT BIOL MED, V147, DOI 10.1016/j.compbiomed.2022.105760; Issa IA, 2017, WORLD J GASTROENTERO, V23, P5086, DOI 10.3748/wjg.v23.i28.5086; Jeong YH, 2016, J KOREAN MED SCI, V31, P1426, DOI 10.3346/jkms.2016.31.9.1426; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Khan MU, 2018, LECT NOTES ELECTR EN, V477, P39, DOI 10.1007/978-981-10-7629-9_5; Kim NH, 2017, INTEST RES, V15, P411, DOI 10.5217/ir.2017.15.3.411; Lee J, 2017, MEDICINE, V96, DOI 10.1097/MD.0000000000007468; Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Lui TKL, 2020, WORLD J GASTROENTERO, V26, P5248, DOI 10.3748/wjg.v26.i35.5248; Macari M, 2004, AM J ROENTGENOL, V183, P127, DOI 10.2214/ajr.183.1.1830127; Mennigen R, 1988, Surg Endosc, V2, P84, DOI 10.1007/BF00704360; Mohammed A., 2018, Y-Net: A deep convolutional neural network for polyp detection; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Nogueira-Rodríguez A, 2021, NEUROCOMPUTING, V423, P721, DOI 10.1016/j.neucom.2020.02.123; Palmier R, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-04786-y; Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018; Puyal JGB, 2022, MED IMAGE ANAL, V82, DOI 10.1016/j.media.2022.102625; Qadri SF, 2023, INT J INTELL SYST, V2023, DOI 10.1155/2023/2345835; Rasouli Pezhman, 2020, Gastroenterol Hepatol Bed Bench, V13, P191; Rawla P, 2019, GASTROENTEROL REV, V14, P89, DOI 10.5114/pg.2018.81072; Ribeiro Jose, 2022, Procedia Computer Science, P477, DOI 10.1016/j.procs.2021.12.039; Ronneberger O., U-Net: Convolutional Networks for Biomedical Image Segmentation; Saberi Z., 2023, IAENG INT J APPL MAT, V53, P1; Sánchez-Peralta LF, 2020, ARTIF INTELL MED, V108, DOI 10.1016/j.artmed.2020.101923; Sanderson E, 2022, LECT NOTES COMPUT SC, V13413, P892, DOI 10.1007/978-3-031-12053-4_65; Senore C, 2017, BEST PRACT RES CL GA, V31, P481, DOI 10.1016/j.bpg.2017.04.008; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Süsstrunk S, 1999, SEVENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P127; Suzuki K, 2010, MED PHYS, V37, P12, DOI 10.1118/1.3263615; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tan MX, 2019, PR MACH LEARN RES, V97; Tang FL, 2022, Arxiv, DOI arXiv:2212.11677; Wang JF, 2022, LECT NOTES COMPUT SC, V13433, P110, DOI 10.1007/978-3-031-16437-8_11; WILLIAMS C, 1973, GUT, V14, P990, DOI 10.1136/gut.14.12.990; World Health Organization, 2022, COLORECTAL CANCER; Yang K, 2022, ALEX ENG J, V61, P917, DOI 10.1016/j.aej.2021.04.072; Yu XH, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109131; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	65	4	4	3	19	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2169-3536			IEEE ACCESS	IEEE Access		2023	11						69295	69309		10.1109/ACCESS.2023.3291783	http://dx.doi.org/10.1109/ACCESS.2023.3291783			15	Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Telecommunications	M5JU4		gold			2024-09-18	WOS:001030586100001
C	Cai, LH; Wu, MJ; Chen, LJ; Bai, WP; Yang, M; Lyu, SC; Zhao, Q		Wang, L; Dou, Q; Fletcher, PT; Speidel, S; Li, S		Cai, Linghan; Wu, Meijing; Chen, Lijiang; Bai, Wenpei; Yang, Min; Lyu, Shuchang; Zhao, Qi			Using Guided Self-Attention with Local Information for Polyp Segmentation	MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION, MICCAI 2022, PT IV	Lecture Notes in Computer Science		English	Proceedings Paper	25th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)	SEP 18-22, 2022	Singapore, SINGAPORE	MICCAI Soc		Colorectal cancer; Polyp segmentation; Transformer; Local-to-Global mechanism; PP-guided self-attention		Automatic and precise polyp segmentation is crucial for the early diagnosis of colorectal cancer. Existing polyp segmentation methods are mostly based on convolutional neural networks (CNNs), which usually utilize the global features to enhance local features through well-designed modules, thereby dealing with the diversity of polyps. Although CNN-based methods achieve impressive results, they are powerless to model explicit long-range relations, which limits their performance. Different from CNN, Transformer has a strong capability of modeling long-range relations owing to self-attention. However, self-attention always spreads attention to unexpected regions and the Transformer's ability of local feature extraction is insufficient, resulting in inaccurate localization and fuzzy boundary. To address these issues, we propose PPFormer for accurate polyp segmentation. Specifically, we first adopt a shallow CNN encoder and a deep Transformer encoder to extract rich features. In the decoder, we present the PP-guided self-attention that uses prediction maps to guide self-attention to focus on the hard regions so as to enhance the model's perception of polyp boundary. Meanwhile, the Local-to-Global mechanism is designed to encourage the Transformer to capture more information in the local-window for better polyp localization. Extensive experiments on five challenging datasets show that PPFormer outperforms other advanced methods and achieves state-of-the-art results with six metrics, i.e. mean Dice and mean IoU.	[Cai, Linghan; Chen, Lijiang; Lyu, Shuchang; Zhao, Qi] Beihang Univ, Inst Elect Informat Engn, Beijing, Peoples R China; [Wu, Meijing; Bai, Wenpei; Yang, Min] Capital Med Univ, Beijing Shijitan Hosp, Dept Gynecol & Obstet, Beijing, Peoples R China	Beihang University; Capital Medical University	Chen, LJ (corresponding author), Beihang Univ, Inst Elect Informat Engn, Beijing, Peoples R China.	chenlijiang@buaa.edu.cn	Wu, MJ/KQV-3644-2024; Zhao, Qi/IQR-6252-2023; Cai, Linghan/GVS-0087-2022; LI, Xiang-Yang/JZE-0275-2024		National Natural Science Foundation of China [62072021]; Fundamental Research Funds for the Central Universities [YWF-22-L-532]; Beijing Hospitals Authority'Ascent Plan [DFL20190701]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Beijing Hospitals Authority'Ascent Plan	This project was partly supported by the National Natural Science Foundation of China (Grant No. 62072021), the Fundamental Research Funds for the Central Universities (Grant No. YWF-22-L-532), and the Beijing Hospitals Authority'Ascent Plan (Grant No. DFL20190701).	Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chen J., 2021, PREPRINT, DOI [DOI 10.48550/ARXIV.2102.04306, 10.48550/arxiv.2102.04306]; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dosovitskiy A., 2021, P ICLR 2021, DOI 10.48550/arXiv.2010.11929; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698; Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247; Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Nguyen TC, 2021, LECT NOTES COMPUT SC, V12901, P633, DOI 10.1007/978-3-030-87193-2_60; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhao XQ, 2021, LECT NOTES COMPUT SC, V12901, P120, DOI 10.1007/978-3-030-87193-2_12; Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609	25	10	10	0	9	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	0302-9743	1611-3349	978-3-031-16440-8; 978-3-031-16439-2	LECT NOTES COMPUT SC			2022	13434						629	638		10.1007/978-3-031-16440-8_60	http://dx.doi.org/10.1007/978-3-031-16440-8_60			10	Computer Science, Interdisciplinary Applications; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Radiology, Nuclear Medicine & Medical Imaging	BU0DG					2024-09-18	WOS:000867306400060
J	Hu, WM; Li, C; Rahaman, MM; Chen, HY; Liu, WL; Yao, YD; Sun, HZ; Grzegorzek, M; Li, XY				Hu, Weiming; Li, Chen; Rahaman, Md Mamunur; Chen, Haoyuan; Liu, Wanli; Yao, Yudong; Sun, Hongzan; Grzegorzek, Marcin; Li, Xiaoyan			EBHI: A new Enteroscope Biopsy Histopathological H&E Image Dataset for image classification evaluation	PHYSICA MEDICA-EUROPEAN JOURNAL OF MEDICAL PHYSICS			English	Article						Enteroscope biopsy; Colorectal histopathology; Image database; Image classification	ARTIFICIAL-INTELLIGENCE; CANCER; SEGMENTATION; DIAGNOSIS; PATHOLOGY	Background and purpose: Colorectal cancer has become the third most common cancer worldwide, ac-counting for approximately 10% of cancer patients. Early detection of the disease is important for the treatment of colorectal cancer patients. Histopathological examination is the gold standard for screening colorectal cancer. However, the current lack of histopathological image datasets of colorectal cancer, especially enteroscope biopsies, hinders the accurate evaluation of computer-aided diagnosis techniques. Therefore, a multi-category colorectal cancer dataset is needed to test various medical image classification methods to find high classification accuracy and strong robustness.Methods: A new publicly available Enteroscope Biopsy Histopathological H&E Image Dataset (EBHI) is published in this paper. To demonstrate the effectiveness of the EBHI dataset, we have utilized several machine learning, convolutional neural networks and novel transformer-based classifiers for experimentation and evaluation, using an image with a magnification of 200x.Results: Experimental results show that the deep learning method performs well on the EBHI dataset. Classical machine learning methods achieve maximum accuracy of 76.02% and deep learning method achieves a maximum accuracy of 95.37%.Conclusion: To the best of our knowledge, EBHI is the first publicly available colorectal histopathology enteroscope biopsy dataset with four magnifications and five types of images of tumor differentiation stages, totaling 5532 images. We believe that EBHI could attract researchers to explore new classification algorithms for the automated diagnosis of colorectal cancer, which could help physicians and patients in clinical settings.	[Hu, Weiming; Li, Chen; Rahaman, Md Mamunur; Chen, Haoyuan; Liu, Wanli] Northeastern Univ, Coll Med & Biol Informat Engn, Microscop Image & Med Image Anal Grp, Shenyang, Peoples R China; [Li, Xiaoyan] China Med Univ, Canc Hosp, Liaoning Canc Hosp & Inst, Shenyang, Peoples R China; [Rahaman, Md Mamunur] Univ New South Wales, Sch Comp Sci & Engn, Sydney, Australia; [Yao, Yudong] Stevens Inst Technol, Dept Elect & Comp Engn, Hoboken, NJ USA; [Sun, Hongzan] China Med Univ, Shengjing Hosp, Dept Radiol, Shenyang, Peoples R China; [Grzegorzek, Marcin] Univ Lubeck, Inst Med Informat, Lubeck, Germany; [Grzegorzek, Marcin] Univ Econ Katowice, Dept Knowledge Engn, Katowice, Poland	Northeastern University - China; China Medical University; University of New South Wales Sydney; Stevens Institute of Technology; China Medical University; University of Lubeck; University of Economics in Katowice	Li, C (corresponding author), Northeastern Univ, Coll Med & Biol Informat Engn, Microscop Image & Med Image Anal Grp, Shenyang, Peoples R China.; Li, XY (corresponding author), China Med Univ, Canc Hosp, Liaoning Canc Hosp & Inst, Shenyang, Peoples R China.	lichen@bmie.neu.edu.cn; lixiaoyan@cancerhosp-ln-cmu.com	Sun, Hongzan/AAQ-2650-2021; Grzegorzek, Marcin/AAF-1647-2021; li, xiaoyan/HPC-4813-2023; Hu, Weiming/GLR-1229-2022; Rahaman, Md Mamunur/AAS-5300-2021	Rahaman, Md Mamunur/0000-0003-2268-2092; Li, Chen/0000-0003-1545-8885	National Natural Science Foundation of China [82220108007]; "Beijing Xisike Clinical Oncology Research Foundation'', China [Y-tongshu2021/qn-0379]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); "Beijing Xisike Clinical Oncology Research Foundation'', China	This work is supported by the "National Natural Science Foundation of China'' (No. 82220108007) and "Beijing Xisike Clinical Oncology Research Foundation'', China (No. Y-tongshu2021/qn-0379). We alsothank Miss. Zixian Li and Mr. Guoxian Li for their important discussion in this work.	Bilal M, 2022, medRxiv; Castiglioni I, 2021, PHYS MEDICA, V83, P9, DOI 10.1016/j.ejmp.2021.02.006; Chan JKC, 2014, INT J SURG PATHOL, V22, P12, DOI 10.1177/1066896913517939; Chen A, 2022, BIOCYBERN BIOMED ENG, V42, P204, DOI 10.1016/j.bbe.2021.12.010; Chen HY, 2022, Arxiv, DOI arXiv:2104.14528; Chen HY, 2022, COMPUT BIOL MED, V143, DOI 10.1016/j.compbiomed.2022.105265; Cooper HS, 1998, HUM PATHOL, V29, P15, DOI 10.1016/S0046-8177(98)90385-9; de Leon MP, 2001, DIGEST LIVER DIS, V33, P372; Diaz O, 2021, PHYS MEDICA, V83, P25, DOI 10.1016/j.ejmp.2021.02.007; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Fiorino C, 2022, PHYS MEDICA, V98, P8, DOI 10.1016/j.ejmp.2022.04.003; Gou JP, 2019, EXPERT SYST APPL, V115, P356, DOI 10.1016/j.eswa.2018.08.021; Gupta V, 2021, BIOCYBERN BIOMED ENG, V41, P1272, DOI DOI 10.1016/j.bbe.2021.08.0110168-8227; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu WM, 2022, COMPUT BIOL MED, V142, DOI 10.1016/j.compbiomed.2021.105207; Jass J R., 2012, Histological typing of intestinal tumours, V2nd; Kather Jakob Nikolas, 2018, Zenodo; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Kather Jakob Nikolas, 2016, Zenodo; Kausar T, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12010288; Kumar A, 2022, IEEE T IND INFORM, V18, P5648, DOI 10.1109/TII.2021.3138919; Kumar A, 2021, IEEE INTERNET THINGS, V8, P17778, DOI 10.1109/JIOT.2021.3119520; Kumar A, 2021, IEEE T FUZZY SYST, V29, P103, DOI 10.1109/TFUZZ.2020.2995968; Kumar A, 2020, INFORM SCIENCES, V508, P405, DOI 10.1016/j.ins.2019.08.072; Kumar V., 2017, Robbins Basic Pathology; Kumari K., 2018, J PRACT CARDIOVASC S, V4, P33, DOI DOI 10.4103/JPCS.JPCS_8_18; Labianca R, 2013, ANN ONCOL, V24, P64, DOI 10.1093/annonc/mdt354; Li XT, 2022, ARTIF INTELL REV, V55, P4809, DOI 10.1007/s10462-021-10121-0; Li YX, 2022, APPL INTELL, V52, P9717, DOI 10.1007/s10489-021-02886-2; Liu WL, 2022, PATTERN RECOGN, V130, DOI [10.1016/j.patcog.2020.108829, 10.1016/j.patcog.2022.108829]; Ma PL, 2023, ARTIF INTELL REV, V56, P1627, DOI 10.1007/s10462-022-10209-1; Manco L, 2021, PHYS MEDICA, V83, P194, DOI 10.1016/j.ejmp.2021.03.026; Musleh MM, 2019, INT J ACAD INF SYST, V3; Oliveira S, 2021, Scientific Reports, V11, P1; Pamudurthy Vijeta, 2020, Proc (Bayl Univ Med Cent), V33, P28, DOI 10.1080/08998280.2019.1686327; Patel CI, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20247299; Paul A, 2018, IEEE T IMAGE PROCESS, V27, P4012, DOI 10.1109/TIP.2018.2834830; Priyanka, 2020, PROCEDIA COMPUT SCI, V167, P1722, DOI 10.1016/j.procs.2020.03.382; Rahaman MM, 2020, J X-RAY SCI TECHNOL, V28, P821, DOI 10.3233/XST-200715; Ren W, 2013, WORLD J GASTROENTERO, V19, P2092, DOI 10.3748/wjg.v19.i13.2092; Riasatian A, 2021, ARXIV; Saw SN, 2022, PHYS MEDICA, V100, P12, DOI 10.1016/j.ejmp.2022.06.003; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Singh R, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108111; Sirinukunwattana K, 2017, MED IMAGE ANAL, V35, P489, DOI 10.1016/j.media.2016.08.008; Sun CH, 2020, BIOCYBERN BIOMED ENG, V40, P1535, DOI 10.1016/j.bbe.2020.09.008; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Trivizakis E, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-94781-6; Wang HF, 2018, EUR J OPER RES, V267, P687, DOI 10.1016/j.ejor.2017.12.001; Wei J, 2021, IEEE WINT CONF APPL, P2472, DOI 10.1109/WACV48630.2021.00252; Xue D, 2020, IEEE ACCESS, V8, P104603, DOI 10.1109/ACCESS.2020.2999816; Yu JW, 2019, BMC NEPHROL, V20, DOI 10.1186/s12882-019-1512-x; Zeebaree DQ, 2021, CMC-COMPUT MATER CON, V66, P3363, DOI 10.32604/cmc.2021.013314; Zhao P, 2022, FRONT MICROBIOL, V13; Zhou XM, 2020, IEEE ACCESS, V8, P90931, DOI 10.1109/ACCESS.2020.2993788	56	10	10	6	24	ELSEVIER SCI LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND	1120-1797	1724-191X		PHYS MEDICA	Phys. Medica	MAR	2023	107								102534	10.1016/j.ejmp.2023.102534	http://dx.doi.org/10.1016/j.ejmp.2023.102534		FEB 2023	9	Radiology, Nuclear Medicine & Medical Imaging	Science Citation Index Expanded (SCI-EXPANDED)	Radiology, Nuclear Medicine & Medical Imaging	9P3IE	36804696				2024-09-18	WOS:000944180000001
J	Mokoatle, M; Marivate, V; Mapiye, D; Bornman, R; Hayes, VM				Mokoatle, Mpho; Marivate, Vukosi; Mapiye, Darlington; Bornman, Riana; Hayes, Vanessa. M.			A review and comparative study of cancer detection using machine learning: SBERT and SimCSE application	BMC BIOINFORMATICS			English	Review						Cancer detection; DNA; Machine learning; SentenceBert; SimCSE	COLORECTAL-CANCER; IMAGE DATABASE; CLASSIFICATION; EPIDEMIOLOGY; ALGORITHMS; NODULES; TISSUE	BackgroundUsing visual, biological, and electronic health records data as the sole input source, pretrained convolutional neural networks and conventional machine learning methods have been heavily employed for the identification of various malignancies. Initially, a series of preprocessing steps and image segmentation steps are performed to extract region of interest features from noisy features. Then, the extracted features are applied to several machine learning and deep learning methods for the detection of cancer.MethodsIn this work, a review of all the methods that have been applied to develop machine learning algorithms that detect cancer is provided. With more than 100 types of cancer, this study only examines research on the four most common and prevalent cancers worldwide: lung, breast, prostate, and colorectal cancer. Next, by using state-of-the-art sentence transformers namely: SBERT (2019) and the unsupervised SimCSE (2021), this study proposes a new methodology for detecting cancer. This method requires raw DNA sequences of matched tumor/normal pair as the only input. The learnt DNA representations retrieved from SBERT and SimCSE will then be sent to machine learning algorithms (XGBoost, Random Forest, LightGBM, and CNNs) for classification. As far as we are aware, SBERT and SimCSE transformers have not been applied to represent DNA sequences in cancer detection settings.ResultsThe XGBoost model, which had the highest overall accuracy of 73 +/- 0.13 % using SBERT embeddings and 75 +/- 0.12 % using SimCSE embeddings, was the best performing classifier. In light of these findings, it can be concluded that incorporating sentence representations from SimCSE's sentence transformer only marginally improved the performance of machine learning models.	[Mokoatle, Mpho; Marivate, Vukosi] Univ Pretoria, Dept Comp Sci, Pretoria, South Africa; [Mapiye, Darlington] CapeBio TM Technol, Centurion, South Africa; [Hayes, Vanessa. M.] Univ Sydney, Sch Med Sci, Sydney, Australia; [Bornman, Riana; Hayes, Vanessa. M.] Univ Pretoria, Sch Hlth Syst & Publ Hlth, Pretoria, South Africa	University of Pretoria; University of Sydney; University of Pretoria	Mokoatle, M (corresponding author), Univ Pretoria, Dept Comp Sci, Pretoria, South Africa.	u19394277@tuks.co.za	M, Vukosi/HPC-0925-2023; Hayes, Vanessa/GPP-2807-2022	Hayes, Vanessa/0000-0002-4524-7280				Abbasi AA, 2020, COGN NEURODYNAMICS, V14, P523, DOI 10.1007/s11571-020-09587-5; Abdullah DM., 2021, QUBAHAN ACAD J, V1, P141, DOI [10.48161/qaj.v1n2a58, DOI 10.48161/QAJ.V1N2A58]; Agarap AFM, 2015, 2ND INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND SOFT COMPUTING (ICMLSC 2018), P5, DOI 10.1145/3184066.3184080; Alanazi SA, 2021, J HEALTHC ENG, V2021, DOI 10.1155/2021/5528622; Albawi S, 2017, I C ENG TECHNOL; Amitay EL, 2019, JNCI-J NATL CANCER I, V111, P475, DOI 10.1093/jnci/djy170; [Anonymous], 2016, Curated Breast Imaging Subset of DDSM; [Anonymous], WHAT IS COL CANC; [Anonymous], 2015, Breast Cancer; [Anonymous], What is cancer?; [Anonymous], VISUALLAB METHODOLOG; [Anonymous], GEN DAT COMM DAT POR; [Anonymous], WHAT IS PROST CANC; [Anonymous], HIST IMAGES MSI VS M; [Anonymous], Zenodo; Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204; Arooj S, 2022, Frontiers in Public Health, P10; Asuntha A, 2020, MULTIMED TOOLS APPL, V79, P7731, DOI 10.1007/s11042-019-08394-3; Ausawalaithong W, 2018, BIOMED ENG INT CONF; Bade BC, 2020, CLIN CHEST MED, V41, P1, DOI 10.1016/j.ccm.2019.10.001; Barlow H, 2019, DATA, V4, DOI 10.3390/data4030129; Barta JA, 2019, ANN GLOB HEALTH, V85, DOI 10.5334/aogh.2419; Bhatia S, 2019, ADV INTELL SYST COMP, V817, P699, DOI 10.1007/978-981-13-1595-4_55; Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998; Bojanowski P., 2017, Transactions of the association for computational linguistics, V5, P135, DOI [DOI 10.1162/TACLA00051, 10.1162/tacla00051]; Breast cancer patients mris, BREAST; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Brockmoeller S, 2022, J PATHOL, V256, P269, DOI 10.1002/path.5831; Bychkov D, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21758-3; Cackowski FC, 2022, CANCER LETT, V524, P103, DOI 10.1016/j.canlet.2021.09.037; Cancer, World Health Organization; Center MM, 2009, CA-CANCER J CLIN, V59, P366, DOI 10.3322/caac.20038; Che HW, 2022, CLIN CHEM, V68, P1164, DOI 10.1093/clinchem/hvac095; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Church KW, 2017, NAT LANG ENG, V23, P155, DOI 10.1017/S1351324916000334; Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7; Damkliang K, 2021, BIOMED ENG-APP BAS C, V33, DOI 10.4015/S1016237221500228; de Carvalho AO, 2018, PATTERN RECOGN, V81, P200, DOI 10.1016/j.patcog.2018.03.032; Desai MM, 2022, JAMA NETW OPEN, V5, DOI 10.1001/jamanetworkopen.2022.2246; Devlin J, 2019, Arxiv, DOI arXiv:1810.04805; Echle A, 2020, GASTROENTEROLOGY, V159, P1406, DOI 10.1053/j.gastro.2020.06.021; Feng YJ, 2019, IEEE ACM T COMPUT BI, V16, P1794, DOI 10.1109/TCBB.2018.2835444; Gao TY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6894; Goldberg Y, 2014, Arxiv, DOI arXiv:1402.3722; Gray R, 2007, LANCET, V370, P2020, DOI 10.1016/s0140-6736(07)61866-2; Harbeck N, 2019, NAT REV DIS PRIMERS, V5, DOI [10.1038/s41572-019-0111-2, 10.1038/s41572-019-0122-z]; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Hassan MR, 2022, FUTURE GENER COMP SY, V127, P462, DOI 10.1016/j.future.2021.09.030; Hegde PS, 2020, IMMUNITY, V52, P17, DOI 10.1016/j.immuni.2019.12.011; Ho CW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-06264-x; Hosseinzadeh M, 2022, EUR RADIOL, V32, P2224, DOI 10.1007/s00330-021-08320-y; Hussain L, 2019, CURR MED IMAGING, V15, P595, DOI 10.2174/1573405614666180718123533; Hussain L, 2018, CANCER BIOMARK, V21, P393, DOI 10.3233/CBM-170643; Iqbal MJ, 2021, CANCER CELL INT, V21, DOI 10.1186/s12935-021-01981-1; Iqbal S, 2021, IEEE ACCESS, V9, P27085, DOI 10.1109/ACCESS.2021.3057654; Janowczyk A., USE CASE 6 INVASIVE; Jones PA, 2007, CELL, V128, P683, DOI 10.1016/j.cell.2007.01.029; Kaggle, LUNG COL CANC HIST I; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Ke GL, 2017, ADV NEUR IN, V30; KELSEY JL, 1991, CA-CANCER J CLIN, V41, P146, DOI 10.3322/canjclin.41.3.146; Khan MBS, 2022, MATH BIOSCI ENG, V19, P7978, DOI 10.3934/mbe.2022373; Khuriwal N., 2018, 2018 3 INT C WORKSH, P1, DOI DOI 10.1109/ICRAIE.2018.8710426; Kourou K, 2015, COMPUT STRUCT BIOTEC, V13, P8, DOI 10.1016/j.csbj.2014.11.005; Li JQ, 2021, BRIEF BIOINFORM, V22, DOI 10.1093/bib/bbab250; Liu YH, 2019, Arxiv, DOI arXiv:1907.11692; Lorenzovici N, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11030514; Loud JT, 2017, SEMIN ONCOL NURS, V33, P121, DOI 10.1016/j.soncn.2017.02.002; Macenko M, 2009, I S BIOMED IMAGING, P1107, DOI 10.1109/ISBI.2009.5193250; Mambou SJ, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092799; Masud M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030748; Meng Y, 2021, Advances in Neural Information Processing Systems, V34; Moreira IC, 2012, ACAD RADIOL, V19, P236, DOI 10.1016/j.acra.2011.09.014; Muti H. S., 2020, The Aachen Protocol for Deep Learning Histopathology: A hands-on guide for data preprocessing, DOI 10.5281/zenodo.3694994; Naseer I, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22124426; Nasir MU, 2022, Computational Intelligence and Neuroscience, V2022; Natarajan S., 2020, Prostate MRI and Ultrasound with Pathology and Coordinates of Tracked Biopsy (Prostate-MRI-US-Biopsy), V10, P7937; Nguyen L, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-31666-w; O'Shea K, 2015, Arxiv, DOI [arXiv:1511.08458, DOI 10.48550/ARXIV.1511.08458]; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pennington J., 2014, P 2014 C EMP METH NA, P1532; Poulos RC, 2019, JNCI CANCER SPECT, V3, DOI 10.1093/jncics/pkz012; Bowman SR, 2015, Arxiv, DOI arXiv:1508.05326; Radhika PR., 2019, P 2019 3 IEEE INT C, VCoimbatore, P1, DOI DOI 10.1109/ICECCT.2019.8869001; Reda I, 2018, TECHNOL CANCER RES T, V17, DOI 10.1177/1533034618775530; Reimers N, 2019, Arxiv, DOI [arXiv:1908.10084, 10.48550/arXiv.1908.10084]; Remya R., 2022, 2022 International Conference on Electronics and Renewable Systems (ICEARS), P1060, DOI 10.1109/ICEARS53579.2022.9751974; Rodrigues MB, 2018, IEEE ACCESS, V6, P18592, DOI 10.1109/ACCESS.2018.2817614; Roy A, 2019, 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P169, DOI 10.1109/UEMCON47517.2019.8993023; Saba T, 2020, J INFECT PUBLIC HEAL, V13, P1274, DOI 10.1016/j.jiph.2020.06.033; Salaken SM, 2017, CAN CON EL COMP EN; Sarwinda D, 2021, PROCEDIA COMPUT SCI, V179, P423, DOI 10.1016/j.procs.2021.01.025; Setio AAA, 2017, MED IMAGE ANAL, V42, P1, DOI 10.1016/j.media.2017.06.015; Shakeel PM, 2020, NEURAL COMPUT APPL, V32, P777, DOI 10.1007/s00521-018-03972-2; Sharma S, 2020, J DIGIT IMAGING, V33, P632, DOI 10.1007/s10278-019-00307-y; Sharma S, 2018, PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON COMPUTATIONAL TECHNIQUES, ELECTRONICS AND MECHANICAL SYSTEMS (CTEMS), P114, DOI 10.1109/CTEMS.2018.8769187; Shen L, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-48995-4; Shin H, 2020, ACS NANO, V14, P5435, DOI 10.1021/acsnano.9b09119; Shiraishi J, 2000, AM J ROENTGENOL, V174, P71, DOI 10.2214/ajr.174.1.1740071; Sonn GA, 2013, J UROLOGY, V189, P86, DOI 10.1016/j.juro.2012.08.095; SUCKLING J, 1994, INT CONGR SER, V1069, P375; Sun YS, 2017, INT J BIOL SCI, V13, P1387, DOI 10.7150/ijbs.21635; Tahmooresi M., 2018, J. Telecommun., Elect. Comput. Engg., V10, P21; Taylor J, 2019, BMJ OPEN, V9, DOI 10.1136/bmjopen-2019-030618; Tissue Image Analytics (TIA) Centre, WARW; Tolkach Y, 2020, NAT MACH INTELL, V2, P411, DOI 10.1038/s42256-020-0200-7; Tsuneki M, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12030768; UCI Center for Machine Learning and Intelligent Systems, 1987, UCI Machine Learning Repository; Vaka AR, 2020, ICT EXPRESS, V6, P320, DOI 10.1016/j.icte.2020.04.009; VANDENBRANDT PA, 1990, J CLIN EPIDEMIOL, V43, P285, DOI 10.1016/0895-4356(90)90009-E; VRI, BREAST CANC HIST DAT; Waks AG, 2019, JAMA-J AM MED ASSOC, V321, P288, DOI 10.1001/jama.2018.19323; Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369; Wang YH, 2019, STUD HEALTH TECHNOL, V264, P438, DOI 10.3233/SHTI190259; Weitz J, 2005, LANCET, V365, P153, DOI 10.1016/S0140-6736(19)32319-0; Williams A, 2018, Arxiv, DOI arXiv:1704.05426; Wu ZF, 2020, Arxiv, DOI arXiv:2012.15466; Yamashita R, 2021, LANCET ONCOL, V22, P132, DOI 10.1016/S1470-2045(20)30535-0; Yoo S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-55972-4; Zheng R S, 2019, Zhonghua Zhong Liu Za Zhi, V41, P19, DOI 10.3760/cma.j.issn.0253-3766.2019.01.005; Zhou DJ, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16777-6	122	12	12	7	22	BMC	LONDON	CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	MAR 23	2023	24	1							112	10.1186/s12859-023-05235-x	http://dx.doi.org/10.1186/s12859-023-05235-x			25	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Science Citation Index Expanded (SCI-EXPANDED)	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	A2ZR2	36959534	Green Published, gold, Green Submitted			2024-09-18	WOS:000953870500003
J	Leo, M; Carcagni, P; Signore, L; Corcione, F; Benincasa, G; Laukkanen, MO; Distante, C				Leo, Marco; Carcagni, Pierluigi; Signore, Luca; Corcione, Francesco; Benincasa, Giulio; Laukkanen, Mikko; Distante, Cosimo			Convolutional Neural Networks in the Diagnosis of Colon Adenocarcinoma	AI			English	Article						colon cancer; histological diagnosis; artificial intelligence; deep learning; transformer networks; dataset	COLORECTAL-CARCINOMA	Colorectal cancer is one of the most lethal cancers because of late diagnosis and challenges in the selection of therapy options. The histopathological diagnosis of colon adenocarcinoma is hindered by poor reproducibility and a lack of standard examination protocols required for appropriate treatment decisions. In the current study, using state-of-the-art approaches on benchmark datasets, we analyzed different architectures and ensembling strategies to develop the most efficient network combinations to improve binary and ternary classification. We propose an innovative two-stage pipeline approach to diagnose colon adenocarcinoma grading from histological images in a similar manner to a pathologist. The glandular regions were first segmented by a transformer architecture with subsequent classification using a convolutional neural network (CNN) ensemble, which markedly improved the learning efficiency and shortened the learning time. Moreover, we prepared and published a dataset for clinical validation of the developed artificial neural network, which suggested the discovery of novel histological phenotypic alterations in adenocarcinoma sections that could have prognostic value. Therefore, AI could markedly improve the reproducibility, efficiency, and accuracy of colon cancer diagnosis, which are required for precision medicine to personalize the treatment of cancer patients.	[Leo, Marco; Carcagni, Pierluigi; Distante, Cosimo] Inst Appl Sci & Intelligent Syst ISASI, Natl Res Council CNR Italy, I-73100 Lecce, Italy; [Signore, Luca; Distante, Cosimo] Univ Salento, Dipartimento Ingn Innovaz, I-73100 Lecce, Italy; [Corcione, Francesco] Clin Mediterranea, I-80122 Naples, Italy; [Benincasa, Giulio] Italo Fdn, I-20146 Milan, Italy; [Laukkanen, Mikko] Univ Naples Federico II, Dept Translat Med Sci, I-80131 Naples, Italy	Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienze Applicate e Sistemi Intelligenti "Eduardo Caianiello" (ISASI-CNR); University of Salento; Mediterranean Clinic; University of Naples Federico II	Laukkanen, MO (corresponding author), Univ Naples Federico II, Dept Translat Med Sci, I-80131 Naples, Italy.	marco.leo@cnr.it; pierluigi.carcagni@cnr.it; luca.signore@unisalento.it; francesco.corcione@unina.it; giulio.benincasa@pinetagrande.it; mikko.laukkanen@utu.fi; cosimo.distante@cnr.it	Carcagni, Pierluigi/AAY-6373-2020; Leo, Marco/AAG-6296-2019; Maione, Francesco/HLG-5940-2023; Benincasa, Giulio/I-5858-2018; Laukkanen, Mikko/ABS-9904-2022; Distante, Cosimo/M-7996-2013; Laukkanen, Mikko/K-5868-2016	Distante, Cosimo/0000-0002-1073-2390; Leo, Marco/0000-0001-5636-6130; Laukkanen, Mikko/0000-0002-1004-7131; Benincasa, Giulio/0000-0002-9060-2915; CARCAGNI', PIERLUIGI/0000-0003-3447-2922	Campania Region POR CUP	Campania Region POR CUP	No Statement Available	Altunbay D, 2010, IEEE T BIO-MED ENG, V57, P665, DOI 10.1109/TBME.2009.2033804; Awan R, 2020, COMPUT ELECTR ENG, V85, DOI 10.1016/j.compeleceng.2019.106450; Awan R, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16516-w; Bokhorst JM, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-35491-z; Bousis D, 2023, GASTROENTEROL REV, V18, P266, DOI 10.5114/pg.2023.129494; Cammarota F, 2016, STEM CELLS INT, V2016, DOI 10.1155/2016/4824573; Carcagnì P, 2019, LECT NOTES COMPUT SC, V11751, P335, DOI 10.1007/978-3-030-30642-7_30; Chen KBY, 2021, CURR ONCOL, V28, P5356, DOI 10.3390/curroncol28060447; Chen WF, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12081916; Ciompi F, 2017, I S BIOMED IMAGING, P160, DOI 10.1109/ISBI.2017.7950492; Compton CC, 2007, CLIN CANCER RES, V13, p6862S, DOI 10.1158/1078-0432.CCR-07-1398; Deng SJ, 2020, FRONT MED-PRC, V14, P470, DOI 10.1007/s11684-020-0782-9; Eun DI, 2020, COMPUT METH PROG BIO, V196, DOI 10.1016/j.cmpb.2020.105615; Fleming M, 2012, J GASTROINTEST ONCOL, V3, P153, DOI 10.3978/j.issn.2078-6891.2012.030; Gertych A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-37638-9; Gupta P, 2019, CANCERS, V11, DOI 10.3390/cancers11122007; Nguyen HT, 2018, ONCOL LETT, V16, P9, DOI 10.3892/ol.2018.8679; Harada S, 2020, ADV ANAT PATHOL, V27, P20, DOI 10.1097/PAP.0000000000000247; HERMANEK P, 1989, BAILLIERE CLIN GASTR, V3, P511, DOI 10.1016/0950-3528(89)90015-8; Ho CW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-06264-x; Hou L, 2016, PROC CVPR IEEE, P2424, DOI 10.1109/CVPR.2016.266; Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244; Klaver CEL, 2018, ANN SURG ONCOL, V25, P212, DOI 10.1245/s10434-017-6037-6; Lanza G, 2011, DIGEST LIVER DIS, V43, pS344, DOI 10.1016/S1590-8658(11)60590-2; Liang P, 2007, ANN SURG ONCOL, V14, P470, DOI 10.1245/s10434-006-9189-3; Maffeis V, 2019, FRONT ONCOL, V9, DOI 10.3389/fonc.2019.01255; Maguire A, 2014, WORLD J GASTROENTERO, V20, P9850, DOI 10.3748/wjg.v20.i29.9850; Marques G, 2022, MULTIMED TOOLS APPL, V81, P28061, DOI 10.1007/s11042-022-12624-6; Montagnon E, 2020, INSIGHTS IMAGING, V11, DOI 10.1186/s13244-019-0832-5; Pei Y, 2020, IEEE ACCESS, V8, P64131, DOI 10.1109/ACCESS.2020.2982543; Puppa G, 2010, ARCH PATHOL LAB MED, V134, P837, DOI 10.1043/1543-2165-134.6.837; Radosavovic I, 2019, IEEE I CONF COMP VIS, P1882, DOI 10.1109/ICCV.2019.00197; Reis HC, 2023, J DIGIT IMAGING, V36, P306, DOI 10.1007/s10278-022-00701-z; Salvi M, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104129; Shaban M, 2020, IEEE T MED IMAGING, V39, P2395, DOI 10.1109/TMI.2020.2971006; Shakeel PM, 2022, NEURAL COMPUT APPL, V34, P9579, DOI 10.1007/s00521-020-04842-6; Shi QS, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107978; Sirinukunwattana K, 2018, LECT NOTES COMPUT SC, V11071, P192, DOI 10.1007/978-3-030-00934-2_22; Sirinukunwattana K, 2017, MED IMAGE ANAL, V35, P489, DOI 10.1016/j.media.2016.08.008; Swiderska-Chadaj Z, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101547; Testa Ugo, 2018, Med Sci (Basel), V6, DOI 10.3390/medsci6020031; Vuong TLT, 2020, 2020 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC); Tong YL, 2020, PATHOL RES PRACT, V216, DOI 10.1016/j.prp.2020.153073; Tsai MJ, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10141662; Tummala S, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13091594; Ueno H, 2012, AM J SURG PATHOL, V36, P193, DOI 10.1097/PAS.0b013e318235edee; Vaswani A, 2017, ADV NEUR IN, V30; Wang EK, 2019, CELLS-BASEL, V8, DOI 10.3390/cells8050499; Zhan ZW, 2022, INT J SOFTW SCI COMP, V14, DOI 10.4018/IJSSCI.311446; Zhang L, 2017, IEEE T CYBERNETICS, V47, P3243, DOI 10.1109/TCYB.2016.2588526; Zhang ZC, 2018, IEEE T MED IMAGING, V37, P1407, DOI 10.1109/TMI.2018.2823338; Zhou YN, 2019, IEEE INT CONF COMP V, P388, DOI 10.1109/ICCVW.2019.00050	52	0	0	1	1	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		2673-2688		AI-BASEL	AI	MAR	2024	5	1					324	341		10.3390/ai5010016	http://dx.doi.org/10.3390/ai5010016			18	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Emerging Sources Citation Index (ESCI)	Computer Science	MC8V9		gold			2024-09-18	WOS:001191528800001
J	Zhu, JB; Ge, MF; Chang, ZM; Dong, WF				Zhu, Jianbo; Ge, Mingfeng; Chang, Zhimin; Dong, Wenfei			GCCSwin-UNet: Global Context and Cross-Shaped Windows Vision Transformer Network for Polyp Segmentation	PROCESSES			English	Article						deep learning; colorectal cancer; colonoscopy images; vision transformer; medical image segmentation	COLORECTAL-CANCER	Accurate polyp segmentation is of great importance for the diagnosis and treatment of colon cancer. Convolutional neural networks (CNNs) have made significant strides in the processing of medical images in recent years. The limited structure of convolutional operations prevents CNNs from learning adequately about global and long-range semantic information interactions, despite the remarkable performance they have attained. Therefore, the GCCSwin-UNet framework is suggested in this study. Specifically, the model utilizes an encoder-decoder structure, using the patch-embedding layer for feature downsampling and the CSwin Transformer block as the encoder for contextual feature extraction. To restore the feature map's spatial resolution during upsampling operations, a symmetric decoder and patch expansion layer are also created. In order to help the backbone module to do better feature learning, we also create a global context module (GCM) and a local position-enhanced module (LPEM). We conducted extensive experiments on the Kvasir-SEG and CVC-ClinicDB datasets, and compared them with existing methods. GCCSwin-UNet reached remarkable results with Dice and MIoU of 86.37% and 83.19% for Kvasir-SEG, respectively, and 91.26% and 84.65% for CVC-ClinicDB, respectively. Finally, quantitative analysis and statistical tests are applied to further demonstrate the validity and plausibility of our method.	[Zhu, Jianbo; Dong, Wenfei] Shandong Univ Tradit Chinese Med, Sch Intelligence & Informat Engn, Jinan 250355, Peoples R China; [Zhu, Jianbo; Ge, Mingfeng; Chang, Zhimin; Dong, Wenfei] Chinese Acad Sci, Suzhou Inst Biomed Engn & Technol, Suzhou 215163, Peoples R China	Shandong University of Traditional Chinese Medicine; Chinese Academy of Sciences; Suzhou Institute of Biomedical Engineering & Technology, CAS	Ge, MF (corresponding author), Chinese Acad Sci, Suzhou Inst Biomed Engn & Technol, Suzhou 215163, Peoples R China.	gemf@sibet.ac.cn	zhu, jianbo/GQR-1845-2022	dong, wen fei/0000-0003-1319-3166	National Key R&D Program of China [2021YFB3602200]	National Key R&D Program of China	This study was funded by the National Key R&D Program of China (No. 2021YFB3602200).	Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Barua I, 2021, ENDOSCOPY, V53, P277, DOI 10.1055/a-1201-7165; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Biller LH, 2021, JAMA-J AM MED ASSOC, V325, P669, DOI 10.1001/jama.2021.0106; Brown JRG, 2022, CLIN GASTROENTEROL H, V20, P1499, DOI 10.1016/j.cgh.2021.09.009; Cao H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2105.05537; Chen LC, 2017, Arxiv, DOI arXiv:1706.05587; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chu XX, 2021, Arxiv, DOI [arXiv:2102.10882, 10.48550/arXiv.2102.10882]; Ciardiello F, 2022, CA-CANCER J CLIN, V72, P372, DOI 10.3322/caac.21728; de Lange T., 2020, P INT C MULT MOD DAE; Dong X., 2022, P IEEECVF C COMPUTER; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Du N, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0155371; Fan D.P., 2020, P INT C MEDICAL IMAG; Fang Y., 2019, P INT C MED IM COMP; Gao YH, 2021, LECT NOTES COMPUT SC, V12903, P61, DOI 10.1007/978-3-030-87199-4_6; Gross S., 2009, Algorithmen - Systeme - Anwendungen Proceedings des Workshops, Bildverarbeitung fur die Medizin, P252, DOI 10.1007/978-3-540-93860-6_51; Ho JAT, 2019, Arxiv, DOI [arXiv:1912.12180, DOI 10.48550/ARXIV.1912.12180]; Hwang S, 2007, IEEE IMAGE PROC, P1029; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14; Le Alexander, 2021, Int J Clin Res Trials, V6, DOI 10.15344/2456-8007/2021/157; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Ronneberger O., 2015, MICCAL; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Shaw P, 2018, Arxiv, DOI [arXiv:1803.02155, 10.48550/arXiv.1803.02155]; Siegel RL, 2020, CA-CANCER J CLIN, V70, P7, DOI [10.3322/caac.21590, 10.3322/caac.21601]; Tian Y, 2021, Arxiv, DOI arXiv:2101.03285; Tolstikhin I.O., 2021, Advances in Neural Information Processing Systems, V34; Turner JK, 2013, EUR J GASTROEN HEPAT, V25, P562, DOI 10.1097/MEG.0b013e32835d1f2d; Valanarasu Jeya Maria Jose, 2021, Medical Image Computing and Computer Assisted Intervention - MICCAI 2021: 24th International Conference, Proceedings. Lecture Notes in Computer Science, Image Processing, Computer Vision, Pattern Recognition, and Graphics (12901), P36, DOI 10.1007/978-3-030-87193-2_4; Vaswani A, 2017, ADV NEUR IN, V30; Xu JJ, 2019, ADV NEUR IN, V32; Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944; Zhu JB, 2023, BIOMED SIGNAL PROCES, V83, DOI 10.1016/j.bspc.2023.104593	37	1	1	8	31	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		2227-9717		PROCESSES	Processes	APR	2023	11	4							1035	10.3390/pr11041035	http://dx.doi.org/10.3390/pr11041035			14	Engineering, Chemical	Science Citation Index Expanded (SCI-EXPANDED)	Engineering	E8AC5		gold			2024-09-18	WOS:000977696300001
C	Lv, ZL; Yan, R; Lin, YX; Wang, Y; Zhang, F		Wang, L; Dou, Q; Fletcher, PT; Speidel, S; Li, S		Lv, Zhilong; Yan, Rui; Lin, Yuexiao; Wang, Ying; Zhang, Fa			Joint Region-Attention and Multi-scale Transformer for Microsatellite Instability Detection from Whole Slide Images in Gastrointestinal Cancer	MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION, MICCAI 2022, PT II	Lecture Notes in Computer Science		English	Proceedings Paper	25th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)	SEP 18-22, 2022	Singapore, SINGAPORE	MICCAI Soc		Microsatellite instability; Gastrointestinal cancer; Region attention; Transformer	PREDICTION; MODEL	Microsatellite instability (MSI) is a crucial biomarker to clinical immunotherapy in gastrointestinal cancer, while additional immunohistochemical or genetic tests for MSI are generally missing due to lack of medical resources. Deep learning has achieved promising performance in detecting MSI from hematoxylin and eosin (H&E) stained histopathology slides. However, these methods are primarily based on patch-supervised slide-label models and then aggregate patch-level results into the slideslevel result, resulting unstable prediction due to noisy patches and aggregation ways. In this paper, we propose a joint region-attention and multi-scale transformer (RAMST) network for microsatellite instability detection from whole slide images in gastrointestinal cancer. Specifically, we present a region-attention mechanism and a feature weight uniform sampling (FWUS) method to learn a representative subset of image patches from whole slide images. Moreover, we introduce the transformer architecture to fuse the multi-scale histopathology features consisting of patch-level features with region-level features to characterize the whole slide images for slide-level MSI detection. Compared to the existing MSI detection methods, the proposed RAMST shows the best performances on the colorectal and stomach cancer dataset from The Cancer Genome Atlas (TCGA) and provides an effective features representation learning method for WSI-label tasks.	[Lv, Zhilong; Lin, Yuexiao; Zhang, Fa] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China; [Lv, Zhilong; Yan, Rui] Univ Chinese Acad Sci, Beijing, Peoples R China; [Lin, Yuexiao] Capital Med Univ, Beijing Chaoyang Hosp, Dept Gen Surg, Beijing, Peoples R China; [Wang, Ying] Capital Med Univ, Beijing Chaoyang Hosp, Dept Pathol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Capital Medical University; Capital Medical University	Zhang, F (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.	zhangfa@ict.ac.cn			Strategic Priority Research Program of the Chinese Academy of Sciences [XDA16021400]; NSFC [61932018, 62072441, 62072280]	Strategic Priority Research Program of the Chinese Academy of Sciences(Chinese Academy of Sciences); NSFC(National Natural Science Foundation of China (NSFC))	The research is supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (No. XDA16021400), and the NSFC Projects Grants (61932018, 62072441 and 62072280).	[Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386; [Anonymous], 2020, ARXIV; Bhargava R, 2016, ANNU REV BIOMED ENG, V18, P387, DOI 10.1146/annurev-bioeng-112415-114722; Bilal M, 2021, LANCET DIGIT HEALTH, V3, pE763, DOI 10.1016/S2589-7500(21)00180-1; Boland CR, 2010, GASTROENTEROLOGY, V138, P2073, DOI [10.1053/j.gastro.2009.12.064, 10.1053/j.gastro.2010.04.024]; Cao R, 2020, THERANOSTICS, V10, P11080, DOI 10.7150/thno.49864; Echle A., 2021, ImmunoInformatics, V3-4, DOI [DOI 10.1016/J.IMMUNO.2021.100008, 10.1016/J.IMMUNO.2021.100008]; Echle A, 2020, GASTROENTEROLOGY, V159, P1406, DOI 10.1053/j.gastro.2020.06.021; He K., 2021, ARXIV, P16000, DOI [DOI 10.48550/ARXIV.2111.06377, 10.1109/CVPR52688.2022.01553]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kather JN, 2019, NAT MED, V25, P1054, DOI 10.1038/s41591-019-0462-y; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Schirris Y, 2021, Arxiv, DOI [arXiv:2107.09405, 10.48550/arXiv.2107.09405]; Schmauch B, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17678-4; Vaswani A, 2017, ADV NEUR IN, V30; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Yamashita R, 2021, IEEE T MED IMAGING, V40, P3945, DOI 10.1109/TMI.2021.3101985; Yamashita R, 2021, LANCET ONCOL, V22, P132, DOI 10.1016/S1470-2045(20)30535-0	18	5	5	1	11	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	0302-9743	1611-3349	978-3-031-16434-7; 978-3-031-16433-0	LECT NOTES COMPUT SC			2022	13432						293	302		10.1007/978-3-031-16434-7_29	http://dx.doi.org/10.1007/978-3-031-16434-7_29			10	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Radiology, Nuclear Medicine & Medical Imaging	BU0DF					2024-09-18	WOS:000867288800029
J	Iqbal, A; Ahmed, Z; Usman, M; Malik, I				Iqbal, Ahmed; Ahmed, Zohair; Usman, Muhammad; Malik, Isra			Rethinking encoder-decoder architecture using vision transformer for colorectal polyp and surgical instruments segmentation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE			English	Article						Vision transformer; Colon polyp segmentation; Semantic segmentation network; Surgical instruments segmentation		Accurate polyp segmentation from colonoscopy images is important for the immediate diagnosis and effective treatment of colon cancer. While significant progress has been made in the polyps segmentation task, there are various challenges that need to be addressed. Polyps can vary greatly in size and shape, and often has no clear boundary between surrounding tissues. Furthermore, surgical instrument segmentation can aid surgeons with the precise positioning and orientation of the instruments, helping them to plan the next steps in the robot-assisted surgery. The proposed colorectal polyp segmentation Transformer (CPS-Former) uses innovative attention blocks in a network that encodes-decodes features like classic Semantic Segmentation Network (SegNet). However, it has special self-attention modules with small convolutional kernels that efficiently extract information from different feature-channels. Moreover, it is equipped with an effective positional embedding to capture information from a large area of context for long distance interactions. Additionally, a fusion block is embedded for scaling-attention that combines the outputs from the encoder-decoder blocks to enhance the semantic features and reduce the non-semantic ones. Transformer encoder blocks also modified by adding a local feedforward layer and skips connections, and adjust the channel sizes to reduce the model trainable parameters. We evaluate our colorectal polyp segmentation network (CPS-Former) on four colorectal polyp public datasets and one surgical instrument segmentation dataset, which show its superiority over other state-of-the-art polyp segmentation models. Our implementation source code and network weights are available at GitHub: https://github.com/ah medeqbal/CPS-Former.	[Iqbal, Ahmed] Sir Syed CASE Inst Technol, Dept Comp Sci, Islamabad, Pakistan; [Iqbal, Ahmed; Ahmed, Zohair; Usman, Muhammad] SZABIST Univ, Predict Analyt Lab, Islamabad Campus, Islamabad, Pakistan; [Ahmed, Zohair] Int Islamic Univ, Dept Comp Sci & Software Engn, Islamabad, Pakistan; [Malik, Isra] COMSATS Univ Islamabad, Dept Comp Sci, Wah Campus, Wah Cantt, Pakistan	Shaheed Zulfikar Ali Bhutto Institute of Science & Technology; International Islamic University, Pakistan; COMSATS University Islamabad (CUI)	Iqbal, A (corresponding author), Sir Syed CASE Inst Technol, Dept Comp Sci, Islamabad, Pakistan.	ahmedeqbal@gmail.com	iqbal, Ahmed/GWQ-5048-2022; Ahmed, Zohair/HJH-7082-2023	Ahmed, Zohair/0000-0002-8228-0784; Iqbal, Ahmed/0000-0003-4946-4167			This work has been conducted under the Predictive Analytics Lab which has been established by Pakistan's National Center for Big Data and Cloud Computing (NCBC) under the supervision of the Higher Ed-ucation Commission (HEC) and Planning Commission of Pakistan.	Alam MJ, 2023, COMPUT BIOL MED, V160, DOI 10.1016/j.compbiomed.2023.106945; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dong B., 2023, CAAI Artif. Intell. Res, V2, P9150015, DOI [10.26599/AIR.2023.9150015, DOI 10.26599/AIR.2023.9150015]; Elkarazle K, 2023, IEEE ACCESS, V11, P69295, DOI 10.1109/ACCESS.2023.3291783; Hu KL, 2023, COMPUT BIOL MED, V160, DOI 10.1016/j.compbiomed.2023.107028; Iqbal A, 2023, KNOWL-BASED SYST, V267, DOI 10.1016/j.knosys.2023.110393; Jha Debesh, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12573), P218, DOI 10.1007/978-3-030-67835-7_19; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Khan TM, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.106023; Lin S, 2021, IEEE ROBOT AUTOM LET, V6, P6773, DOI 10.1109/LRA.2021.3096156; Lin Y, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108917; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Oktay O., 2018, Medical Imaging with Deep Learning, DOI DOI 10.48550/ARXIV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shu X., 2023, 2023 International Conference on CyberPhysical Social Intelligence (ICCSI), P435; Shu X, 2024, IEEE T IND INFORM, V20, P6099, DOI 10.1109/TII.2023.3342442; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Su R, 2021, FRONT GENET, V12, DOI 10.3389/fgene.2021.639930; Su YZ, 2023, NEUROCOMPUTING, V545, DOI 10.1016/j.neucom.2023.126233; Sun YW, 2021, IEEE ROBOT AUTOM LET, V6, P3870, DOI 10.1109/LRA.2021.3066956; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Wang CW, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106168; Wang Junwen, 2023, Comput Biol Med, V161, P107038, DOI 10.1016/j.compbiomed.2023.107038; Yang L, 2022, COMPUT BIOL MED, V151, DOI 10.1016/j.compbiomed.2022.106216; Yang L, 2022, IEEE T MED ROBOT BIO, V4, P696, DOI 10.1109/TMRB.2022.3193420; Yu T, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104953; Yue GH, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3244219; Zhang YY, 2023, BIOMED SIGNAL PROCES, V86, DOI 10.1016/j.bspc.2023.105133; Zhao X., 2021, Automatic Polyp Segmentation via Multi-scale Subtraction Network Medical Image Computing and Computer Assisted Intervention (MICCAI) 2021, DOI [10.1007/978-3-030-87193-2_12, DOI 10.1007/978-3-030-87193-2_12]; Zhou T, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109555	32	0	0	6	6	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0952-1976	1873-6769		ENG APPL ARTIF INTEL	Eng. Appl. Artif. Intell.	OCT	2024	136		B						108962	10.1016/j.engappai.2024.108962	http://dx.doi.org/10.1016/j.engappai.2024.108962		JUL 2024	12	Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Automation & Control Systems; Computer Science; Engineering	A6H4W					2024-09-18	WOS:001283523600001
J	Lo, CM; Jiang, JK; Lin, CC				Lo, Chung-Ming; Jiang, Jeng-Kai; Lin, Chun-Chi			Detecting microsatellite instability in colorectal cancer using Transformer-based colonoscopy image classification and retrieval	PLOS ONE			English	Article							INSTITUTE WORKSHOP	Colorectal cancer (CRC) is a major global health concern, with microsatellite instability-high (MSI-H) being a defining characteristic of hereditary nonpolyposis colorectal cancer syndrome and affecting 15% of sporadic CRCs. Tumors with MSI-H have unique features and better prognosis compared to MSI-L and microsatellite stable (MSS) tumors. This study proposed establishing a MSI prediction model using more available and low-cost colonoscopy images instead of histopathology. The experiment utilized a database of 427 MSI-H and 1590 MSS colonoscopy images and vision Transformer (ViT) with different feature training approaches to establish the MSI prediction model. The accuracy of combining pre-trained ViT features was 84% with an area under the receiver operating characteristic curve of 0.86, which was better than that of DenseNet201 (80%, 0.80) in the experiment with support vector machine. The content-based image retrieval (CBIR) approach showed that ViT features can obtain a mean average precision of 0.81 compared to 0.79 of DenseNet201. ViT reduced the issues that occur in convolutional neural networks, including limited receptive field and gradient disappearance, and may be better at interpreting diagnostic information around tumors and surrounding tissues. By using CBIR, the presentation of similar images with the same MSI status would provide more convincing deep learning suggestions for clinical use.	[Lo, Chung-Ming] Natl Chengchi Univ, Grad Inst Lib Informat & Archival Studies, Taipei, Taiwan; [Jiang, Jeng-Kai; Lin, Chun-Chi] Taipei Vet Gen Hosp, Dept Surg, Div Colon & Rectal Surg, Taipei, Taiwan; [Jiang, Jeng-Kai; Lin, Chun-Chi] Natl Yang Ming Chiao Tung Univ, Sch Med, Dept Surg, Taipei, Taiwan	National Chengchi University; Taipei Veterans General Hospital; National Yang Ming Chiao Tung University	Lin, CC (corresponding author), Taipei Vet Gen Hosp, Dept Surg, Div Colon & Rectal Surg, Taipei, Taiwan.; Lin, CC (corresponding author), Natl Yang Ming Chiao Tung Univ, Sch Med, Dept Surg, Taipei, Taiwan.	cclin15@vghtpe.gov.tw		Lo, Chung-Ming/0000-0001-5068-6002	Ministry of Science and Technology of Taiwan [MOST 111-2221-E-004-012]; VGHUST Joint Research Program [VGHUST112-G1-4-1, VGHUST112-G1-4-2]	Ministry of Science and Technology of Taiwan(Ministry of Science and Technology, Taiwan); VGHUST Joint Research Program	The authors thank the Ministry of Science and Technology of Taiwan (MOST 111-2221-E-004-012) and VGHUST Joint Research Program (VGHUST112-G1-4-1, VGHUST112-G1-4-2) for financially supporting this study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.	Alexander J, 2001, AM J PATHOL, V158, P527, DOI 10.1016/S0002-9440(10)63994-6; Alkadri S, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104770; Benson AB, 2021, J Natl Compr Canc Netw, V19, P329, DOI DOI 10.6004/JNCCN.2021.0012; Berg KD, 2000, J MOL DIAGN, V2, P20, DOI 10.1016/S1525-1578(10)60611-3; Boland CR, 2010, GASTROENTEROLOGY, V138, P2073, DOI [10.1053/j.gastro.2009.12.064, 10.1053/j.gastro.2010.04.024]; Boland CR, 1998, CANCER RES, V58, P5248; Cercek A, 2022, NEW ENGL J MED, V386, P2363, DOI 10.1056/NEJMoa2201445; Chang XA, 2023, CELL REP MED, V4, DOI 10.1016/j.xcrm.2022.100914; Chen J, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105878; Chen ML, 2018, INT J CLIN EXP PATHO, V11, P1431; Chen W, 2017, DIAGN PATHOL, V12, P1, DOI 10.1186/s13000-017-0613-8; Cheng DT, 2017, BMC MED GENOMICS, V10, DOI 10.1186/s12920-017-0271-4; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Echle A, 2022, ESMO OPEN, V7, DOI 10.1016/j.esmoop.2022.100400; Echle A, 2020, GASTROENTEROLOGY, V159, P1406, DOI 10.1053/j.gastro.2020.06.021; Esfahani AT, 2019, J CELL PHYSIOL, V234, P13128, DOI 10.1002/jcp.27983; Fujii S, 2022, CLIN CANCER RES, V28, P2623, DOI 10.1158/1078-0432.CCR-21-4391; Guo BW, 2023, J PATHOL CLIN RES, V9, P223, DOI 10.1002/cjp2.312; He K, 2020, BMC MED IMAGING, V20, DOI 10.1186/s12880-020-00457-4; Hildebrand LA, 2021, CANCERS, V13, DOI 10.3390/cancers13030391; Hissong E, 2018, MODERN PATHOL, V31, P1756, DOI 10.1038/s41379-018-0094-7; Hu JF, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.848798; Komura D, 2022, CELL REP, V38, DOI 10.1016/j.celrep.2022.110424; Lam CS, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0253890; Lee SH, 2021, INT J CANCER, V149, P728, DOI 10.1002/ijc.33599; Leiby Jacob S, 2022, Annu Int Conf IEEE Eng Med Biol Soc, V2022, P3068, DOI 10.1109/EMBC48229.2022.9871553; Liang CW, 2021, CANCERS, V13, DOI 10.3390/cancers13225787; Lin CC, 2014, J SURG ONCOL, V110, P451, DOI 10.1002/jso.23675; Lo CM, 2023, COMPUT MED IMAG GRAP, V107, DOI 10.1016/j.compmedimag.2023.102242; Lo CM, 2023, COMPUT METH PROG BIO, V237, DOI 10.1016/j.cmpb.2023.107575; Lo CM, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10081494; Lo Chung-Ming, 2021, Ultrasound Med Biol, V47, P2266, DOI 10.1016/j.ultrasmedbio.2021.03.038; Lou JJ, 2022, COMPUT METH PROG BIO, V225, DOI 10.1016/j.cmpb.2022.107095; Ludford K, 2023, J CLIN ONCOL, V41, P2181, DOI 10.1200/JCO.22.01351; Mei WJ, 2022, FRONT IMMUNOL, V13, DOI 10.3389/fimmu.2022.1019582; N. C. C. Network, Colon Cancer; Nosrati V, 2022, COMPUT BIOL MED, V148, DOI 10.1016/j.compbiomed.2022.105820; Park JH, 2022, INT J MOL SCI, V23, DOI 10.3390/ijms23052462; Pei Q, 2022, EUR RADIOL, V32, P714, DOI 10.1007/s00330-021-08167-3; Peng TY, 2019, LECT NOTES COMPUT SC, V11764, P676, DOI 10.1007/978-3-030-32239-7_75; Qiu WJ, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.925079; RodriguezBigas MA, 1997, J NATL CANCER I, V89, P1758, DOI 10.1093/jnci/89.23.1758; Smith JR, 2001, J AM SOC INF SCI TEC, V52, P969, DOI 10.1002/asi.1162; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Trojan J, 2021, ONCOLOGIST, V26, pE2110, DOI 10.1002/onco.13955; Vilar E, 2010, NAT REV CLIN ONCOL, V7, P153, DOI 10.1038/nrclinonc.2009.237; Wu XM, 2020, ACAD RADIOL, V27, pE254, DOI 10.1016/j.acra.2019.12.007; Xia JF, 2022, COMPUT BIOL MED, V143, DOI 10.1016/j.compbiomed.2021.105206; Yamashita R, 2021, LANCET ONCOL, V22, P132, DOI 10.1016/S1470-2045(20)30535-0; Yang L, 2018, EUR RADIOL, V28, P2058, DOI 10.1007/s00330-017-5146-8; Ye MH, 2023, FRONT ONCOL, V13, DOI 10.3389/fonc.2023.1178772; Ying ML, 2022, BMC CANCER, V22, DOI 10.1186/s12885-022-09584-3	53	0	0	2	2	PUBLIC LIBRARY SCIENCE	SAN FRANCISCO	1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA	1932-6203			PLOS ONE	PLoS One	JAN 25	2024	19	1							e0292277	10.1371/journal.pone.0292277	http://dx.doi.org/10.1371/journal.pone.0292277			16	Multidisciplinary Sciences	Science Citation Index Expanded (SCI-EXPANDED)	Science & Technology - Other Topics	HH0B2	38271352	gold, Green Published			2024-09-18	WOS:001158471300049
J	Jiang, XY; Wang, SH; Zhang, YD				Jiang, Xiaoyan; Wang, Shuihua; Zhang, Yudong			Vision transformer promotes cancer diagnosis: A comprehensive review	EXPERT SYSTEMS WITH APPLICATIONS			English	Review						Vision transformer; Cancer diagnosis; Prostate cancer; Breast cancer; Colorectal cancer; Lung cancer	CERVICAL-CANCER; NEURAL-NETWORKS; IMAGE; CNN; ATTENTION; VIT; GAN	Background: The approaches based on vision transformers (ViTs) are advancing the field of medical artificial intelligence (AI) and cancer diagnosis. Recently, many researchers have developed artificial intelligence methods for cancer diagnosis based on ViTs. In this paper, 98 pertinent articles since 2020 were carefully chosen from digital databases, including Google scholar, Elsevier, and Springer Link, to review the research progress of artificial intelligence methods for cancer imaging based on ViT. Method : The basic structure of ViT is introduced, and corresponding modules such as patch embedding, positional embedding, transformer encoder, multi-head self -attention (MSA), layer normalization (LN), and residual connections, multilayer perceptron (MLP) are elaborated; a comprehensive review of improved ViT models in the medical field is presented. The application of ViT technology in cancer analysis based on medical images was reviewed. Results : ViT has achieved great success in cancer diagnosis based on medical images, showing its advantages in image classification, image reconstruction, image detection, image segmentation, image registration, image fusion, and other tasks. In these task studies, the most common task is cancer image classification and segmentation. There is still a lot of room for improvement in the aspects of multi-task learning, multi-modal learning, model generality, generalization ability, and explainability, and it also faces the mutual restriction of model scale and performance. Conclusion : The ViT training model for cancer diagnosis can potentially improve. The ViT model of self-supervised learning and semi-supervised learning mechanism is promising research. The lightweight attention module design, ViTs based on mobile networks, and the development of 3DViT will promote cancer diagnosis based on medical images to be more accurate and efficient.	[Jiang, Xiaoyan] Nanjing Normal Univ Special Educ, Sch Math & Informat Sci, Nanjing 210038, Peoples R China; [Wang, Shuihua] Xian Jiaotong Liverpool Univ, Dept Biol Sci, Suzhou 215123, Jiangsu, Peoples R China; [Zhang, Yudong] Univ Leicester, Sch Comp & Math Sci, Leicester LE1 7RH, England; [Zhang, Yudong] King Abdulaziz Univ, Fac Comp & Informat Technol, Dept Informat Technol, Jeddah 21589, Saudi Arabia; [Zhang, Yudong] Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454000, Henan, Peoples R China	Nanjing Normal University of Special Education; Xi'an Jiaotong-Liverpool University; University of Leicester; King Abdulaziz University; Henan Polytechnic University	Zhang, YD (corresponding author), Univ Leicester, Sch Comp & Math Sci, Leicester LE1 7RH, England.; Jiang, XY (corresponding author), Nanjing Normal Univ Special Educ, Nanjing 210038, Peoples R China.	jxy@njts.edu.cn; shuihuawang@ieee.org; yudongzhang@ieee.org	Zhang, Yudong/IAO-5053-2023		MRC [MC_PC_17171]; Royal Society [RP202G0230]; BHF [AA/18/3/34220]; Hope Foundation for Cancer Research [RM60G0680]; GCRF [P202PF11]; Sino-UK Industrial Fund [RP202G0289]; Data Science Enhancement Fund [P202RE237]; Fight for Sight [24NN201]; Sino-UK Education Fund [OP202006]; Major project of philosophy and social science research in colleges and universities in Jiangsu Province, China [RM32G0178B8]; BBSRC; LIAS [2023SJZD125];  [P202ED10];  [P202RE969]	MRC(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Royal Society(Royal Society); BHF(British Heart Foundation); Hope Foundation for Cancer Research; GCRF(UK Research & Innovation (UKRI)); Sino-UK Industrial Fund; Data Science Enhancement Fund; Fight for Sight; Sino-UK Education Fund; Major project of philosophy and social science research in colleges and universities in Jiangsu Province, China; BBSRC(UK Research & Innovation (UKRI)Biotechnology and Biological Sciences Research Council (BBSRC)); LIAS; ; 	This paper is partially supported by MRC (MC_PC_17171) ; Royal Society (RP202G0230) ; BHF (AA/18/3/34220) ; Hope Foundation for Cancer Research (RM60G0680) ; GCRF (P202PF11) ; Sino-UK Industrial Fund (RP202G0289) ; LIAS (P202ED10, P202RE969) ; Data Science Enhancement Fund (P202RE237) ; Fight for Sight (24NN201) ; Sino-UK Education Fund (OP202006) ; BBSRC (RM32G0178B8) ; Major project of philosophy and social science research in colleges and universities in Jiangsu Province, China (2023SJZD125) .	Adeyinka AA, 2018, LECT NOTES ARTIF INT, V11308, P321, DOI 10.1007/978-3-030-05918-7_29; Ahishakiye E, 2021, INTEL MED, V1, P118, DOI 10.1016/j.imed.2021.03.003; Aitazaz T, 2023, NEURAL COMPUT APPL, V35, P7963, DOI 10.1007/s00521-022-07516-7; Al-hammuri K, 2023, VIS COMPUT IND BIOME, V6, DOI 10.1186/s42492-023-00140-9; Al-masni MA, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-89686-3; Aladhadh S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22114008; Alduais Y, 2023, MEDICINE, V102, DOI 10.1097/MD.0000000000032899; Ali H, 2023, BMC MED IMAGING, V23, DOI 10.1186/s12880-023-01098-z; Ali ML, 2023, IEEE ACCESS, V11, P115740, DOI 10.1109/ACCESS.2023.3324383; ALPERT NM, 1990, J NUCL MED, V31, P1717; Alshammari H, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/3998193; Arango-Argoty G., 2023, medRxiv; Ardila D, 2019, NAT MED, V25, P954, DOI 10.1038/s41591-019-0447-x; Arkin E, 2023, MULTIMED TOOLS APPL, V82, P21353, DOI 10.1007/s11042-022-13801-3; Arshed MA, 2023, INFORMATION, V14, DOI 10.3390/info14070415; Asiri AA, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13122094; Atabansi CC, 2023, BIOMED ENG ONLINE, V22, DOI 10.1186/s12938-023-01157-0; Ayalew Yodit Abebe, 2021, BMC Biomed Eng, V3, P4, DOI 10.1186/s42490-021-00050-y; Ayana G, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13020178; Ayana G, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12112654; Azad R., 2022, arXiv; Azad R, 2023, Arxiv, DOI arXiv:2301.03505; Bandyopadhyay O, 2019, J DIGIT IMAGING, V32, P300, DOI 10.1007/s10278-018-0145-0; Baydoun A, 2021, IEEE ACCESS, V9, P17208, DOI [10.1109/ACCESS.2021.3049781, 10.1109/access.2021.3049781]; Borah N, 2022, 2022 IEEE CALCUTTA CONFERENCE, CALCON, P238, DOI 10.1109/CALCON56258.2022.10060315; Bradley SH, 2019, BRIT J GEN PRACT, V69, pE827, DOI 10.3399/bjgp19X706853; Caballo M, 2020, COMPUT BIOL MED, V118, DOI 10.1016/j.compbiomed.2020.103629; Cai YM, 2023, BMC MED INFORM DECIS, V23, DOI 10.1186/s12911-023-02129-z; Cao B., 2023, IEEE Transactions on Multimedia., P1; Cao KY, 2023, J X-RAY SCI TECHNOL, V31, P731, DOI 10.3233/XST-230014; Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951; Chae JW, 2023, IEEE ACCESS, V11, P29391, DOI 10.1109/ACCESS.2023.3260983; Chang ZH, 2023, IEEE T IMAGE PROCESS, V32, P2077, DOI 10.1109/TIP.2023.3263113; Chen J, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105878; Chen JY, 2022, MED IMAGE ANAL, V82, DOI 10.1016/j.media.2022.102615; Chen JY, 2021, Arxiv, DOI [arXiv:2104.06468, DOI 10.48550/ARXIV.2104.06468]; Chen R. J., 2022, arXiv, DOI DOI 10.48550/ARXIV.2203.00585; Chen SW, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12112442; Chen WM, 2023, J BONE ONCOL, V43, DOI 10.1016/j.jbo.2023.100508; Chen Y, 2022, NEUROCOMPUTING, V514, P328, DOI 10.1016/j.neucom.2022.09.138; Cheng A, 2019, PROC SPIE, V10955, DOI 10.1117/12.2512533; Chhikara BS, 2023, CHEM BIOL LETT, V10; Cho KYJ, 2023, MED IMAGE ANAL, V89, DOI 10.1016/j.media.2023.102894; Connal S, 2023, J TRANSL MED, V21, DOI 10.1186/s12967-023-03960-8; d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830; Dai Y, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11081384; Dalmaz O, 2022, IEEE T MED IMAGING, V41, P2598, DOI 10.1109/TMI.2022.3167808; Deo SVS, 2022, ANN SURG ONCOL, V29, P6497, DOI 10.1245/s10434-022-12151-6; Dif N, 2022, APPL INTELL, V52, P358, DOI 10.1007/s10489-021-02425-z; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Du J, 2016, KNOWL-BASED SYST, V113, P4, DOI 10.1016/j.knosys.2016.09.008; Du SY, 2024, Arxiv, DOI arXiv:2307.02100; Duan H., 2022, 2022 7 INT C AUT CON; Emmett L, 2021, EUR UROL, V80, P682, DOI 10.1016/j.eururo.2021.08.002; Fan CY, 2023, IEEE ACCESS, V11, P129763, DOI 10.1109/ACCESS.2023.3302522; Fan ZZ, 2023, COMPUT BIOL MED, V162, DOI 10.1016/j.compbiomed.2023.107070; Feng H, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13063489; Flügge T, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-29204-9; Gani H., 2022, arXiv; Gao L, 2022, LECT NOTES COMPUT SC, V13683, P744, DOI 10.1007/978-3-031-20050-2_43; Gao L, 2024, PATTERN RECOGN, V146, DOI 10.1016/j.patcog.2023.109964; Gao YH, 2022, Arxiv, DOI [arXiv:2203.00131, 10.48550/arXiv.2203.00131]; Gassenmaier S, 2021, CANCERS, V13, DOI 10.3390/cancers13143593; Giavarina D., 2023, Biochimica Clinica., V1; Go J., 2004, STRUCTURAL SYNTACTIC; Gokhale M, 2023, COMPUT BIOL MED, V155, DOI 10.1016/j.compbiomed.2023.106643; Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204; Gray S., 2017, arXiv, V3, P2; Gruetzemacher R, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505245; Gulzar Y, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12125990; Guo P, 2022, CANCERS, V14, DOI 10.3390/cancers14102401; Habashi SA, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13101749; Han XJ, 2023, COMPUT GRAPH-UK, V116, P262, DOI 10.1016/j.cag.2023.08.030; Hatamizadeh A., 2022, arXiv, DOI [10.48550/arXiv.2204.00631, DOI 10.48550/ARXIV.2204.00631]; Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181; He J, 2020, IEEE T GEOSCI REMOTE, V58, P165, DOI 10.1109/TGRS.2019.2934760; He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553; He KL, 2023, INTEL MED, V3, P59, DOI 10.1016/j.imed.2022.07.002; He Z, 2022, INFORM SCIENCES, V608, P1093, DOI 10.1016/j.ins.2022.06.091; Heller N, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101821; Henry EU, 2022, Arxiv, DOI arXiv:2211.10043; Hossain MS, 2022, J KING SAUD UNIV-COM, V34, P86, DOI 10.1016/j.jksuci.2019.10.014; Hossain S, 2024, IEEE J BIOMED HEALTH, V28, P1261, DOI 10.1109/JBHI.2023.3266614; Hu MZ, 2023, Arxiv, DOI [arXiv:2304.13973, DOI 10.48550/ARXIV.2304.13973, 10.48550/arXiv.2304.13973]; Hu WM, 2023, COMPUT BIOL MED, V161, DOI 10.1016/j.compbiomed.2023.107034; Huang K, 2022, PATTERN RECOGN LETT, V160, P122, DOI 10.1016/j.patrec.2022.06.006; Huang P, 2023, IEEE T MED IMAGING, V42, P15, DOI 10.1109/TMI.2022.3202248; Huang WL, 2024, PATTERN RECOGN, V145, DOI 10.1016/j.patcog.2023.109897; Hunter B, 2022, CANCERS, V14, DOI 10.3390/cancers14061524; Ibrahem H, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103849; Ikromjanov K., 2022, 2022 INT C ART INT I; Illimoottil M, 2023, CANCERS, V15, DOI 10.3390/cancers15133267; Iqbal A, 2023, KNOWL-BASED SYST, V267, DOI 10.1016/j.knosys.2023.110393; Jiang XY, 2023, CANCERS, V15, DOI 10.3390/cancers15143608; Jiang Y, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12060797; Jiang ZC, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/7529893; Kanayama T, 2019, LECT NOTES COMPUT SC, V11768, P530, DOI 10.1007/978-3-030-32254-0_59; Kaur Gagandeep, 2022, Neurosci Inform, V2, P100035, DOI 10.1016/j.neuri.2021.100035; Keutayeva A, 2023, IEEE ACCESS, V11, P107562, DOI 10.1109/ACCESS.2023.3320561; Khan A, 2023, ARTIF INTELL REV, V56, pS2917, DOI 10.1007/s10462-023-10595-0; Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244; Kim B, 2023, ABDOM RADIOL, V48, P201, DOI 10.1007/s00261-022-03701-3; Kingma DP, 2015, ADV NEUR IN, V28; Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643; Korkmaz Y, 2022, IEEE T MED IMAGING, V41, P1747, DOI 10.1109/TMI.2022.3147426; Krahenbuhl P., 2015, arXiv; Krishna GS, 2023, Arxiv, DOI [arXiv:2302.01104, 10.48550/arXiv.2302.01104, DOI 10.48550/ARXIV.2302.01104]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee JH, 2023, J ELECTR ENG TECHNOL, V18, P3135, DOI 10.1007/s42835-023-01542-8; Lee K, 2024, Arxiv, DOI arXiv:2107.04589; Lee YM, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13193079; Li JT, 2022, BIOMED OPT EXPRESS, V13, P6003, DOI 10.1364/BOE.467683; Li J, 2023, MED IMAGE ANAL, V85, DOI 10.1016/j.media.2023.102762; Li M, 2022, INT C PATT RECOG, P4406, DOI 10.1109/ICPR56361.2022.9956431; Li T, 2022, IEEE SIGNAL PROC LET, V29, P827, DOI 10.1109/LSP.2022.3157517; Li X., 2023, INT J NETW DYN INTEL, V2, P93, DOI [DOI 10.53941/IJNDI0201006, DOI 10.53941/IJNDI02010064A]; Li Y, 2023, P IEEE CVF INT C COM; Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476; Li YH, 2023, IEEE T MED IMAGING, V42, P3907, DOI 10.1109/TMI.2023.3317132; Li YH, 2023, IEEE T MED IMAGING, V42, P3395, DOI 10.1109/TMI.2023.3288001; Li Z., 2023, IEEE Transactions on Medical Imaging., V1; Li Z, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104785; Li ZX, 2023, ISCIENCE, V26, DOI 10.1016/j.isci.2022.105872; Liang JL, 2020, IEEE J-STARS, V13, P4325, DOI 10.1109/JSTARS.2020.3011333; Lin A., 2022, Medical Image Computing and Computer Assisted Intervention - MICCAI 2022; Liu Q, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104856; Liu WL, 2022, PATTERN RECOGN, V130, DOI [10.1016/j.patcog.2020.108829, 10.1016/j.patcog.2022.108829]; Liu Y, 2024, IEEE T NEUR NET LEAR, V35, P7478, DOI 10.1109/TNNLS.2022.3227717; Liu YJ, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.922761; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Liu ZJ, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-31339-8; Lo CM, 2023, COMPUT MED IMAG GRAP, V107, DOI 10.1016/j.compmedimag.2023.102242; Lungu-Stan VC, 2023, LECT NOTES COMPUT SC, V14254, P268, DOI 10.1007/978-3-031-44207-0_23; [马岽奡 Ma Dongao], 2021, [中国图象图形学报, Journal of Image and Graphics], V26, P487; Ma YJ, 2024, PATTERN RECOGN, V145, DOI 10.1016/j.patcog.2023.109905; Malaviya N., 2023, INT C ART INT SMART; Mali MT, 2022, 2022 INN INT SYST AP, P1; Manzari ON, 2023, COMPUT BIOL MED, V157, DOI 10.1016/j.compbiomed.2023.106791; Matsoukas C., 2021, arXiv; Maurício J, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13095521; Mehta S, 2022, Arxiv, DOI arXiv:2110.02178; Mkindu H, 2023, SIGNAL IMAGE VIDEO P, V17, P2473, DOI 10.1007/s11760-022-02464-0; Mogan JN, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23083809; Mojtahedi R, 2022, LECT NOTES COMPUT SC, V13594, P110, DOI 10.1007/978-3-031-18814-5_11; Mok TCW, 2022, PROC CVPR IEEE, P20803, DOI 10.1109/CVPR52688.2022.02017; Munir K, 2019, CANCERS, V11, DOI 10.3390/cancers11091235; Munir M, 2023, P IEEE CVF C COMP VI; Naseer M, 2021, ADV NEUR IN, V34; Nejad RR, 2023, 2023 5 INT C BIOENG, P1; Niu C, 2022, PHYS MED BIOL, V67, DOI 10.1088/1361-6560/ac92ba; Noda Y, 2022, EUR RADIOL, V32, P384, DOI 10.1007/s00330-021-08121-3; Ordun C, 2023, Arxiv, DOI arXiv:2308.12271; Pacal I, 2023, NEURAL COMPUT APPL, V35, P18813, DOI 10.1007/s00521-023-08757-w; Padinharayil H, 2023, GENES DIS, V10, P960, DOI 10.1016/j.gendis.2022.07.023; Pareek PK, 2022, J HEALTHC ENG, V2022, DOI 10.1155/2022/5171016; Parvaiz A, 2023, ENG APPL ARTIF INTEL, V122, DOI 10.1016/j.engappai.2023.106126; Pathan S, 2018, BIOMED SIGNAL PROCES, V39, P237, DOI 10.1016/j.bspc.2017.07.010; Patrício C, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3625287; Poudel S, 2021, APPL SOFT COMPUT, V109, DOI 10.1016/j.asoc.2021.107445; Prabhakar C, 2023, Arxiv, DOI arXiv:2301.07382; Preshlock S, 2016, CHEM REV, V116, P719, DOI 10.1021/acs.chemrev.5b00493; Qin W., 2022, P AS C COMP VIS; Qin ZW, 2020, COMPUT METH PROG BIO, V195, DOI 10.1016/j.cmpb.2020.105568; Raghu M, 2021, ADV NEUR IN, V34; Ramachandran P, 2019, ADV NEUR IN, V32; Ramana K, 2023, IEEE T INTELL TRANSP, V24, P3922, DOI 10.1109/TITS.2022.3233801; Rajalakshmi NR, 2021, INT J IMAG SYST TECH, V31, P59, DOI 10.1002/ima.22516; Rodríguez AO, 2004, REV MEX FIS, V50, P272; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rotman G, 2022, T ASSOC COMPUT LING, V10, P1209, DOI 10.1162/tacl_a_00515; Roy S, 2023, Arxiv, DOI [arXiv:2304.05396, DOI 10.48550/ARXIV.2304.05396]; Saeed N, 2023, BIOENGINEERING-BASEL, V10, DOI 10.3390/bioengineering10070879; Sagar A, 2021, LECT NOTES COMPUT SC, V12969, P34, DOI 10.1007/978-3-030-90874-4_4; Saha M, 2021, IEEE ACCESS, V9, P79829, DOI [10.1109/access.2021.3084597, 10.1109/ACCESS.2021.3084597]; Salama WM, 2021, ALEX ENG J, V60, P4701, DOI 10.1016/j.aej.2021.03.048; Sargazi S, 2022, NANOMATERIALS-BASEL, V12, DOI 10.3390/nano12071102; Sera T., 2021, Transparency in Biology, P167; Shah S. M. A. H., 2023, IEEE Access; Shamshad F, 2023, MED IMAGE ANAL, V88, DOI 10.1016/j.media.2023.102802; Shen YF, 2022, AAAI CONF ARTIF INTE, P2207; Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0; Song PF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2022.3232143; Springenberg M, 2023, MED IMAGE ANAL, V87, DOI 10.1016/j.media.2023.102809; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sui D, 2021, BIOMED RES INT, V2021, DOI 10.1155/2021/6207964; Sun JW, 2023, COMPUT BIOL MED, V152, DOI 10.1016/j.compbiomed.2022.106444; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302; Takase S, 2023, Arxiv, DOI arXiv:2206.00330; Tan LX, 2023, MED BIOL ENG COMPUT, V61, P1565, DOI 10.1007/s11517-023-02799-x; Tang ZH, 2023, COMPUT MED IMAG GRAP, V110, DOI 10.1016/j.compmedimag.2023.102302; [田永林 Tian Yonglin], 2022, [自动化学报, Acta Automatica Sinica], V48, P957; Touvron H, 2021, PR MACH LEARN RES, V139, P7358; Tsochatzidis L, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105913; Valanarasu JMJ, 2021, LECT NOTES COMPUT SC, V12901, P36, DOI 10.1007/978-3-030-87193-2_4; Vaswani A, 2017, ADV NEUR IN, V30; Wang J, 2023, COMPUT BIOL MED, V167, DOI 10.1016/j.compbiomed.2023.107622; Wang JJ, 2023, COMPUT BIOL MED, V165, DOI 10.1016/j.compbiomed.2023.107336; Wang L, 2023, ICASSP 2023 2023 IEE, P1; Wang P., 2023, Meta-Radiology., V1; Wang PX, 2022, INT C PATT RECOG, P4623, DOI 10.1109/ICPR56361.2022.9956705; Wang Q, 2019, Arxiv, DOI [arXiv:1906.01787, DOI 10.48550/ARXIV.1906.01787]; Wang SH, 2020, MULTIMED TOOLS APPL, V79, P15135, DOI 10.1007/s11042-018-6798-3; Wang SH, 2020, MULTIMED TOOLS APPL, V79, P15117, DOI 10.1007/s11042-018-6661-6; Wang SH, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-0932-7; Wang W, 2022, FRONT PHARMACOL, V13, DOI 10.3389/fphar.2022.929755; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11; Wang XY, 2022, MED IMAGE ANAL, V81, DOI 10.1016/j.media.2022.102559; Wei C, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23073420; Wells PNT, 1999, REP PROG PHYS, V62, P671, DOI 10.1088/0034-4885/62/5/201; Wessels F, 2023, WORLD J UROL, V41, P2233, DOI 10.1007/s00345-023-04489-7; Woo S, 2018, EUR RADIOL, V28, P530, DOI 10.1007/s00330-017-4958-x; Wu BC, 2020, Arxiv, DOI [arXiv:2006.03677, 10.48550/arXiv.2006.03677]; Wu JD, 2023, Arxiv, DOI [arXiv:2304.12620, 10.48550/arXiv.2304.12620]; Wu YX, 2023, NEURAL COMPUT APPL, V35, P1931, DOI [10.1007/s00521-022-07859-1, 10.1007/s11071-022-07935-0]; Xiao HG, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104791; Xie EZ, 2021, ADV NEUR IN, V34; Xin C, 2022, COMPUT BIOL MED, V149, DOI 10.1016/j.compbiomed.2022.105939; Xu G, 2021, arXiv; Xu HM, 2024, IEEE REV BIOMED ENG, V17, P63, DOI 10.1109/RBME.2023.3297604; Xu T., 2022, Ovarian Cancer Whole-Slide Histopathology Images.; Xu T., 2023, P 2023 4 INT C COMP; Xu X, 2023, Arxiv, DOI arXiv:2304.01053; Xu Y., 2023, IEEE Transactions on Circuits and Systems for Video Technology; Xu Y, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.1009581; Yang G, 2023, NEURAL PROCESS LETT, V55, P9335, DOI 10.1007/s11063-023-11204-5; Yang J., 2021, arXiv, DOI 10.48550/arXiv.2107.00641; Yang SR, 2023, Arxiv, DOI arXiv:2204.08610; Yang T, 2023, Arxiv, DOI arXiv:2210.04020; Yeh CF, 2019, Arxiv, DOI arXiv:1910.12977; Yu BT, 2018, I S BIOMED IMAGING, P626, DOI 10.1109/ISBI.2018.8363653; Yu B, 2023, KNOWL-BASED SYST, V275, DOI 10.1016/j.knosys.2023.110721; Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060; Zeid M. A. E., 2021, 2021 10 INT C INT CO; Zhai XH, 2022, PROC CVPR IEEE, P12094, DOI 10.1109/CVPR52688.2022.01179; Zhang C, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3292418; Zhang HQ, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.925903; Zhang JH, 2023, ARTIF INTELL REV, V56, P1013, DOI 10.1007/s10462-022-10192-7; Zhang QM, 2023, INT J COMPUT VISION, V131, P1141, DOI 10.1007/s11263-022-01739-w; Zhang R, 2020, IEEE T INF FOREN SEC, V15, P1138, DOI 10.1109/TIFS.2019.2936913; Zhang TG, 2023, PATTERN RECOGN LETT, V170, P106, DOI 10.1016/j.patrec.2023.04.013; Zhang TY, 2022, Arxiv, DOI [arXiv:2208.06833, DOI 10.1016/J.CMPB.2023.107969]; Zhang XQ, 2023, BIOMED SIGNAL PROCES, V82, DOI 10.1016/j.bspc.2022.104532; Zhang YD, 2023, J IMAGING, V9, DOI 10.3390/jimaging9070147; Zhang Y, 2022, TECHNOL CANCER RES T, V21, DOI 10.1177/15330338221085358; Zhao B, 2023, COMPUT MED IMAG GRAP, V103, DOI 10.1016/j.compmedimag.2022.102150; Zhao C, 2022, MULTIMED TOOLS APPL, V81, P24265, DOI 10.1007/s11042-022-12670-0; Zhao CH, 2022, IEEE T IMAGE PROCESS, V31, P3838, DOI 10.1109/TIP.2022.3176537; Zhao Y, 2023, PRO BIOMED OPT IMAG, V12464, DOI 10.1117/12.2653352; Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865; Zheng Yi, 2022, IEEE Trans Med Imaging, V41, P3003, DOI 10.1109/TMI.2022.3176598; Zhou XL, 2023, INTERDISCIP SCI, V15, P15, DOI 10.1007/s12539-022-00532-0; Zidan U, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119452; Zou Q, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12041055; Zou X, 2023, MATH BIOSCI ENG, V20, P15244, DOI 10.3934/mbe.2023682	255	0	0	5	5	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0957-4174	1873-6793		EXPERT SYST APPL	Expert Syst. Appl.	OCT 15	2024	252		A							10.1016/j.eswa.2024.124113	http://dx.doi.org/10.1016/j.eswa.2024.124113		MAY 2024	20	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Operations Research & Management Science	ZD3H6					2024-09-18	WOS:001273311300002
J	Mozaffari, J; Amirkhani, A; Shokouhi, SB				Mozaffari, Javad; Amirkhani, Abdollah; Shokouhi, Shahriar B.			ColonGen: an efficient polyp segmentation system for generalization improvement using a new comprehensive dataset	PHYSICAL AND ENGINEERING SCIENCES IN MEDICINE			English	Article						Colorectal cancer; Convolutional neural networks; Transformers; Dataset; Polyp segmentation	DEEP	Colorectal cancer (CRC) is one of the most common causes of cancer-related deaths. While polyp detection is important for diagnosing CRC, high miss rates for polyps have been reported during colonoscopy. Most deep learning methods extract features from images using convolutional neural networks (CNNs). In recent years, vision transformer (ViT) models have been employed for image processing and have been successful in image segmentation. It is possible to improve image processing by using transformer models that can extract spatial location information, and CNNs that are capable of aggregating local information. Despite this, recent research shows limited effectiveness in increasing data diversity and generalization accuracy. This paper investigates the generalization proficiency of polyp image segmentation based on transformer architecture and proposes a novel approach using two different ViT architectures. This allows the model to learn representations from different perspectives, which can then be combined to create a richer feature representation. Additionally, a more universal and comprehensive dataset has been derived from the datasets presented in the related research, which can be used for improving generalizations. We first evaluated the generalization of our proposed model using three distinct training-testing scenarios. Our experimental results demonstrate that our ColonGen-V1 outperforms other state-of-the-art methods in all scenarios. As a next step, we used the comprehensive dataset for improving the performance of the model against in- and out-of-domain data. The results show that our ColonGen-V2 outperforms state-of-the-art studies by 5.1%, 1.3%, and 1.1% in ETIS-Larib, Kvasir-Seg, and CVC-ColonDB datasets, respectively. The inclusive dataset and the model introduced in this paper are available to the public through this link: https://github.com/javadmozaffari/Polyp_segmentation.	[Mozaffari, Javad; Shokouhi, Shahriar B.] Iran Univ Sci & Technol, Sch Elect Engn, Tehran 1684613114, Iran; [Amirkhani, Abdollah] Iran Univ Sci & Technol, Sch Automot Engn, Tehran 1684613114, Iran	Iran University Science & Technology; Iran University Science & Technology	Amirkhani, A (corresponding author), Iran Univ Sci & Technol, Sch Automot Engn, Tehran 1684613114, Iran.	javad_mozaffari@elec.iust.ac.ir; amirkhani@iust.ac.ir; bshokouhi@iust.ac.ir	Amirkhani, Abdollah/C-6743-2019					Ahn SB, 2012, GUT LIVER, V6, P64, DOI 10.5009/gnl.2012.6.1.64; Ali S, 2023, SCI DATA, V10, DOI 10.1038/s41597-023-01981-y; Ashish V., 2017, Advances in neural information processing systems, V30, P5998, DOI 10.48550/arXiv.1706.03762; Chenarlogh VA, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-10429-z; Ayatollahi F, 2021, MED PHYS, V48, P5897, DOI 10.1002/mp.15156; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Barshooi AH, 2022, BIOMED SIGNAL PROCES, V72, DOI 10.1016/j.bspc.2021.103326; Bazi Y, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030516; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361; Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5; Chen YZ, 2020, IEEE IMAGE PROC, P523, DOI 10.1109/ICIP40778.2020.9190729; Cortes C, 2012, J MACH LEARN RES, V13, P795; Dai Y, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11081384; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Ding M, 2022, EUR C COMP VIS; Dong B., 2021, arXiv; Dosovitskiy A., 2021, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.2010.11929; Fan D, 2017, IEEE ICC; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Fang YX, 2021, ADV NEUR IN; Guo CJ, 2021, IEEE ICC, DOI 10.1109/ICC42927.2021.9500511; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hong DF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3130716; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Jha D., 2019, INT C MULT MOD; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Ji GP, 2022, MACH INTELL RES, V19, P531, DOI 10.1007/s11633-022-1371-y; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Kitasaka T., 2022, COMP M BIO BIO E-IV, V11, P1187; Li QL, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI); Li Y., 2022, ARXIV; Liu GQ, 2023, EXPERT SYST APPL, V214, DOI 10.1016/j.eswa.2022.118975; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Megía PJ, 2021, CATAL TODAY, V367, P145, DOI 10.1016/j.cattod.2020.04.069; Meng YD, 2022, IEEE T MED IMAGING, V41, P690, DOI 10.1109/TMI.2021.3123567; Misawa M, 2021, GASTROINTEST ENDOSC, V93, P960, DOI 10.1016/j.gie.2020.07.060; Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241; Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018; Lan PN, 2021, LECT NOTES COMPUT SC, V13018, P15, DOI 10.1007/978-3-030-90436-4_2; Raghu M, 2021, ADV NEUR IN, V34; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sánchez-Peralta LF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10238501; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tjoa MP, 2003, BIOMED ENG ONLINE, V2, DOI 10.1186/1475-925X-2-9; Wang M, 2021, IEEE ENG MED BIO, P2936, DOI 10.1109/EMBC46164.2021.9630787; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Xie EZ, 2021, ADV NEUR IN, V34; Yu YG, 2022, IEEE I C VI COM I PR, DOI 10.1109/VCIP56404.2022.10008828; Yuan ZX, 2022, IEEE T CIRC SYST VID, V32, P2068, DOI 10.1109/TCSVT.2021.3082763; Zhang D., 2018, ACM Trans. Intell. Syst. Technol., V9, P1; Zhang L, 2017, COMM COM INF SC, V723, P707, DOI 10.1007/978-3-319-60964-5_62; Zhou Z, 2018, LECT NOTES COMPUTER, V11045	59	1	1	8	15	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	2662-4729	2662-4737		PHYS ENG SCI MED	Phys. Eng. Sci. Med.	MAR	2024	47	1					309	325		10.1007/s13246-023-01368-8	http://dx.doi.org/10.1007/s13246-023-01368-8		JAN 2024	17	Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Radiology, Nuclear Medicine & Medical Imaging	MA8K2	38224384				2024-09-18	WOS:001142232200001
C	Ling, TY; Wu, CY; Yu, H; Cai, T; Wang, D; Zhou, YC; Chen, M; Ding, KF		Greenspan, H; Madabhushi, A; Mousavi, P; Salcudean, S; Duncan, J; Syeda-Mahmood, T; Taylor, R		Ling, Tianyi; Wu, Chengyi; Yu, Huan; Cai, Tian; Wang, Da; Zhou, Yincong; Chen, Ming; Ding, Kefeng			Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation	MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION, MICCAI 2023, PT VII	Lecture Notes in Computer Science		English	Proceedings Paper	26th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)	OCT 08-12, 2023	Vancouver, CANADA			Colonoscopy; Polyp Segmentation; Vision Transformer		Colorectal polyps detected during colonoscopy are strongly associated with colorectal cancer, making polyp segmentation a critical clinical decision-making tool for diagnosis and treatment planning. However, accurate polyp segmentation remains a challenging task, particularly in cases involving diminutive polyps and other intestinal substances that produce a high false-positive rate. Previous polyp segmentation networks based on supervised binary masks may have lacked global semantic perception of polyps, resulting in a loss of capture and discrimination capability for polyps in complex scenarios. To address this issue, we propose a novel Gaussian-Probabilistic guided semantic fusion method that progressively fuses the probability information of polyp positions with the decoder supervised by binary masks. Our Probabilistic Modeling Ensemble Vision Transformer Network(PETNet) effectively suppresses noise in features and significantly improves expressive capabilities at both pixel and instance levels, using just simple types of convolutional decoders. Extensive experiments on five widely adopted datasets show that PETNet outperforms existing methods in identifying polyp camouflage, appearance changes, and small polyp scenes, and achieves a speed about 27FPS in edge computing devices. Codes are available at: https://github.com/Seasonsling/PETNet.	[Ling, Tianyi; Wang, Da; Ding, Kefeng] China Natl Minist Educ, Key Lab Mol Biol Med Sci, Dept Colorectal Surg & Oncol, Key Lab Canc Prevent & Intervent, Hangzhou, Zhejiang, Peoples R China; [Ling, Tianyi; Wang, Da; Ding, Kefeng] Zhejiang Univ, Sch Med, Affiliated Hosp 2, Hangzhou, Zhejiang, Peoples R China; [Ling, Tianyi; Wu, Chengyi; Yu, Huan; Zhou, Yincong; Chen, Ming] Zhejiang Univ, Dept Bioinformat, Coll Life Sci, Hangzhou, Zhejiang, Peoples R China; [Wu, Chengyi] Zhejiang Univ, Sch Med, Affiliated Hosp 1, Dept Hepatobiliary & Pancreat Surg, Hangzhou, Zhejiang, Peoples R China; [Yu, Huan] Zhejiang Univ, Sch Med, Affiliated Hosp 2, Dept Thorac Surg, Hangzhou, Zhejiang, Peoples R China; [Cai, Tian] Zhejiang Univ, Affiliated Hosp 2, Dept Hepatobiliary & Pancreat Surg, Sch Med, Hangzhou, Zhejiang, Peoples R China	Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University	Ding, KF (corresponding author), China Natl Minist Educ, Key Lab Mol Biol Med Sci, Dept Colorectal Surg & Oncol, Key Lab Canc Prevent & Intervent, Hangzhou, Zhejiang, Peoples R China.; Ding, KF (corresponding author), Zhejiang Univ, Sch Med, Affiliated Hosp 2, Hangzhou, Zhejiang, Peoples R China.; Chen, M (corresponding author), Zhejiang Univ, Dept Bioinformat, Coll Life Sci, Hangzhou, Zhejiang, Peoples R China.	mchen@zju.edu.cn; dingkefeng@zju.edu.cn	Chen, Ming/AAV-3682-2020		National Natural Sciences Foundation of China [31771477, 32070677]; Fundamental Research Funds for the Central Universities [226-2022-00009]; Key R&D Program of Zhejiang [2023C03049]	National Natural Sciences Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Key R&D Program of Zhejiang	This work was supported by the National Natural Sciences Foundation of China (Nos. 31771477, 32070677), the Fundamental Research Funds for the Central Universities (No. 226-2022-00009), and the Key R&D Program of Zhejiang (No. 2023C03049).	Ahmed AMAA, 2020, Arxiv, DOI arXiv:2012.06771; Ali HA, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101897; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Fan DP, 2020, Arxiv, DOI [arXiv:2006.11392, DOI 10.1007/978-3-030-59725-226]; Jha D, 2019, Arxiv, DOI [arXiv:1911.07067, 10.48550/ARXIV.1911.07067, DOI 10.48550/ARXIV.1911.07067]; Jha D, 2019, Arxiv, DOI [arXiv:1911.07069, DOI 10.48550/ARXIV.1911.07069, 10.48550/arxiv.1911.07069]; Ji GP, 2022, Arxiv, DOI arXiv:2205.12853; Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959; National Health Commission of the People's Republic of China, 2020, Zhonghua Wai Ke Za Zhi, V58, P561, DOI 10.3760/cma.j.cn112139-20200518-00390; Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/crv52889.2021.00032, 10.1109/CRV52889.2021.00032]; Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Tajbakhsh Nima, 2015, Inf Process Med Imaging, V24, P327, DOI 10.1007/978-3-319-19992-4_25; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2111.04734; Wang JF, 2022, LECT NOTES COMPUT SC, V13433, P110, DOI 10.1007/978-3-031-16437-8_11; Wang WH, 2023, Arxiv, DOI arXiv:2106.13797; Zhang RF, 2022, LECT NOTES COMPUT SC, V13433, P99, DOI 10.1007/978-3-031-16437-8_10; Zhang YD, 2021, Arxiv, DOI arXiv:2102.08005; Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850; Zhou ZW, 2018, Arxiv, DOI arXiv:1807.10165	22	0	0	2	4	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	0302-9743	1611-3349	978-3-031-43989-6; 978-3-031-43990-2	LECT NOTES COMPUT SC			2023	14226						572	581		10.1007/978-3-031-43990-2_54	http://dx.doi.org/10.1007/978-3-031-43990-2_54			10	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Radiology, Nuclear Medicine & Medical Imaging	BW1RL					2024-09-18	WOS:001109636000054
J	Bui, DC; Song, B; Kim, K; Kwak, JT				Bui, Doanh C.; Song, Boram; Kim, Kyungeun; Kwak, Jin Tae			DAX-Net: A dual-branch dual-task adaptive cross-weight feature fusion network for robust multi-class cancer classification in pathology images	COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE			English	Article						Cancer classification; Hybrid model; CNN; Transformer; Feature fusion; Multi-task learning	ARTIFICIAL-INTELLIGENCE	Background and Objective: Multi -class cancer classification has been extensively studied in digital and computa- tional pathology due to its importance in clinical decision -making. Numerous computational tools have been proposed for various types of cancer classification. Many of them are built based on convolutional neural networks. Recently, Transformer -style networks have shown to be effective for cancer classification. Herein, we present a hybrid design that leverages both convolutional neural networks and transformer architecture to obtain superior performance in cancer classification. Methods: We propose a dual -branch dual -task adaptive cross -weight feature fusion network, called DAX-Net, which exploits heterogeneous feature representations from the convolutional neural network and Transformer network, adaptively combines them to boost their representation power, and conducts cancer classification as categorical classification and ordinal classification. For an efficient and effective optimization of the proposed model, we introduce two loss functions that are tailored to the two classification tasks. Results: To evaluate the proposed method, we employed colorectal and prostate cancer datasets, of which each contains both in -domain and out -of -domain test sets. For colorectal cancer, the proposed method obtained an accuracy of 88.4%, a quadratic kappa score of 0.945, and an F1 score of 0.831 for the in -domain test set, and 84.4%, 0.910, and 0.768 for the out -of -domain test set. For prostate cancer, it achieved an accuracy of 71.6%, a kappa score of 0.635, and an F1 score of 0.655 for the in -domain test set, 79.2% accuracy, 0.721 kappa score, and 0.686 F1 score for the first out -of -domain test set, and 58.1% accuracy, 0.564 kappa score, and 0.493 F1 score for the second out -of -domain test set. It is worth noting that the performance of the proposed method outperformed other competitors by significant margins, in particular, with respect to the out -of -domain test sets. Conclusions: The experimental results demonstrate that the proposed method is not only accurate but also robust to varying conditions of the test sets in comparison to several, related methods. These results suggest that the proposed method can facilitate automated cancer classification in various clinical settings.	[Bui, Doanh C.; Kwak, Jin Tae] Korea Univ, Sch Elect Engn, Seoul 02841, South Korea; [Song, Boram; Kim, Kyungeun] Sungkyunkwan Univ, Kangbuk Samsung Hosp, Sch Med, Dept Pathol, Seoul 03181, South Korea	Korea University; Sungkyunkwan University (SKKU)	Kwak, JT (corresponding author), Korea Univ, Sch Elect Engn, Seoul 02841, South Korea.	jkwak@korea.ac.kr	Bui, Doanh/IUO-3703-2023; Bui Cao, Doanh/AFK-6839-2022	Bui Cao, Doanh/0000-0003-1310-5808; Kwak, Jin Tae/0000-0003-0287-4097	National Research Foundation of Korea (NRF) [NRF-2021R1A2C2014557, 2021R1A4A1031864]; Institute of Information & communication Tech-nology Planning & Evaluation (IITP) [RS-2022-00167143]; Korea Health Technology R& D Project through the Korea Health Indus-try Development Institute (KHIDI) - Ministry of Health & Welfare, Republic of Korea [HI21C1137]	National Research Foundation of Korea (NRF)(National Research Foundation of Korea); Institute of Information & communication Tech-nology Planning & Evaluation (IITP)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); Korea Health Technology R& D Project through the Korea Health Indus-try Development Institute (KHIDI) - Ministry of Health & Welfare, Republic of Korea	This work was supported by a grant of the National Research Foundation of Korea (NRF) (No. NRF-2021R1A2C2014557 and No. 2021R1A4A1031864) , Institute of Information & communication Tech-nology Planning & Evaluation (IITP) (No. RS-2022-00167143) , and the Korea Health Technology R& D Project through the Korea Health Indus-try Development Institute (KHIDI) , funded by the Ministry of Health & Welfare, Republic of Korea (No. HI21C1137) .	Abels E, 2019, J PATHOL, V249, P286, DOI 10.1002/path.5331; Aljuaid H, 2022, COMPUT METH PROG BIO, V223, DOI 10.1016/j.cmpb.2022.106951; Arvaniti E, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30535-1; Aubreville M, 2023, MED IMAGE ANAL, V84, DOI 10.1016/j.media.2022.102699; Bayramoglu N, 2016, INT C PATT RECOG, P2440, DOI 10.1109/ICPR.2016.7900002; Budak Ü, 2019, APPL SOFT COMPUT, V85, DOI 10.1016/j.asoc.2019.105765; Chan L, 2019, IEEE I CONF COMP VIS, P10661, DOI 10.1109/ICCV.2019.01076; Chen C, 2022, BMC MED INFORM DECIS, V22, DOI 10.1186/s12911-022-01919-1; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cui M, 2021, LAB INVEST, V101, P412, DOI 10.1038/s41374-020-00514-0; Ding MD, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109532; Doan TNN, 2022, LECT NOTES COMPUT SC, V13432, P171, DOI 10.1007/978-3-031-16434-7_17; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Fu BK, 2022, COMPUT METH PROG BIO, V221, DOI 10.1016/j.cmpb.2022.106924; Graham S, 2023, MED IMAGE ANAL, V83, DOI 10.1016/j.media.2022.102685; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang SG, 2020, CANCER LETT, V471, P61, DOI 10.1016/j.canlet.2019.12.007; Huo XZ, 2024, BIOMED SIGNAL PROCES, V87, DOI 10.1016/j.bspc.2023.105534; Jiang Y, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214587; Kashif MN, 2016, I S BIOMED IMAGING, P1029, DOI 10.1109/ISBI.2016.7493441; Korbar Bruno, 2017, J Pathol Inform, V8, P30, DOI 10.4103/jpi.jpi_34_17; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lee J, 2023, COMPUT METH PROG BIO, V241, DOI 10.1016/j.cmpb.2023.107749; Li H, 2022, COMPUT BIOL MED, V151, DOI 10.1016/j.compbiomed.2022.106265; Li MS, 2023, PHYS MED BIOL, V68, DOI 10.1088/1361-6560/acf556; Li X, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232127; Li Z, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104785; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Lou JJ, 2022, COMPUT METH PROG BIO, V225, DOI 10.1016/j.cmpb.2022.107095; Mayerich D.M., 2014, SPIE, V9041, P38; Mehta S, 2018, IEEE WINT CONF APPL, P663, DOI 10.1109/WACV.2018.00078; Meidan Ding, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P2028, DOI 10.1109/BIBM52615.2021.9669476; Morales S, 2021, DIGIT SIGNAL PROCESS, V119, DOI 10.1016/j.dsp.2021.103196; Raghu M, 2021, ADV NEUR IN, V34; Ren Z, 2023, IEEE Open J. Eng. Med. Biol., P1; Ren ZY, 2023, CAAI T INTELL TECHNO, V8, P549, DOI 10.1049/cit2.12216; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Shao ZC, 2021, ADV NEUR IN; Sharma H, 2017, COMPUT MED IMAG GRAP, V61, P2, DOI 10.1016/j.compmedimag.2017.06.001; Stacke K, 2021, IEEE J BIOMED HEALTH, V25, P325, DOI 10.1109/JBHI.2020.3032060; Stoean C, 2016, SMART INNOV SYST TEC, V55, P145, DOI 10.1007/978-3-319-39345-2_13; Su L, 2023, COMPUT METH PROG BIO, V232, DOI 10.1016/j.cmpb.2023.107446; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tan MX, 2019, PR MACH LEARN RES, V97; Vuong TLT, 2020, 2020 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC); Vuong TTL, 2021, MED IMAGE ANAL, V73, DOI 10.1016/j.media.2021.102206; Vaswani A, 2017, ADV NEUR IN, V30; Vuong T.T.L., 2022, Lecture Notes in Computer Science, P543; Vuong TTL, 2022, IEEE J BIOMED HEALTH, V26, P1152, DOI 10.1109/JBHI.2021.3099817; Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yan R, 2020, METHODS, V173, P52, DOI 10.1016/j.ymeth.2019.06.014; Yang H, 2020, IEEE T MED IMAGING, V39, P1306, DOI 10.1109/TMI.2019.2948026; Zhang JH, 2023, BIOMED SIGNAL PROCES, V86, DOI 10.1016/j.bspc.2023.105126; Zheng Yi, 2022, IEEE Trans Med Imaging, V41, P3003, DOI 10.1109/TMI.2022.3176598	57	1	1	7	7	ELSEVIER IRELAND LTD	CLARE	ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000, IRELAND	0169-2607	1872-7565		COMPUT METH PROG BIO	Comput. Meth. Programs Biomed.	MAY	2024	248								108112	10.1016/j.cmpb.2024.108112	http://dx.doi.org/10.1016/j.cmpb.2024.108112		MAR 2024	14	Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Biomedical; Medical Informatics	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Medical Informatics	OI7J8	38479146	hybrid			2024-09-18	WOS:001206704800001
