PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Kim, HR; Kim, KJ; Lim, KT; Choi, DH		Park, T; Cho, YR; Hu, X; Yoo, I; Woo, HG; Wang, J; Facelli, J; Nam, S; Kang, M		Kim, Hwa-Rang; Kim, Kwang-Ju; Lim, Kil-Taek; Choi, Doo-Hyun			Colorectal Cancer Image Segmentation and Classification with Deep Neural Network Based on Information Theory	2020 IEEE INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND BIOMEDICINE	IEEE International Conference on Bioinformatics and Biomedicine-BIBM		English	Proceedings Paper	IEEE International Conference on Bioinformatics and Biomedicine (IEEE BIBM)	DEC 16-19, 2020	ELECTR NETWORK	IEEE, Seoul Natl Univ, Bioinformat Inst, Korea Genome Open HRD, Korea Genome Organization, Bio Synergy Res Ctr, Korean Federation of Science and Technology Societies, Seoul Natl Univ, Dept Stat, IEEE Tech Comm Computat Life Sci		CRC; MSI; DeepLabv3+; OctaveResNet; Information theory		Colorectal cancer (CRC) is the development of cancer from the colon or rectum. Microsatellite instability (MSI) status can be considered as an indicator to predict the prognosis of CRC. We employ MSI prediction of CRC image by designing a neural network model of which base network is DeepLabv3+ with OctaveResNet. Additionally, we add a channel sort module to divide a feature map along with channel intensity. Then each feature map goes through distinct convolution paths. Each convolution path is designed based on information theory: the most important feature goes through the lightest convolution path, vice versa. By dividing feature map and applying different amount of convolutional operation, the model can extract features efficiently. In the experiment, total model weight is reduced but accuracy increases.	[Kim, Hwa-Rang; Choi, Doo-Hyun] Kyungpook Natl Univ, Grad Sch Elect & Elect Engn, Sch Elect Engn, Daegu, South Korea; [Kim, Kwang-Ju; Lim, Kil-Taek] Elect & Telecommun Res Inst, Daegu, South Korea	Kyungpook National University (KNU); Electronics & Telecommunications Research Institute - Korea (ETRI)	Kim, HR (corresponding author), Kyungpook Natl Univ, Grad Sch Elect & Elect Engn, Sch Elect Engn, Daegu, South Korea.	khr1393@knu.ac.kr; kwangju@etri.re.kr; ktl@etri.re.kr; dhc@ee.knu.ac.kr		Kim, KwangJu/0000-0001-8458-4506	Electronics and Telecommunications Research Institute (ETRI) - Korean government [20ZD1100]; Seoul National University Hospital by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI) - Ministry of Health & Welfare, Republic of Korea [HI18C0316]	Electronics and Telecommunications Research Institute (ETRI) - Korean government; Seoul National University Hospital by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI) - Ministry of Health & Welfare, Republic of Korea	This work was supported by Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government. [20ZD1100, Development of ICT Convergence Technology for Daegu-GyeongBuk Regional Industry] De-identified pathology images and annotations used in this research were prepared and provided by the Seoul National University Hospital by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HI18C0316).	Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1	5	1	1	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	2156-1125	2156-1133	978-1-7281-6215-7	IEEE INT C BIOINFORM			2020							2968	2970		10.1109/BIBM49941.2020.9313157	http://dx.doi.org/10.1109/BIBM49941.2020.9313157			3	Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology	Conference Proceedings Citation Index - Science (CPCI-S)	Biochemistry & Molecular Biology; Computer Science; Mathematical & Computational Biology	BR6BW					2024-09-18	WOS:000659487103010
C	Qiang, Q; Hong, W; Likang, P			ACM	Qiang, Qi; Hong, Wang; Likang, Peng			An Efficient Method of Histological Cell Image Detection Based on Spatial Information Convolution Neural Network	ICVIP 2019: PROCEEDINGS OF 2019 3RD INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING			English	Proceedings Paper	3rd International Conference on Video and Image Processing (ICVIP)	DEC 20-23, 2019	Shanghai, PEOPLES R CHINA			Cell nuclei detection; Convolutional neural network; Spatial information		As an important research direction in the field of medical images, histopathological cell image detection has been widely used in computer-aided diagnosis, biological research fields. With the rise of deep learning, neural network is applied to medical image analysis, which can realize the automatic detection and classification of histological cell images. In order to solve the problem that the output of the existing neural network is affected by spatial information factors in its topological domain, on the basis of the traditional convolution neural network. Combined with the spatial position information, an improved convolution neural network model for histological cell image detection is proposed. Taking the traditional convolution neural network as the carrier, the convolution neural network model based on spatial information is constructed, which makes the model has the ability to fuse spatial information and eigenvector. Histopathological cell images were preprocessed by color deconvolution. Finally, a model verification experiment based on colorectal cancer image dataset is designed. The model proposed in this paper shows better performance than the state-of-the-art methods in four different categories (more than 20000 experimental images): the experimental accuracy is 75.8%, and the recall rate is 82.3%. F1 reached 80.1%.	[Qiang, Qi; Hong, Wang; Likang, Peng] Wuhan Univ Technol, Wuhan, Hubei, Peoples R China	Wuhan University of Technology	Qiang, Q (corresponding author), Wuhan Univ Technol, Wuhan, Hubei, Peoples R China.	qq5255898@163.com; whong2002@vip.sina.com; plk940508@163.com						Al-Kofahi Y, 2010, IEEE T BIO-MED ENG, V57, P841, DOI 10.1109/TBME.2009.2035102; Ali S, 2012, IEEE T MED IMAGING, V31, P1448, DOI 10.1109/TMI.2012.2190089; Arteta C, 2012, LECT NOTES COMPUT SC, V7510, P348, DOI 10.1007/978-3-642-33415-3_43; Cosatto E, 2008, INT C PATT RECOG, P672; Khan AM, 2014, IEEE T BIO-MED ENG, V61, P1729, DOI 10.1109/TBME.2014.2303294; Kuse Manohar, 2011, J Pathol Inform, V2, pS2, DOI 10.4103/2153-3539.92028; Veta M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070221; Vink JP, 2013, J MICROSC-OXFORD, V249, P124, DOI 10.1111/jmi.12001; Xie YP, 2015, LECT NOTES COMPUT SC, V9351, P374, DOI 10.1007/978-3-319-24574-4_45; Xie YP, 2015, LECT NOTES COMPUT SC, V9351, P358, DOI 10.1007/978-3-319-24574-4_43; Xu J, 2016, IEEE T MED IMAGING, V35, P119, DOI 10.1109/TMI.2015.2458702; Yuan YY, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3004330	12	0	0	0	1	ASSOC COMPUTING MACHINERY	NEW YORK	1515 BROADWAY, NEW YORK, NY 10036-9998 USA			978-1-4503-7682-2				2019							69	73		10.1145/3376067.3376109	http://dx.doi.org/10.1145/3376067.3376109			5	Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Imaging Science & Photographic Technology	BS1ZY					2024-09-18	WOS:000697996900014
J	Wu, YT; Shih, FY; Wang, CL; Hsiao, KT; Liu, YC; Chang, FC; Yu, ED				Wu, Yi-Ta; Shih, Frank Y.; Wang, Cheng-Long; Hsiao, Kuang-Ting; Liu, You-Cheng; Chang, Fu-Chieh; Yu, En-Da			The Deep Hybrid Neural Network and an Application on Polyp Detection	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE			English	Article						Deep convolutional neural network; deep hybrid neural network; deep morphological neural network; polyp detection	COLORECTAL-CANCER; COLONOSCOPY; POLYPECTOMY	Mathematical morphology and convolution operators are two different methods to extract the characteristics and structures of images. Over the past decades, Deep Convolutional Neural Networks (DCNN) have been proven to be more powerful than traditional image-processing approaches. In this paper, we propose a novel structure called Deep Hybrid Neural Network (DHNN) by taking advantage of the convolution and morphological neural layers. Its practical application to polyp detection in medical images is illustrated. For experimental completeness, we adopt nine polyp image datasets, including publicly available data and our own collected data. For performance comparisons, we select three backbone models. Experimental results show that our DHNN achieves the best performance in comparisons in terms of computational complexity and accurate performance.	[Wu, Yi-Ta; Hsiao, Kuang-Ting; Liu, You-Cheng; Chang, Fu-Chieh] Insign Med Technol Hong Kong Ltd, Room 635,6 F,Bldg 17 W,17 Sci Pk West Ave,Hong Kon, Hong Kong, Peoples R China; [Shih, Frank Y.] New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA; [Shih, Frank Y.] Asia Univ, Dept Comp Sci & Informat Engn, Taichung 413, Taiwan; [Wang, Cheng-Long; Yu, En-Da] Naval Med Univ, Changhai Hosp, Dept Colorectal Surg, Shanghai, Peoples R China; [Wang, Cheng-Long; Yu, En-Da] Naval Med Univ, Affiliated Hosp 1, Changhai Hosp, Gastrointestinal Endoscopy Ctr, Shanghai, Peoples R China; [Wang, Cheng-Long] Xiamen Univ, Chenggong Hosp, Dept Gastroenterol, Army Grp Mil Hosp 73, Xiamen, Fujian, Peoples R China	New Jersey Institute of Technology; Asia University Taiwan; Naval Medical University; Naval Medical University; Xiamen University	Liu, YC; Chang, FC (corresponding author), Insign Med Technol Hong Kong Ltd, Room 635,6 F,Bldg 17 W,17 Sci Pk West Ave,Hong Kon, Hong Kong, Peoples R China.; Shih, FY (corresponding author), New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA.; Shih, FY (corresponding author), Asia Univ, Dept Comp Sci & Informat Engn, Taichung 413, Taiwan.; Yu, ED (corresponding author), Naval Med Univ, Changhai Hosp, Dept Colorectal Surg, Shanghai, Peoples R China.; Yu, ED (corresponding author), Naval Med Univ, Affiliated Hosp 1, Changhai Hosp, Gastrointestinal Endoscopy Ctr, Shanghai, Peoples R China.	yita.wu@insignmedical.com; shih@njit.edu; sunshinewangcl@126.com; eddie.hsiao@insign-medical.com; thomas.liu@insign-medical.com; kevin.chang@insign-medical.com; endayu@yeah.net		Hsiao, Kuang Ting/0009-0001-0735-0797; Chang, Kevin/0009-0004-3393-9708; YU, En-Da/0000-0003-4631-610X				Ahmad T, 2022, CLUSTER COMPUT, V25, P2403, DOI 10.1007/s10586-021-03456-4; Ali N., 2020, ARXIV; [Anonymous], 2014, ENGL J MED, V370, P1298; [Anonymous], 2014, INT J COMPUT ASSIST, V9, P283; Aslanian HR, 2013, AM J GASTROENTEROL, V108, P166, DOI 10.1038/ajg.2012.237; azquez D. V, 2017, J HEALTHC ENG, P1; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brenner H, 2011, ANN INTERN MED, V154, P22, DOI 10.7326/0003-4819-154-1-201101040-00004; Carrinho P, 2023, EXPERT SYST APPL, V232, DOI 10.1016/j.eswa.2023.120834; Chin J., 2011, CHIN J GASTROENTEROL, V20, P979, DOI DOI 10.3969/J.ISSN.1006-5709.2011.11.001; Dawwas MF, 2014, NEW ENGL J MED, V370, P2539, DOI [10.1056/NEJMc1405329, 10.1056/NEJMoa1309086]; Deng ZY, 2023, IET IMAGE PROCESS, V17, P3869, DOI 10.1049/ipr2.12903; Edwards BK, 2010, CANCER-AM CANCER SOC, V116, P544, DOI 10.1002/cncr.24760; International Agency for Research on Cancer, 2018, COL CANC FACTSH; Ishaq S, 2017, DIGEST LIVER DIS, V49, P721, DOI 10.1016/j.dld.2017.03.030; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Lieberman DA, 2012, GASTROENTEROLOGY, V143, P844, DOI 10.1053/j.gastro.2012.06.001; Nogueira K, 2021, IEEE ACCESS, V9, P114308, DOI 10.1109/ACCESS.2021.3104405; Sánchez-Peralta LF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10238501; Seeff LC, 2004, GASTROENTEROLOGY, V127, P1670, DOI 10.1053/j.gastro.2004.09.051; Shen YC, 2022, INT J PATTERN RECOGN, V36, DOI 10.1142/S0218001422520231; Shih FY, 2005, PATTERN RECOGN, V38, P2323, DOI 10.1016/j.patcog.2005.04.003; Shih FY, 2005, COMPUT VIS IMAGE UND, V99, P291, DOI 10.1016/j.cviu.2005.01.001; SHIH FY, 1995, IEEE T IMAGE PROCESS, V4, P1027, DOI 10.1109/83.392345; Siegel R, 2013, CA-CANCER J CLIN, V63, P11, DOI 10.3322/caac.21166; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Stewart C. P., 2014, WORLD CANC REPORT 20; Yeh CH, 2022, IEEE T NEUR NET LEAR, V33, P6129, DOI 10.1109/TNNLS.2021.3072414; Zauber AG, 2012, NEW ENGL J MED, V366, P687, DOI 10.1056/NEJMoa1100370; Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312	30	0	0	2	2	WORLD SCIENTIFIC PUBL CO PTE LTD	SINGAPORE	5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE	0218-0014	1793-6381		INT J PATTERN RECOGN	Int. J. Pattern Recognit. Artif. Intell.	MAR 30	2024	38	04								10.1142/S0218001424520098	http://dx.doi.org/10.1142/S0218001424520098		APR 2024	21	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	PO0X3					2024-09-18	WOS:001205501000004
J	Tiwari, S				Tiwari, Shamik			An Analysis in Tissue Classification for Colorectal Cancer Histology Using Convolution Neural Network and Colour Models	INTERNATIONAL JOURNAL OF INFORMATION SYSTEM MODELING AND DESIGN			English	Article						Color Models; Computer Vision-Based Identification; Convolution Neural Network; Tissue Classification	SEGMENTATION; IMAGES	Computer vision-based identification of different tissue categories in histological images is a critical application of the computer-assisted diagnosis (CAD). Computer-assisted diagnosis systems support to reduce the cost and increase the efficiency of this process. Traditional image classification approaches depend on feature extraction methods designed for a specific problem based on domain information. Deep learning approaches are becoming important alternatives with advance of machine learning technologies to overcome the numerous difficulties of the feature-based approaches. A method for the classification of histological images of human colorectal cancer containing seven different types of tissue using convolutional neural network (CNN) is proposed in this article. The method is evaluated using four different colour models in absence and presence of Gaussian noise. The highest classification accuracies are achieved with HVI colour model, which is 95.8% in nonexistence and 78.5% in existence of noise respectively.	[Tiwari, Shamik] UPES Univ, Sch Comp Sci & Engn, Dehra Dun, India	University of Petroleum & Energy Studies (UPES)	Tiwari, S (corresponding author), UPES Univ, Sch Comp Sci & Engn, Dehra Dun, India.		tiwari, shamik/AAR-2040-2021	tiwari, shamik/0000-0002-5987-7101				AALTONEN LA, 1993, SCIENCE, V260, P812, DOI 10.1126/science.8484121; [Anonymous], IEEE T PATTERN ANAL; [Anonymous], 2017, INT C SMART VEH TECH; [Anonymous], 2017, ICCV 2017 WORKSH COM; [Anonymous], 2017, ADV CHINESE DOCUMENT; [Anonymous], 2011, INT J COMPUT APPL T, DOI DOI 10.5120/2254-2886; [Anonymous], 2018, ARXIV180106734; [Anonymous], 1995, CONVOLUTIONAL NETWOR; [Anonymous], 2014 IEEE C COMP VIS; [Anonymous], 2017, INDONES J ELECT ENG; [Anonymous], 2015, Int. Arab J. Inform. Technol. (IAJIT); [Anonymous], ARXIV180700049; [Anonymous], 2002, P INT C IMAGE PROCES; Bergmeir C, 2012, COMPUT METH PROG BIO, V107, P497, DOI 10.1016/j.cmpb.2011.09.017; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen TH, 2004, IEEE IMAGE PROC, P1707; Ibraheem N., 2012, ARPN J SCI TECHNOLOG, V2, P265, DOI DOI 10.1016/J.SCIENTA.2016.01.030; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Ker J, 2018, IEEE ACCESS, V6, P9375, DOI 10.1109/ACCESS.2017.2788044; Li T, 2004, BIOINFORMATICS, V20, P2429, DOI 10.1093/bioinformatics/bth267; Madabhushi Anant., 2009, Digital pathology image analysis: opportunities and challenges; Mouelhi A, 2013, BIOMED SIGNAL PROCES, V8, P421, DOI 10.1016/j.bspc.2013.04.003; Mukhopadhyay S, 2018, PROC SPIE, V10501, DOI 10.1117/12.2291485; Prema CE, 2018, FIRE TECHNOL, V54, P255, DOI 10.1007/s10694-017-0683-x; Rathore S, 2014, COMPUT BIOL MED, V47, P76, DOI 10.1016/j.compbiomed.2013.12.010; Rowatt K, 2018, J HISTOTECHNOL, V41, P29, DOI 10.1080/01478885.2017.1417696; SCHWARZ MW, 1987, ACM T GRAPHIC, V6, P123, DOI 10.1145/31336.31338; Simard PY, 2003, PROC INT CONF DOC, P958; Song Y, 2013, IEEE T MED IMAGING, V32, P797, DOI 10.1109/TMI.2013.2241448; Sural S, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P589; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wang X. J., 2017, U.S. Patent, Patent No. [9,678,992, 9678992]; Wigington C, 2017, PROC INT CONF DOC, P639, DOI 10.1109/ICDAR.2017.110; Ye JP, 2004, IEEE ACM T COMPUT BI, V1, P181, DOI 10.1109/TCBB.2004.45; Zarit B. D., 1999, Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378), P58, DOI 10.1109/RATFG.1999.799224	35	13	13	0	6	IGI GLOBAL	HERSHEY	701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA	1947-8186	1947-8194		INT J INF SYST MODEL	Int. J. Inf. Syst. Modeling Des.	OCT-DEC	2018	9	4			SI				1	10.4018/IJISMD.2018100101	http://dx.doi.org/10.4018/IJISMD.2018100101			19	Computer Science, Information Systems	Emerging Sources Citation Index (ESCI)	Computer Science	HH3QG					2024-09-18	WOS:000455633600002
C	Brandao, P; Mazomenos, E; Ciuti, G; Calio, R; Bianchi, F; Menciassi, A; Dario, P; Koulaouzidis, A; Arezzo, A; Stoyanov, D		Armato, SG; Petrick, NA		Brandao, Patrick; Mazomenos, Evangelos; Ciuti, Gastone; Calio, Renato; Bianchi, Federico; Menciassi, Arianna; Dario, Paolo; Koulaouzidis, Anastasios; Arezzo, Alberto; Stoyanov, Danail			Fully Convolutional Neural Networks for Polyp Segmentation in Colonoscopy	MEDICAL IMAGING 2017: COMPUTER-AIDED DIAGNOSIS	Proceedings of SPIE		English	Proceedings Paper	Conference on Medical Imaging - Computer-Aided Diagnosis	FEB 13-16, 2017	Orlando, FL	SPIE, Alpin Med Syst				Colorectal cancer (CRC) is one of the most common and deadliest forms of cancer, accounting for nearly 10% of all forms of cancer in the world. Even though colonoscopy is considered the most effective method for screening and diagnosis, the success of the procedure is highly dependent on the operator skills and level of hand-eye coordination. In this work, we propose to adaptfully convolution neural networks (FCN), to identify and segment polyps in colonoscopy images. We converted three established networks into a fully convolution architecture and fine-tuned their learned representations to the polyp segmentation task. We validate our framework on the 2015 MICCAI polyp detection challenge dataset, surpassing the state-of-the-art in automated polyp detection. Our method obtained high segmentation accuracy and a detection precision and recall of 73.61% and 86.31%, respectively.	[Brandao, Patrick; Mazomenos, Evangelos; Stoyanov, Danail] UCL, Ctr Med Image Comp, London, England; [Ciuti, Gastone; Calio, Renato; Bianchi, Federico; Menciassi, Arianna; Dario, Paolo] Scuola Super Sant Anna, BioRobot Inst, Pisa, Italy; [Koulaouzidis, Anastasios] Royal Infirm Edinburgh NHS Trust, Endoscopy Unit, Edinburgh, Midlothian, Scotland; [Arezzo, Alberto] Univ Turin, Dept Surg Sci, Turin, Italy	University of London; University College London; Scuola Superiore Sant'Anna; University of Edinburgh; Royal Infirmary of Edinburgh; University of Turin	Brandao, P (corresponding author), UCL, Ctr Med Image Comp, London, England.	patrick.brandao.15@ucl.ac.uk; e.mazomenos@ucl.ac.uk; danail.stoyanov@ucl.ac.uk	Bianchi, Federico/JZT-6891-2024; Arezzo, Alberto/AAB-6552-2020; Koulaouzidis, Anastasios/G-9060-2014; Stoyanov, Danail/V-1043-2019; CIUTI, Gastone/T-6377-2018	Stoyanov, Danail/0000-0002-0980-3227; Bianchi, Federico/0000-0002-6155-5531; CIUTI, Gastone/0000-0002-0855-7976; Arezzo, Alberto/0000-0002-2110-4082; Mazomenos, Evangelos/0000-0003-0357-5996; Koulaouzidis, Anastasios/0000-0002-2248-489X	European Community's Horizon Programme (H) [688592]; EPSRC [EP/N013220/1, EP/N022750/1, EP/N027078/1, NS/A000027/1]; Wellcome Trust [WT101957, 201080/Z/16/Z]; EU-Horizon project EndoVESPA [H2020-ICT-2015-688592]; Wellcome Trust [201080/Z/16/Z] Funding Source: Wellcome Trust	European Community's Horizon Programme (H); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Wellcome Trust(Wellcome Trust); EU-Horizon project EndoVESPA; Wellcome Trust(Wellcome Trust)	The research leading to these results has received funding from the European Community's Horizon 2020 Programme (H2020/2014-2020) under Grant Agreement num. 688592 (EndoVESPA Project). Danail Stoyanov receives funding from the EPSRC (EP/N013220/1, EP/N022750/1, EP/N027078/1, NS/A000027/1),The Wellcome Trust (WT101957, 201080/Z/16/Z) and the EU-Horizon 2020 project EndoVESPA (H2020-ICT-2015-688592).	[Anonymous], P 3 INT C LEARNING R; Bernal J., 2016, IEEE T MED IMAG; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P., 2016, P CRAS, P1; Ciuti G, 2016, J MICRO-BIO ROBOT, V11, P1, DOI 10.1007/s12213-016-0087-x; Dumoulin V, 2018, GUIDE CONVOLUTION AR, DOI DOI 10.48550/ARXIV.1603.07285; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A., ADV NEURAL INF PROCE, V25, P1097; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tajbakhsh N, 2015, I S BIOMED IMAGING, P79, DOI 10.1109/ISBI.2015.7163821	15	109	115	1	10	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X	1996-756X	978-1-5106-0713-2; 978-1-5106-0714-9	PROC SPIE			2017	10134								UNSP 101340F	10.1117/12.2254361	http://dx.doi.org/10.1117/12.2254361			7	Engineering, Biomedical; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Engineering; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	BI1NB		Green Published			2024-09-18	WOS:000406425300014
J	Liu, ZH; Ji, CM; Ni, JC; Wang, YT; Qiao, LJ; Zheng, CH				Liu, Zhi-Hao; Ji, Cun-Mei; Ni, Jian-Cheng; Wang, Yu-Tian; Qiao, Li-Juan; Zheng, Chun-Hou			Convolution Neural Networks Using Deep Matrix Factorization for Predicting Circrna-Disease Association	IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS			English	Article						Convolution neural networks; circRNAs-disease association; deep matrix factorization	COLORECTAL-CANCER; CIRCULAR RNAS; TUMOR-GROWTH; PROLIFERATION; METASTASIS; APOPTOSIS; ONTOLOGY	CircRNAs have a stable structure, which gives them a higher tolerance to nucleases. Therefore, the properties of circular RNAs are beneficial in disease diagnosis. However, there are few known associations between circRNAs and disease. Biological experiments identify new associations is time-consuming and high-cost. As a result, there is a need of building efficient and achievable computation models to predict potential circRNA-disease associations. In this paper, we design a novel convolution neural networks framework(DMFCNNCD) to learn features from deep matrix factorization to predict circRNA-disease associations. Firstly, we decompose the circRNA-disease association matrix to obtain the original features of the disease and circRNA, and use the mapping module to extract potential nonlinear features. Then, we integrate it with the similarity information to form a training set. Finally, we apply convolution neural networks to predict the unknown association between circRNAs and diseases. The five-fold cross-validation on various experiments shows that our method can predict circRNA-disease association and outperforms state of the art methods.	[Liu, Zhi-Hao; Ji, Cun-Mei; Ni, Jian-Cheng; Wang, Yu-Tian; Qiao, Li-Juan; Zheng, Chun-Hou] Qufu Normal Univ, Sch Cyber Sci & Engn, Qufu 273165, Shandong, Peoples R China	Qufu Normal University	Wang, YT; Zheng, CH (corresponding author), Qufu Normal Univ, Sch Cyber Sci & Engn, Qufu 273165, Shandong, Peoples R China.	liuzhihao19971002@gmail.com; cunmeiji@126.com; nijch@126.com; wytfuture@163.com; qlj@qfnu.edu.cn; zhengch99@126.com	Liu, Jin-Xing/AAU-7257-2020; LI, LIXIN/KFS-0074-2024	Ji, Cunmei/0000-0002-7004-3351; Wang, Yutian/0000-0002-8033-8727; Liu, zhihao/0000-0002-1781-7952	National Natural Science Foundation of China [61873001, U19A2064]; Natural Science Foundation of Shandong Province [ZR2020KC022]; Open Project of Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, Anhui University [MMC202006]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Shandong Province(Natural Science Foundation of Shandong Province); Open Project of Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, Anhui University	This work was supported in part by the National Natural Science Foundation of China under Grants 61873001 and U19A2064, in part by the Natural Science Foundation of Shandong Province under Grant ZR2020KC022, in part by the Open Project of Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, Anhui University, under Grant MMC202006.	Chen X, 2018, BIOINFORMATICS, V34, P3178, DOI 10.1093/bioinformatics/bty333; Chen X, 2015, SCI REP-UK, V5, DOI 10.1038/srep11338; Chen X, 2019, THERANOSTICS, V9, P588, DOI 10.7150/thno.29678; COCQUERELLE C, 1993, FASEB J, V7, P155, DOI 10.1096/fasebj.7.1.7678559; Fan CY, 2018, INT J BIOL SCI, V14, P1950, DOI 10.7150/ijbs.28260; Fan CY, 2018, DATABASE-OXFORD, DOI 10.1093/database/bay044; Han X, 2021, CELL CYCLE, V20, P369, DOI 10.1080/15384101.2021.1874684; He Y, 2021, BRIEF BIOINFORM, V22, DOI 10.1093/bib/bbaa229; Huang GL, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0131225; Jeck WR, 2014, NAT BIOTECHNOL, V32, P453, DOI 10.1038/nbt.2890; Ji WX, 2018, BIOCHEM BIOPH RES CO, V497, P122, DOI 10.1016/j.bbrc.2018.02.036; Kristensen LS, 2018, ONCOGENE, V37, P555, DOI 10.1038/onc.2017.361; Lei XJ, 2018, INT J MOL SCI, V19, DOI 10.3390/ijms19113410; Li GH, 2019, RSC ADV, V9, P33222, DOI 10.1039/c9ra06133a; Lu CQ, 2021, IEEE J BIOMED HEALTH, V25, P891, DOI 10.1109/JBHI.2020.2999638; Ma H, 2021, FRONT ONCOL, V10; Miao XF, 2020, DIGEST LIVER DIS, V52, P1494, DOI 10.1016/j.dld.2020.07.019; Pan ZX, 2019, MOL THER-NUCL ACIDS, V17, P414, DOI 10.1016/j.omtn.2019.06.014; Qiu LP, 2019, J CANCER, V10, P3361, DOI 10.7150/jca.31243; Qu SB, 2017, RNA BIOL, V14, P992, DOI 10.1080/15476286.2016.1220473; Rafiemanesh H, 2016, J THORAC DIS, V8, DOI 10.21037/jtd.2016.03.91; Salzman J, 2013, PLOS GENET, V9, DOI 10.1371/journal.pgen.1003777; Schriml LM, 2015, MAMM GENOME, V26, P584, DOI 10.1007/s00335-015-9576-9; Schriml LM, 2012, NUCLEIC ACIDS RES, V40, pD940, DOI 10.1093/nar/gkr972; Shang QF, 2019, MOL CANCER, V18, DOI 10.1186/s12943-018-0934-6; Shen Z, 2022, IEEE ACM T COMPUT BI, V19, P753, DOI 10.1109/TCBB.2020.3007544; Shen Z, 2020, IEEE ACM T COMPUT BI, V17, P1741, DOI 10.1109/TCBB.2019.2910513; Tian F, 2017, BIOCHEM BIOPH RES CO, V493, P1260, DOI 10.1016/j.bbrc.2017.09.136; Tian Fang, 2017, Zhongguo Fei Ai Za Zhi, V20, P459, DOI 10.3779/j.issn.1009-3419.2017.07.04; Trabelsi A, 2019, BIOINFORMATICS, V35, pI269, DOI 10.1093/bioinformatics/btz339; Wan L, 2016, BIOMED RES INT, V2016, DOI 10.1155/2016/1579490; Wang D, 2010, BIOINFORMATICS, V26, P1644, DOI 10.1093/bioinformatics/btq241; Wang F, 2018, BIOMED PHARMACOTHER, V98, P775, DOI 10.1016/j.biopha.2018.01.015; Wang FL, 2016, AM J CANCER RES, V6, P1167; Wang JJ, 2018, J CLIN LAB ANAL, V32, DOI 10.1002/jcla.22379; Wang L, 2020, BIOINFORMATICS, V36, P4038, DOI 10.1093/bioinformatics/btz825; Wang SG, 2021, MOL THER NUCL ACIDS, V24, P154, DOI 10.1016/j.omtn.2021.02.014; Wang XB, 2020, ONCOTARGETS THER, V13, P1941, DOI 10.2147/OTT.S240642; Wei H, 2020, BRIEF BIOINFORM, V21, P1356, DOI 10.1093/bib/bbz057; Wen JY, 2020, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.00773; Xiao Q, 2019, IEEE J BIOMED HEALTH, V23, P2661, DOI 10.1109/JBHI.2019.2891779; Xie GB, 2021, INTERDISCIP SCI, V13, P582, DOI 10.1007/s12539-021-00455-2; Xue HJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3203; Yan C, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2522-6; Yao YS, 2019, BIOMED PHARMACOTHER, V111, P1367, DOI 10.1016/j.biopha.2018.12.120; Yu QN, 2020, BIOSCIENCE REP, V40, DOI 10.1042/BSR20191961; Zeng KX, 2018, CELL DEATH DIS, V9, DOI 10.1038/s41419-018-0454-8; Zhang QH, 2021, BRIEF BIOINFORM, V22, DOI 10.1093/bib/bbaa435; Zhang R, 2018, EUR REV MED PHARMACO, V22, P118, DOI 10.26355/eurrev_201801_14108; Zhao ZP, 2018, CASE STUD CONSTR MAT, V9, DOI 10.1016/j.cscm.2018.e00193; Zheng K, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1007872; Zheng XB, 2017, NEOPLASMA, V64, P321, DOI 10.4149/neo_2017_301; Zhong LH, 2018, BIOCHEM BIOPH RES CO, V499, P1044, DOI 10.1016/j.bbrc.2018.03.221; Zhu Q, 2018, BIOCHEM BIOPH RES CO, V497, P626, DOI 10.1016/j.bbrc.2018.02.119	54	3	3	4	37	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	1545-5963	1557-9964		IEEE ACM T COMPUT BI	IEEE-ACM Trans. Comput. Biol. Bioinform.	JAN-FEB	2023	20	1					277	284		10.1109/TCBB.2021.3138339	http://dx.doi.org/10.1109/TCBB.2021.3138339			8	Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Mathematics, Interdisciplinary Applications; Statistics & Probability	Science Citation Index Expanded (SCI-EXPANDED)	Biochemistry & Molecular Biology; Computer Science; Mathematics	J9ZZ6	34951853				2024-09-18	WOS:001013148100022
J	Rachapudi, V; Devi, GL				Rachapudi, Venubabu; Devi, G. Lavanya			Improved convolutional neural network based histopathological image classification	EVOLUTIONARY INTELLIGENCE			English	Article						Convolution neural network; Histopathological image classification; Machine learning	BREAST-CANCER DIAGNOSIS	Histopathological image classification is one of the important application areas of medical imaging. However, an accurate and efficient classification is still an open-ended research due to the complexity in histopathological images. For the same, this paper presents an efficient architecture of convolutional neural network for the classification of histopathological images. The proposed method consists of five subsequent blocks of layers, each having convolutional, drop-out, and max-pooling layers. The performance of the introduced classification system is validated on colorectal cancer histology image dataset which consists of RGB-colored images belonging to eight different classes. The experimental results confirm the higher performance of the proposed convolutional neural network against existing different machine learning models with the lowest error rate of 22.7%.	[Rachapudi, Venubabu] Koneru Lakshmaiah Educ Fdn, Dept Comp Sci & Engn, Guntur, Andhra Pradesh, India; [Devi, G. Lavanya] Andhra Univ, Dept Comp Sci & Syst Engn, AUCE A, Visakhapatnam, Andhra Pradesh, India	Koneru Lakshmaiah Education Foundation (K L Deemed to be University); Andhra University	Rachapudi, V (corresponding author), Koneru Lakshmaiah Educ Fdn, Dept Comp Sci & Engn, Guntur, Andhra Pradesh, India.	venubabu.r@gmail.com	G, LAVANYA/IST-9952-2023; RACHAPUDI, VENUBABU/U-5960-2018	RACHAPUDI, VENUBABU/0000-0002-5969-7733; , Lavanya Devi/0000-0002-9214-5789				Asri H, 2016, PROCEDIA COMPUT SCI, V83, P1064, DOI 10.1016/j.procs.2016.04.224; Baloglu UB, 2019, PATTERN RECOGN LETT, V122, P23, DOI 10.1016/j.patrec.2019.02.016; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51; Filipczuk P, 2013, IEEE T MED IMAGING, V32, P2169, DOI 10.1109/TMI.2013.2275151; Gurcan Metin N, 2009, IEEE Rev Biomed Eng, V2, P147, DOI 10.1109/RBME.2009.2034865; Han ZY, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04075-z; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Kowal M, 2013, COMPUT BIOL MED, V43, P1563, DOI 10.1016/j.compbiomed.2013.08.003; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Lo SCB, 1995, IEEE T MED IMAGING, V14, P711, DOI 10.1109/42.476112; Maqlin P., 2015, Mining Intelligence and Knowledge Exploration. Third International Conference, MIKE 2015. Proceedings: LNCS 9468, P269, DOI 10.1007/978-3-319-26832-3_26; Mittal H, 2019, ADV INTELL SYST, V817, P231, DOI 10.1007/978-981-13-1595-4_18; Mittal H, 2019, SWARM EVOL COMPUT, V45, P15, DOI 10.1016/j.swevo.2018.12.005; Oh SL, 2019, COMPUT BIOL MED, V105, P92, DOI 10.1016/j.compbiomed.2018.12.012; Pal R, 2018, 2018 11 INT C CONT C, P1; Pal R, 2019, APPL INTELL, V49, P3406, DOI 10.1007/s10489-019-01460-1; Rachapudi V., 2019, RECENT PATENTS COMPU, V12, P329, DOI DOI 10.2174/2213275912666181210165129; Saha M, 2018, COMPUT MED IMAG GRAP, V64, P29, DOI 10.1016/j.compmedimag.2017.12.001; Saraswat M, 2014, MICRON, V65, P20, DOI 10.1016/j.micron.2014.04.001; Talo M, 2019, COGN SYST RES, V54, P176, DOI 10.1016/j.cogsys.2018.12.007; Yildirim Ö, 2018, COMPUT BIOL MED, V102, P411, DOI 10.1016/j.compbiomed.2018.09.009; Zhang YG, 2013, MACH VISION APPL, V24, P1405, DOI 10.1007/s00138-012-0459-8; Zheng YS, 2017, PATTERN RECOGN, V71, P14, DOI 10.1016/j.patcog.2017.05.010	25	20	20	2	15	SPRINGER HEIDELBERG	HEIDELBERG	TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY	1864-5909	1864-5917		EVOL INTELL	Evol. Intell.	SEP	2021	14	3			SI		1337	1343		10.1007/s12065-020-00367-y	http://dx.doi.org/10.1007/s12065-020-00367-y		FEB 2020	7	Computer Science, Artificial Intelligence	Emerging Sources Citation Index (ESCI)	Computer Science	WH0KC					2024-09-18	WOS:000520057700001
J	Safarov, S; Whangbo, TK				Safarov, Sirojbek; Whangbo, Taeg Keun			A-DenseUNet: Adaptive Densely Connected UNet for Polyp Segmentation in Colonoscopy Images with Atrous Convolution	SENSORS			English	Article						semantic segmentation; convolutional neural networks; colonoscopy; polyp segmentation; deep learning; attention; dilated convolution	NEURAL-NETWORKS; CANCER	Colon carcinoma is one of the leading causes of cancer-related death in both men and women. Automatic colorectal polyp segmentation and detection in colonoscopy videos help endoscopists to identify colorectal disease more easily, making it a promising method to prevent colon cancer. In this study, we developed a fully automated pixel-wise polyp segmentation model named A-DenseUNet. The proposed architecture adapts different datasets, adjusting for the unknown depth of the network by sharing multiscale encoding information to the different levels of the decoder side. We also used multiple dilated convolutions with various atrous rates to observe a large field of view without increasing the computational cost and prevent loss of spatial information, which would cause dimensionality reduction. We utilized an attention mechanism to remove noise and inappropriate information, leading to the comprehensive re-establishment of contextual features. Our experiments demonstrated that the proposed architecture achieved significant segmentation results on public datasets. A-DenseUNet achieved a 90% Dice coefficient score on the Kvasir-SEG dataset and a 91% Dice coefficient score on the CVC-612 dataset, both of which were higher than the scores of other deep learning models such as UNet++, ResUNet, U-Net, PraNet, and ResUNet++ for segmenting polyps in colonoscopy images.	[Safarov, Sirojbek] Gachon Univ, Dept IT Convergence Engn, Seongnam Si 461701, Gyeonggi Do, South Korea; [Whangbo, Taeg Keun] Gachon Univ, Dept Comp Sci, Seongnam Si 461701, Gyeonggi Do, South Korea	Gachon University; Gachon University	Whangbo, TK (corresponding author), Gachon Univ, Dept Comp Sci, Seongnam Si 461701, Gyeonggi Do, South Korea.	sirojbeksafarov@gmail.com; tkwhangbo@gachon.ac.kr		Safarov, Sirojbek/0000-0001-5724-3271	GRRC program of Gyeonggi province [GRRC-Gachon2020]	GRRC program of Gyeonggi province	This work was supported by the GRRC program of Gyeonggi province. [GRRC-Gachon2020(B04), Development of AI-based Healthcare Devices].	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Alexandre LA, 2008, INT CONF BIOMED, P38, DOI 10.1109/BMEI.2008.246; Ameling S., 2009, P BILDV MED ALG SYST, P346; Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865; Arezzo A., 2018, J. Med. Robot. Res., V3, DOI 10.1142/S2424905X18400020; Arnold M, 2017, GUT, V66, P683, DOI 10.1136/gutjnl-2015-310912; Attanasio A, 2021, IEEE T MED ROBOT BIO, V3, P53, DOI 10.1109/TMRB.2021.3054326; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Casella A, 2020, ANN BIOMED ENG, V48, P848, DOI 10.1007/s10439-019-02424-9; Chen H, 2016, AAAI CONF ARTIF INTE, P1160; Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273; Chen H, 2015, LECT NOTES COMPUT SC, V9349, P507, DOI 10.1007/978-3-319-24553-9_62; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chollet Francois, 2015, Keras; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Holschneider M., 1989, WAVELETS INVERSE PRO, P289; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hwang S., 2007, 2007 IEEE INT C IMAG, V2, DOI DOI 10.1109/ICIP.2007.4379193; Iakovidis DK, 2005, COMP MED SY, P575, DOI 10.1109/CBMS.2005.6; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Li H., 2018, ARXIV; Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960; Li W, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67529-x; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Moeskops P, 2016, IEEE T MED IMAGING, V35, P1252, DOI 10.1109/TMI.2016.2548501; Mori Y, 2018, NAT BIOMED ENG, V2, P713, DOI 10.1038/s41551-018-0308-9; Rabeneck L, 2003, AM J GASTROENTEROL, V98, P471, DOI 10.1016/S0002-9270(02)05928-2; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roth HR, 2015, LECT NOTES COMPUT SC, V9349, P556, DOI 10.1007/978-3-319-24553-9_68; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Setio AAA, 2016, IEEE T MED IMAGING, V35, P1160, DOI 10.1109/TMI.2016.2536809; Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tajbakhsh N, 2015, I S BIOMED IMAGING, P79, DOI 10.1109/ISBI.2015.7163821; Vaswani A, 2017, ADV NEUR IN, V30; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang Y, 2015, COMPUT METH PROG BIO, V120, P164, DOI 10.1016/j.cmpb.2015.04.002; Wang Y, 2014, IEEE J BIOMED HEALTH, V18, P1379, DOI 10.1109/JBHI.2013.2285230; Yun K, 2020, IEEE ACCESS, V8, P32502, DOI 10.1109/ACCESS.2020.2973390; Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]; Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609	52	39	43	1	37	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		1424-8220		SENSORS-BASEL	Sensors	FEB	2021	21	4							1441	10.3390/s21041441	http://dx.doi.org/10.3390/s21041441			16	Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation	Science Citation Index Expanded (SCI-EXPANDED)	Chemistry; Engineering; Instruments & Instrumentation	QQ7DQ	33669539	gold, Green Submitted, Green Published			2024-09-18	WOS:000624682200001
J	Lu, CK; Liew, WS; Tang, TB; Lin, CH				Lu, Cheng-Kai; Liew, Win Sheng; Tang, Tong Boon; Lin, Cheng-Hung			Implementation of a Convolutional Neural Network Into an Embedded Device for Polyps Detection	IEEE EMBEDDED SYSTEMS LETTERS			English	Article						Convolutional neural networks; Field programmable gate arrays; Hardware; Training; Power demand; Cancer; Artificial intelligence; Artificial intelligence (AI); colorectal cancer (CRC); convolutional neural network (CNN); embedded devices; microprocessor; polyp detection		The increasing rates of colorectal cancer and associated mortality have attracted interest in the use of computer-aided diagnosis tools based on artificial intelligence (AI) for the detection of polyps at an early stage. Most AI models are implemented on software platforms; however, due to the demands of embedded devices, hardware implementations have to fulfill the demands of real-time applications with better accuracy and low-power consumption. In this letter, we propose an optimized four-layer network that can be implanted into an embedded device and determine the feasibility of implanting our convolutional neural network (CNN) into a microprocessor. The essential functions of the CNN (i.e., padding, convolution, ReLU, max-pooling, fully connected, and softmax layers) are implemented in the microprocessor. The proposed method achieves efficient classification with high performance and takes only 2.5488 mW at a working frequency of 8 MHz. We conclude this letter with a discussion of the results and future direction of research.	[Lu, Cheng-Kai] Natl Taiwan Normal Univ, Dept Elect Engn, Taipei 106, Taiwan; [Liew, Win Sheng; Tang, Tong Boon] Univ Teknol PETRONAS, Dept Elect & Elect Engn, Seri Iskandar 32610, Malaysia; [Lin, Cheng-Hung] Yuan Ze Univ, Dept Elect Engn, Taoyuan 320, Taiwan	National Taiwan Normal University; Universiti Teknologi Petronas; Yuan Ze University	Lu, CK (corresponding author), Natl Taiwan Normal Univ, Dept Elect Engn, Taipei 106, Taiwan.	cklu@ntnu.edu.tw	Tang, Tong/G-5610-2011; Lin, Cheng-Hung/V-5553-2019	Lu, ChengKai/0000-0002-5819-0754; Lin, Cheng-Hung/0000-0001-8373-2271; Tang, Tong Boon/0000-0002-5721-6828	Ministry of Higher Education Malaysia through the Fundamental Research Grant Scheme	Ministry of Higher Education Malaysia through the Fundamental Research Grant Scheme	No Statement Available	Chandan S, 2021, GASTROINTEST ENDOSC, V93, P68, DOI 10.1016/j.gie.2020.06.015; Eu CY, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165630; Gowda M. N., 2017, PROC 14 IEEE INDIA C, P1, DOI [10.1109/INDICON.2017.8487787, DOI 10.1109/INDICON.2017.8487787]; grand-challenge, Polyp-Grand challenge; grand-challenge, PolypCVC-ClinicDB; Heller S, 2018, IEEE ENG MED BIO, P2268, DOI 10.1109/EMBC.2018.8512735; Hugle M., 2018, PROC INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2018.8489493, 10.1109/IJCNN.2018.8489493]; Jafari A., 2017, PROC IEEE INT S CIRC, P1; Khatwani M, 2018, BIOMED CIRC SYST C, P499; Kiral-Kornek I, 2018, EBIOMEDICINE, V27, P103, DOI 10.1016/j.ebiom.2017.11.032; Lacey G, 2016, Arxiv, DOI arXiv:1602.04283; Liew WS, 2021, COMPUT METH PROG BIO, V206, DOI 10.1016/j.cmpb.2021.106114; Mapara SS, 2017, J CONTROL RELEASE, V261, P337, DOI 10.1016/j.jconrel.2017.07.005; Odagawa M., 2019, IEEE INT SYMP CIRC S, P1, DOI DOI 10.1109/iscas.2019.8702379; Patel K, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0236452; Patino-Barrientos S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020501; Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P164, DOI 10.1145/3083187.3083212; Vu TH, 2018, INT CONF BIG DATA, P326, DOI 10.1109/BigComp.2018.00055; Wang JC, 2018, IEEE T CIRCUITS-I, V65, P1941, DOI 10.1109/TCSI.2017.2767204	19	0	0	2	2	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	1943-0663	1943-0671		IEEE EMBED SYST LETT	IEEE Embed. Syst. Lett.	MAR	2024	16	1					5	8		10.1109/LES.2023.3234973	http://dx.doi.org/10.1109/LES.2023.3234973			4	Computer Science, Hardware & Architecture; Computer Science, Software Engineering; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KJ6I7					2024-09-18	WOS:001179627800007
J	Cui, RS; Yang, RZ; Liu, F; Cai, CQ				Cui, Rongsheng; Yang, Runzhuo; Liu, Feng; Cai, Chunqian			N-Net: Lesion region segmentations using the generalized hybrid dilated convolutions for polyps in colonoscopy images	FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY			English	Article						N-shape deep neural network; generalized hybrid dilated convolution; colonoscopy; colorectal polyp identification; lesion region segmentation; deep learning		Colorectal cancer is the cancer with the second highest and the third highest incidence rates for the female and the male, respectively. Colorectal polyps are potential prognostic indicators of colorectal cancer, and colonoscopy is the gold standard for the biopsy and the removal of colorectal polyps. In this scenario, one of the main concerns is to ensure the accuracy of lesion region identifications. However, the missing rate of polyps through manual observations in colonoscopy can reach 14%-30%. In this paper, we focus on the identifications of polyps in clinical colonoscopy images and propose a new N-shaped deep neural network (N-Net) structure to conduct the lesion region segmentations. The encoder-decoder framework is adopted in the N-Net structure and the DenseNet modules are implemented in the encoding path of the network. Moreover, we innovatively propose the strategy to design the generalized hybrid dilated convolution (GHDC), which enables flexible dilated rates and convolutional kernel sizes, to facilitate the transmission of the multi-scale information with the respective fields expanded. Based on the strategy of GHDC designing, we design four GHDC blocks to connect the encoding and the decoding paths. Through the experiments on two publicly available datasets on polyp segmentations of colonoscopy images: the Kvasir-SEG dataset and the CVC-ClinicDB dataset, the rationality and superiority of the proposed GHDC blocks and the proposed N-Net are verified. Through the comparative studies with the state-of-the-art methods, such as TransU-Net, DeepLabV3+ and CA-Net, we show that even with a small amount of network parameters, the N-Net outperforms with the Dice of 94.45%, the average symmetric surface distance (ASSD) of 0.38 pix and the mean intersection-over-union (mIoU) of 89.80% on the Kvasir-SEG dataset, and with the Dice of 97.03%, the ASSD of 0.16 pix and the mIoU of 94.35% on the CVC-ClinicDB dataset.	[Cui, Rongsheng; Yang, Runzhuo; Liu, Feng] Nankai Univ, Coll Elect Informat & Opt Engn, Tianjin, Peoples R China; [Liu, Feng] Nankai Univ, Tianjin Key Lab Optoelect Sensor & Sensing Network, Tianjin, Peoples R China; [Cai, Chunqian] Tianjin Univ Tradit Chinese Med, Teaching Hosp 1, Tianjin, Peoples R China; [Cai, Chunqian] Natl Clin Res Ctr Chinese Med Acupuncture & Moxibu, Tianjin, Peoples R China	Nankai University; Nankai University; Tianjin University of Traditional Chinese Medicine	Liu, F (corresponding author), Nankai Univ, Coll Elect Informat & Opt Engn, Tianjin, Peoples R China.; Liu, F (corresponding author), Nankai Univ, Tianjin Key Lab Optoelect Sensor & Sensing Network, Tianjin, Peoples R China.	liuf@nankai.edu.cn	Cui, Rongsheng/GZA-6965-2022		National Natural Science Foundation of China; Natural Science Foundation of Tianjin City of Peoples Republic of China;  [61901233];  [19JCQNJC00900]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Tianjin City of Peoples Republic of China; ; 	Funding This work was supported by the National Natural Science Foundation of China (grant no. 61901233) and the Natural Science Foundation of Tianjin City of Peoples Republic of China (grant no. 19JCQNJC00900).	Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chen J., 2021, PREPRINT, DOI [DOI 10.48550/ARXIV.2102.04306, 10.48550/arxiv.2102.04306]; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen WM, 2020, IEEE T MED IMAGING, V39, P1582, DOI 10.1109/TMI.2019.2953626; Fu XR, 2019, IEEE ACCESS, V7, P148645, DOI 10.1109/ACCESS.2019.2946582; Ganz M, 2012, IEEE T BIO-MED ENG, V59, P2144, DOI 10.1109/TBME.2012.2195314; Gu R, 2021, IEEE T MED IMAGING, V40, P699, DOI 10.1109/TMI.2020.3035253; He BS, 2020, FRONT BIOENG BIOTECH, V8, DOI 10.3389/fbioe.2020.00737; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Kingma D. P., 2017, arXiv; Li KD, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0255809; Pereira AAL, 2020, CLIN COLORECTAL CANC, V19, pE264, DOI 10.1016/j.clcc.2020.06.004; Liu JF, 2020, IEEE J BIOMED HEALTH, V24, P3520, DOI 10.1109/JBHI.2020.3004271; Ma YJ, 2022, IEEE T MULTIMEDIA, V24, P261, DOI 10.1109/TMM.2021.3050059; Misawa M, 2021, GASTROINTEST ENDOSC, V93, P960, DOI 10.1016/j.gie.2020.07.060; Oktay O, 2018, Arxiv, DOI arXiv:1804.03999; PIBAdb, 2022, COL POL IM COH; Qadir HA, 2020, IEEE J BIOMED HEALTH, V24, P180, DOI 10.1109/JBHI.2019.2907434; Ren YC, 2019, IEEE J BIOMED HEALTH, V23, P324, DOI 10.1109/JBHI.2018.2808199; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rozo A, 2022, FRONT BIOENG BIOTECH, V10, DOI 10.3389/fbioe.2022.806761; Rundle AG, 2008, GASTROENTEROLOGY, V134, P1311, DOI 10.1053/j.gastro.2008.02.032; Sánchez-Peralta LF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10238501; Sasmal P, 2018, INT CONF CIRC SYST S, P201, DOI 10.1109/ICSIGSYS.2018.8372666; Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900; Sun ZJ, 2016, IEEE COMMUN LETT, V20, P622, DOI 10.1109/LCOMM.2016.2518662; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tan JX, 2020, IEEE T MED IMAGING, V39, P2013, DOI 10.1109/TMI.2019.2963177; Torre LA, 2015, CA-CANCER J CLIN, V65, P87, DOI 10.3322/caac.21262; van Rijn JC, 2006, AM J GASTROENTEROL, V101, P343, DOI 10.1111/j.1572-0241.2006.00390.x; van Toledo DEFWM, 2022, LANCET GASTROENTEROL, V7, P747, DOI 10.1016/S2468-1253(22)00090-5; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Yu LQ, 2017, IEEE J BIOMED HEALTH, V21, P65, DOI 10.1109/JBHI.2016.2637004; Alom MZ, 2018, Arxiv, DOI [arXiv:1802.06955, DOI 10.48550/ARXIV.1802.06955]; Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568; Zhou LC, 2018, IEEE COMPUT SOC CONF, P192, DOI 10.1109/CVPRW.2018.00034; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	41	2	2	0	11	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND	2296-4185			FRONT BIOENG BIOTECH	Front. Bioeng. Biotechnol.	OCT 7	2022	10								963590	10.3389/fbioe.2022.963590	http://dx.doi.org/10.3389/fbioe.2022.963590			15	Biotechnology & Applied Microbiology; Engineering, Biomedical	Science Citation Index Expanded (SCI-EXPANDED)	Biotechnology & Applied Microbiology; Engineering	5R5DJ	36277395	Green Published, gold			2024-09-18	WOS:000874530800001
C	Venkatayogi, N; Hu, Q; Kara, OC; Mohanraj, TG; Atashzar, SF; Alambeigi, F			IEEE	Venkatayogi, Nethra; Hu, Qin; Kara, Ozdemir Can; Mohanraj, Tarunraj G.; Atashzar, S. Farokh; Alambeigi, Farshid			On the Potentials of Surface Tactile Imaging and Dilated Residual Networks for Early Detection of Colorectal Cancer Polyps	2023 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, IROS	IEEE International Conference on Intelligent Robots and Systems		English	Proceedings Paper	IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	OCT 01-05, 2023	Detroit, MI	IEEE, RSJ			PARIS CLASSIFICATION; DIAGNOSIS	This study proposes a novel diagnosis framework to decrease the early detection miss rate of colorectal cancer (CRC) polyps by using a hypersensitive vision-based tactile sensor (HySenSe) and a deep residual neural network. The HySenSe generates high-resolution 3D textural images of 160 realistic polyp phantoms for accurate classification via the proposed deep learning (DL) architecture. The DL module explores lightweight dilated convolutions, residual neural network architecture, and transfer learning to overcome the challenge of a small dataset of 229 images. Results show that the proposed architecture outperforms state-of-the-art DL models (i.e., EfficientNet and DenseNet) with a 94% accuracy, offering a promising solution for improving early detection of CRC polyps. The proposed framework can be used as a diagnostic module within tele-assessment medical robots, highlighting the potential of advanced technology and deep learning to revolutionize the early detection and treatment of CRC.	[Venkatayogi, Nethra] Univ Texas Austin, Dept Biomed Engn, Austin, TX 78712 USA; [Hu, Qin; Atashzar, S. Farokh] NYU, Dept Elect & Comp Engn, New York, NY USA; [Hu, Qin; Atashzar, S. Farokh] NYU, Dept Mech & Aerosp Engn, New York, NY USA; [Kara, Ozdemir Can; Mohanraj, Tarunraj G.; Alambeigi, Farshid] Univ Texas Austin, Walker Dept Mech Engn, Austin, TX USA; [Kara, Ozdemir Can; Mohanraj, Tarunraj G.; Alambeigi, Farshid] Univ Texas Austin, Texas Robot, Austin, TX USA	University of Texas System; University of Texas Austin; New York University; New York University Tandon School of Engineering; New York University; New York University Tandon School of Engineering; University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Venkatayogi, N (corresponding author), Univ Texas Austin, Dept Biomed Engn, Austin, TX 78712 USA.	venkatayoginethra@utexas.edu; qh503@nyu.edu; ozdemirckara@utexas.edu; tarunrajgm@utexas.edu; f.atashzar@nyu.edu; farshid.alambeigi@austin.utexas.edu	Hu, Qin/JXW-7142-2024; Atashzar, Seyed/AAB-8243-2019; Alambeigi, Farshid/N-1974-2019		National Cancer Institute of the National Institutes of Health [R21CA280747]	National Cancer Institute of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI))	Research reported in this publication was supported by the National Cancer Institute of the National Institutes of Health under Award Number R21CA280747.	Axon A, 2005, ENDOSCOPY, V37, P570, DOI 10.1055/s-2005-861352; Camboni D, 2021, IEEE T MED ROBOT BIO, V3, P64, DOI 10.1109/TMRB.2020.3037255; Garrido A, 2021, IEEE ACCESS, V9, P148048, DOI 10.1109/ACCESS.2021.3124019; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heijnsdijk EAM, 2003, SURG ENDOSC, V17, P1923, DOI 10.1007/s00464-003-9002-3; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Jemal A, 2009, CA-CANCER J CLIN, V59, P225, DOI 10.3322/caac.20006; Kapuria Siddhartha, 2023, 2023 International Symposium on Medical Robotics (ISMR), P1, DOI 10.1109/ISMR57123.2023.10130197; Kara O. C., 2023 IEEE INT C INT; Kara O. C., 2023 IEEE INT C INT 2023 IEEE INT C INT; Kara OC, 2022, IEEE SENSOR, DOI 10.1109/SENSORS52175.2022.9967133; Kara OC, 2023, ANN BIOMED ENG, V51, P1499, DOI 10.1007/s10439-023-03153-w; KUDO S, 1994, J CLIN PATHOL, V47, P880, DOI 10.1136/jcp.47.10.880; Kudo S, 2020, CLIN GASTROENTEROL H, V18, P1874, DOI 10.1016/j.cgh.2019.09.009; Kudo SE, 2019, DIGEST ENDOSC, V31, P363, DOI 10.1111/den.13340; Kusters K. C., 2022, MED IMAGING 2022 COM, V12033, P442; Li M, 2014, WORLD J GASTROENTERO, V20, P12649, DOI 10.3748/wjg.v20.i35.12649; Lou GC, 2014, TURK J GASTROENTEROL, V25, P182, DOI 10.5152/tjg.2014.4664; Mori Y, 2017, ENDOSCOPY, V49, P813, DOI 10.1055/s-0043-109430; Othman W, 2022, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.705662; Patino-Barrientos S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020501; Poudel S, 2020, IEEE ACCESS, V8, P99227, DOI 10.1109/ACCESS.2020.2996770; Rex DK, 2019, CLIN GASTROENTEROL H, V17, P1428, DOI 10.1016/j.cgh.2018.09.040; Ribeiro E, 2017, I S BIOMED IMAGING, P1044, DOI 10.1109/ISBI.2017.7950695; Ribeiro E, 2016, COMPUT MATH METHOD M, V2016, DOI 10.1155/2016/6584725; Sánchez-Peralta LF, 2020, ARTIF INTELL MED, V108, DOI 10.1016/j.artmed.2020.101923; Shaukat A, 2020, GASTROINTEST ENDOSC, V92, P997, DOI 10.1016/j.gie.2020.09.039; Shin Y, 2017, IEEE ENG MED BIO, P3277, DOI 10.1109/EMBC.2017.8037556; Tan MX, 2019, PR MACH LEARN RES, V97; van Doorn SC, 2015, AM J GASTROENTEROL, V110, P180, DOI 10.1038/ajg.2014.326; Venkatayogi Nethra, 2022, 2022 IEEE Sensors, P1, DOI 10.1109/SENSORS52175.2022.9967308; Viscaino M, 2021, WORLD J GASTROENTERO, V27, P6399, DOI 10.3748/wjg.v27.i38.6399; Wang A., 2021, ANN TRANSLATIONAL ME ANN TRANSLATIONAL ME, V9; Wang WL, 2020, BMC PSYCHIATRY, V20, DOI 10.1186/s12888-020-02566-4; Won CH, 2021, IEEE SENS J, V21, P12578, DOI 10.1109/JSEN.2021.3078369; Yu DJ, 2014, LECT NOTES ARTIF INT, V8818, P364, DOI 10.1007/978-3-319-11740-9_34; Yu F., 2017, P IEEE C COMP VIS PA, P472, DOI 10.1109/CVPR.2017.75; Yuan WZ, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122762; Zhang RK, 2017, IEEE J BIOMED HEALTH, V21, P41, DOI 10.1109/JBHI.2016.2635662; Zhou SW, 2019, CHIN CONTR CONF, P8568, DOI [10.23919/ChiCC.2019.8865226, 10.23919/chicc.2019.8865226]; Zulina N., 2019, EUR C BIOM OPT, p11073 52	42	0	0	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2153-0858		978-1-6654-9190-7	IEEE INT C INT ROBOT			2023							4655	4661		10.1109/IROS55552.2023.10342161	http://dx.doi.org/10.1109/IROS55552.2023.10342161			7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods; Robotics	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Robotics	BW2ZH					2024-09-18	WOS:001133658803077
J	Liu, S; Liu, X; Chang, SL; Sun, YF; Li, KY; Hou, Y; Wang, SW; Meng, J; Zhao, QL; Wu, SB; Yang, K; Xue, LY				Liu, Shuang; Liu, Xiao; Chang, Shilong; Sun, Yufeng; Li, Kaiyuan; Hou, Ya; Wang, Shiwei; Meng, Jie; Zhao, Qingliang; Wu, Sibei; Yang, Kun; Xue, Linyan			Multi-Classification of Polyps in Colonoscopy Images Based on an Improved Deep Convolutional Neural Network	CMC-COMPUTERS MATERIALS & CONTINUA			English	Article						Colorectal polyps; four-and six-category classifications; convolutional neural network; dilated residual network	COLORECTAL-CANCER; TASK-FORCE	Achieving accurate classification of colorectal polyps during colonoscopy can avoid unnecessary endoscopic biopsy or resection. This study aimed to develop a deep learning model that can automatically classify colorectal polyps histologically on white-light and narrow-band imaging (NBI) colonoscopy images based on World Health Organization (WHO) and Workgroup serrAted polypS and Polyposis (WASP) classification criteria for colorectal polyps. White-light and NBI colonoscopy images of colorectal polyps exhibiting pathological results were firstly collected and classified into four categories: conventional adenoma, hyperplastic polyp, sessile serrated adenoma/polyp (SSAP) and normal, among which conventional adenoma could be further divided into three sub-categories of tubular adenoma, villous adenoma and villioustublar adenoma, subsequently the images were re-classified into six categories. In this paper, we proposed a novel convolutional neural network termed Polyp-DedNet for the four-and six-category classification tasks of colorectal polyps. Based on the existing classification network ResNet50, Polyp-DedNet adopted dilated convolution to retain more high-dimensional spatial information and an Efficient Channel Attention (ECA) module to improve the classification performance further. To eliminate gridding artifacts caused by dilated convolutions, traditional convolutional layers were used instead of the max pooling layer, and two convolutional layers with progressively decreasing dilation were added at the end of the network. Due to the inevitable imbalance of medical image data, a regularization method DropBlock and a Class-Balanced (CB) Loss were performed to prevent network overfitting. Furthermore, the 5-fold cross -validation was adopted to estimate the performance of Polyp-DedNet for the multi-classification task of colorectal polyps. Mean accuracies of the proposed Polyp-DedNet for the four-and six-category classifications of colorectal polyps were 89.91% +/- 0.92% and 85.13% +/- 1.10%, respectively. The metrics of precision, recall and F1-score were also improved by 1%similar to 2% compared to the baseline ResNet50. The proposed Polyp-DedNet presented state-of-the-art performance for colorectal polyp classifying on white-light and NBI colonoscopy images, highlighting its considerable potential as an AI-assistant system for accurate colorectal polyp diagnosis in colonoscopy.	[Liu, Shuang; Liu, Xiao; Chang, Shilong; Li, Kaiyuan; Hou, Ya; Wang, Shiwei; Wu, Sibei; Yang, Kun; Xue, Linyan] Hebei Univ, Coll Qual & Tech Supervis, Baoding 071002, Peoples R China; [Liu, Shuang; Yang, Kun; Xue, Linyan] Hebei Technol Innovat Ctr Lightweight New Energy V, Baoding 071002, Peoples R China; [Liu, Shuang; Yang, Kun; Xue, Linyan] Hebei Univ, Natl & Local Joint Engn Res Ctr Metrol Instrument, Baoding 071002, Peoples R China; [Sun, Yufeng] Hebei Univ, Coll Elect Informat Engn, Baoding 071002, Peoples R China; [Meng, Jie] Hebei Univ, Affiliated Hosp, Dept Orthoped, Baoding 071002, Peoples R China; [Zhao, Qingliang] Xiamen Univ, Ctr Mol Imaging & Translat Med, State Key Lab Mol Vaccinol & Mol Diagnost, Dept Lab Med,Sch Publ Hlth, Xiamen 361102, Peoples R China	Hebei University; Hebei University; Hebei University; Hebei University; Xiamen University	Yang, K; Xue, LY (corresponding author), Hebei Univ, Coll Qual & Tech Supervis, Baoding 071002, Peoples R China.; Yang, K; Xue, LY (corresponding author), Hebei Technol Innovat Ctr Lightweight New Energy V, Baoding 071002, Peoples R China.; Yang, K; Xue, LY (corresponding author), Hebei Univ, Natl & Local Joint Engn Res Ctr Metrol Instrument, Baoding 071002, Peoples R China.	yangkun@hbu.edu.cn; lyxue@hbu.edu.cn	Zhao, Qingliang/AAH-5050-2020; Meng, Jie/B-3935-2019		Research Fund for Foundation of Hebei University [DXK201914]; President of Hebei University [XZJJ201914]; Post-graduate's Innovation Fund Project of Hebei University [HBU2022SS003]; Special Project for Cultivating College Students' Scientific and Technological Innovation Ability in Hebei Province [22E50041D]; Guangdong Basic and Applied Basic Research Foundation [2021A1515011654]; Fundamental Research Funds for the Central Universities of China [20720210117]	Research Fund for Foundation of Hebei University; President of Hebei University; Post-graduate's Innovation Fund Project of Hebei University; Special Project for Cultivating College Students' Scientific and Technological Innovation Ability in Hebei Province; Guangdong Basic and Applied Basic Research Foundation; Fundamental Research Funds for the Central Universities of China(Fundamental Research Funds for the Central Universities)	This work was funded by the Research Fund for Foundation of Hebei University (DXK201914), the President of Hebei University (XZJJ201914) , the Post-graduate's Innovation Fund Project of Hebei University (HBU2022SS003) the Special Project for Cultivating College Students' Scientific and Technological Innovation Ability in Hebei Province (22E50041D), Guangdong Basic and Applied Basic Research Foundation (2021A1515011654), and the Fundamental Research Funds for the Central Universities of China (20720210117).	Almadi MA, 2015, CAN J GASTROENTEROL, V29, P304, DOI 10.1155/2015/789038; Bibbins-Domingo K, 2016, JAMA-J AM MED ASSOC, V315, P2564, DOI 10.1001/jama.2016.5989; Byrne MF, 2019, GUT, V68, P94, DOI 10.1136/gutjnl-2017-314547; Cao W, 2021, CHINESE MED J-PEKING, V134, P783, DOI 10.1097/CM9.0000000000001474; Chen PJ, 2018, GASTROENTEROLOGY, V154, P568, DOI 10.1053/j.gastro.2017.10.010; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Fan SH, 2018, PHYS MED BIOL, V63, DOI 10.1088/1361-6560/aad51c; Ghiasi G, 2018, ADV NEUR IN, V31; GIGER ML, 1988, MED PHYS, V15, P158, DOI 10.1118/1.596247; Göret CC, 2018, MED SCI MONITOR, V24, P6809, DOI 10.12659/MSM.911012; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; IJspeert JEG, 2017, GUT, V66, P1225, DOI 10.1136/gutjnl-2015-310784; Ijspeert JE, 2015, GASTROINTEST ENDOSC, V81, pAB260, DOI 10.1016/j.gie.2015.03.1348; Khan MA, 2020, NEURAL COMPUT APPL, V32, P15929, DOI 10.1007/s00521-019-04514-0; Lee BI, 2012, CLIN ENDOSC, V45, P25, DOI 10.5946/ce.2012.45.1.25; Ozawa T, 2020, THER ADV GASTROENTER, V13, DOI 10.1177/1756284820910659; Pericàs JM, 2016, NEW ENGL J MED, V375, P387, DOI [10.1056/NEJMra1513581, 10.1056/NEJMc1604867]; Poudel S, 2020, IEEE ACCESS, V8, P99227, DOI 10.1109/ACCESS.2020.2996770; Powers D. M. W., 2011, J MACH LEARN TECHNOL, V2, P37; Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044; Rex DK, 2017, AM J GASTROENTEROL, V112, P1016, DOI 10.1038/ajg.2017.174; Shin Y, 2018, COMPUT MED IMAG GRAP, V69, P33, DOI 10.1016/j.compmedimag.2018.08.001; Tamaki T, 2011, LECT NOTES COMPUT SC, V6493, P452; Tan MX, 2021, PR MACH LEARN RES, V139, P7102; Wang Q., 2020, P IEEE CVF C COMP VI, P11531, DOI [DOI 10.1109/CVPR42600.2020.01155, DOI 10.1109/isqed48828.2020.9137057]; Wang WL, 2020, BMC PSYCHIATRY, V20, DOI 10.1186/s12888-020-02566-4; Wang YM, 2021, COMPLEXITY, V2021, DOI [10.1155/2021/6622149, 10.34133/2021/9817487]; Yamada M, 2015, GASTROINTEST ENDOSC, V82, P108, DOI 10.1016/j.gie.2014.12.037; Zauber AG, 2012, NEW ENGL J MED, V366, P687, DOI 10.1056/NEJMoa1100370	31	0	1	4	27	TECH SCIENCE PRESS	HENDERSON	871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA	1546-2218	1546-2226		CMC-COMPUT MATER CON	CMC-Comput. Mat. Contin.		2023	75	3					5837	5852		10.32604/cmc.2023.034720	http://dx.doi.org/10.32604/cmc.2023.034720			16	Computer Science, Information Systems; Materials Science, Multidisciplinary	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Materials Science	G7YJ8		gold			2024-09-18	WOS:000991265700020
C	Tomar, NK; Srivastava, A; Bagci, U; Jha, D		Shen, L; Gonzalez, AR; Santosh, KC; Lai, Z; Sicilia, R; Almeida, JR; Kane, B		Tomar, Nikhil Kumar; Srivastava, Abhishek; Bagci, Ulas; Jha, Debesh			Automatic Polyp Segmentation with Multiple Kernel Dilated Convolution Network	2022 IEEE 35TH INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL SYSTEMS (CBMS)	IEEE International Symposium on Computer-Based Medical Systems		English	Proceedings Paper	35th IEEE International Symposium on Computer-Based Medical Systems (CBMS)	JUL 21-23, 2022	ELECTR NETWORK	IEEE, IEEE Comp Soc		Deep learning; polyp segmentation; colonoscopy; multi-scale fusion; dilated convolution	COLORECTAL-CANCER; MISS RATE; COLONOSCOPY; VALIDATION; RISK	The detection and removal of precancerous polyps through colonoscopy is the primary technique for the prevention of colorectal cancer worldwide. However, the miss rate of colorectal polyp varies significantly among the endoscopists. It is well known that a computer-aided diagnosis (CAD) system can assist endoscopists in detecting colon polyps and minimize the variation among endoscopists. In this study, we introduce a novel deep learning architecture, named MKDCNet, for automatic polyp segmentation robust to significant changes in polyp data distribution. MKDCNet is simply an encoder-decoder neural network that uses the pre-trained ResNet50 as the encoder and novel multiple kernel dilated convolution (MKDC) block that expands the field of view to learn more robust and heterogeneous representation. Extensive experiments on four publicly available polyp datasets and cell nuclei dataset show that the proposed MKDCNet outperforms the state-of-the-art methods when trained and tested on the same dataset as well when tested on unseen polyp datasets from different distributions. With rich results, we demonstrated the robustness of the proposed architecture. From an efficiency perspective, our algorithm can process at (approximate to 45) frames per second on RTX 3090 GPU. MKDCNet can be a strong benchmark for building real-time systems for clinical colonoscopies. The code of the proposed MKDCNet is available at https://github.com/nikhilroxtomar/MKDCNet.	[Tomar, Nikhil Kumar] Indira Gandhi Natl Open Univ, Sch Comp & Informat Sci, Bengaluru, India; [Srivastava, Abhishek] Indian Stat Inst, Comp Vis & Pattern Recognit Unit, Bengaluru, India; [Bagci, Ulas; Jha, Debesh] Northwestern Univ, Dept Radiol, Machine & Hybrid Intelligence Lab, Evanston, IL 60208 USA	Indian Statistical Institute; Northwestern University	Tomar, NK (corresponding author), Indira Gandhi Natl Open Univ, Sch Comp & Informat Sci, Bengaluru, India.		Jha, Debesh/M-2526-2019; Srivastava, Abhishek/AFD-6242-2022; Bagci, Ulas/A-4225-2012	Bagci, Ulas/0000-0001-7379-6829	NIH [R01-CA246704, R01-CA240639]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This project is supported by the NIH funding: R01-CA246704 and R01-CA240639.	A. C. Society, 2020, COLORECTAL CANC FACT, P48; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Borgli H, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00622-y; Brenner H, 2011, ANN INTERN MED, V154, P22, DOI 10.7326/0003-4819-154-1-201101040-00004; Caicedo JC, 2019, NAT METHODS, V16, P1247, DOI 10.1038/s41592-019-0612-7; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Davidson KW, 2021, JAMA-J AM MED ASSOC, V325, P1965, DOI 10.1001/jama.2021.6238; Dawwas MF, 2014, NEW ENGL J MED, V370, P2539, DOI [10.1056/NEJMc1405329, 10.1056/NEJMoa1309086]; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heresbach D, 2008, ENDOSCOPY, V40, P284, DOI 10.1055/s-2007-995618; Hetzel JT, 2010, AM J GASTROENTEROL, V105, P2656, DOI 10.1038/ajg.2010.315; Hicks S., 2021, Nordic Machine Intelligence, V1, P1; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Huang C-H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.07172; Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Lan PN, 2021, Arxiv, DOI arXiv:2107.05023; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shen YT, 2021, LECT NOTES COMPUT SC, V12901, P559, DOI 10.1007/978-3-030-87193-2_53; Short MW, 2015, AM FAM PHYSICIAN, V91, P93; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23; Tomar NK, 2023, IEEE T NEUR NET LEAR, V34, P9375, DOI 10.1109/TNNLS.2022.3159394; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Wu H., 2022, IEEE T CYBERNETICS; Zauber AG, 2012, NEW ENGL J MED, V366, P687, DOI 10.1056/NEJMoa1100370; Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	33	13	13	0	3	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2372-9198		978-1-6654-6770-4	COMP MED SY			2022							317	322		10.1109/CBMS55023.2022.00063	http://dx.doi.org/10.1109/CBMS55023.2022.00063			6	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Engineering, Biomedical	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BT9UP	36777398	Green Accepted, Green Submitted			2024-09-18	WOS:000864612300056
J	Hou, WJ; Wang, Y; Zhao, ZQ; Cong, YZ; Pang, W; Tian, Y				Hou, Wenju; Wang, Yan; Zhao, Ziqi; Cong, Yizhi; Pang, Wei; Tian, Yuan			Hierarchical graph neural network with subgraph perturbations for key gene cluster discovery in cancer staging	COMPLEX & INTELLIGENT SYSTEMS			English	Article						Sample-specific networks; Graph convolution networks; Graph pooling; Cancer staging; Significant subgraph extraction	DRIVER GENES; IMMUNE MICROENVIRONMENT; COMPUTATIONAL METHODS; COLORECTAL-CANCER; MUTATION; METASTASIS; ADHESION; CXCR4; RATIO	Analyzing highly individual-specific genomic data to understand genetic interactions in cancer development is still challenging, with significant implications for the discovery of individual biomarkers as well as personalized medicine. With the rapid development of deep learning, graph neural networks (GNNs) have been employed to analyze a wide range of biomolecular networks. However, many neural networks are limited to black box models, which are only capable of making predictions, and they are often challenged to provide reliable biological and clinical insights. In this research, for sample-specific networks, a novel end-to-end hierarchical graph neural network with interpretable modules is proposed, which learns structural features at multiple scales and incorporates a soft mask layer in extracting subgraphs that contribute to classification. The perturbations caused by the input graphs' deductions are used to evaluate key gene clusters, and the samples are then grouped into classes to produce both sample- and stage-level explanations. Experiments on four gene expression datasets from The Cancer Genome Atlas (TCGA) show that the proposed model not only rivals the advanced GNN methods in cancer staging but also identifies key gene clusters that have a great impact on classification confidence, providing potential targets for personalized medicine.	[Hou, Wenju; Wang, Yan] Jilin Univ, Coll Comp Sci & Technol, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China; [Zhao, Ziqi] Jilin Univ, Coll Software, Changchun 130012, Peoples R China; [Wang, Yan; Cong, Yizhi; Tian, Yuan] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China; [Pang, Wei] Heriot Watt Univ, Sch Math & Comp Sci, Edinburgh EH14 4AS, Scotland	Jilin University; Jilin University; Jilin University; Heriot Watt University	Wang, Y (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.; Wang, Y (corresponding author), Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.	houwj20@mails.jlu.edu.cn; wy6868@jlu.edu.cn; zqzhao21@mails.jlu.edu.cn; congyz20@mails.jlu.edu.cn; w.pang@hw.ac.uk; yuantian@jlu.edu.cn	Zhao, Ziqi/JCD-6062-2023	Hou, Wenju/0000-0001-9247-8324; Pang, Wei/0000-0002-1761-6659	National Natural Science Foundation of China [62072212]; Development Project of Jilin Province of China [20220508125RC, 20210508060RQ]; National Key R amp; D Program [2018YFC2001302]; Jilin Provincial Key Laboratory of Big Data Intelligent Cognition [20210504003GH]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Development Project of Jilin Province of China; National Key R amp; D Program; Jilin Provincial Key Laboratory of Big Data Intelligent Cognition	AcknowledgementsThis work was supported by the National Natural Science Foundation of China (No. 62072212), the Development Project of Jilin Province of China (Nos. 20220508125RC, 20210508060RQ), National Key R & D Program (No. 2018YFC2001302), and the Jilin Provincial Key Laboratory of Big Data Intelligent Cognition (No. 20210504003GH).	Cao C, 2022, NUCLEIC ACIDS RES, V50, pD1123, DOI 10.1093/nar/gkab957; Chakravarthy R, 2016, BIOCHEM BIOPH RES CO, V478, P1541, DOI 10.1016/j.bbrc.2016.08.149; Chaudhary MS, 2021, BIOINFORMATICS, V37, P2521, DOI 10.1093/bioinformatics/btab145; Chen Z, 2022, BIOINFORMATICS, V38, P2781, DOI 10.1093/bioinformatics/btac203; Chen ZR, 2020, NEUROCOMPUTING, V417, P322, DOI 10.1016/j.neucom.2020.08.063; Colaprico A, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-019-13803-0; Del Giudice M, 2021, INT J MOL SCI, V22, DOI 10.3390/ijms22094563; Fan XL, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107084; Fanjul-Fernández M, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3531; Fey M., 2019, ICLR WORKSHOP REPRES; Gao HY, 2019, PR MACH LEARN RES, V97; Goïta AA, 2022, CANCERS, V14, DOI 10.3390/cancers14071810; Gottlin EB, 2011, J THORAC ONCOL, V6, P1687, DOI 10.1097/JTO.0b013e3182217bec; Guo WF, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1008962; Guo WF, 2020, BRIEF BIOINFORM, V21, P1641, DOI 10.1093/bib/bbz089; Hamilton WL, 2017, ADV NEUR IN, V30; Herbst RS, 2018, NATURE, V553, P446, DOI 10.1038/nature25183; Hou HY, 2019, CELL PROLIFERAT, V52, DOI 10.1111/cpr.12617; Hu W., 2019, 7 INT C LEARN REPR I, DOI DOI 10.48550/ARXIV.1810.00826; Hu Z, 2018, NAT REV IMMUNOL, V18, P168, DOI 10.1038/nri.2017.131; Huang DW, 2009, NAT PROTOC, V4, P44, DOI 10.1038/nprot.2008.211; Huang JJ, 2019, IEEE I CONF COMP VIS, P6489, DOI 10.1109/ICCV.2019.00658; Huang YH, 2021, BRIEF BIOINFORM, V22, DOI 10.1093/bib/bbaa062; Jahagirdar S, 2021, J PROTEOME RES, V20, P932, DOI 10.1021/acs.jproteome.0c00696; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Jiang L, 2019, NUCLEIC ACIDS RES, V47, DOI 10.1093/nar/gkz566; Jiang M, 2016, MOL CARCINOGEN, V55, P1087, DOI 10.1002/mc.22352; Jin T, 2021, BIOINFORMATICS, V37, P1115, DOI 10.1093/bioinformatics/btaa935; Kalpana G, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52746-w; Kan YX, 2022, BRIEF BIOINFORM, V23, DOI 10.1093/bib/bbab429; Kan YX, 2021, BRIEF FUNCT GENOMICS, V20, P333, DOI 10.1093/bfgp/elab032; Khasahmadi AH, 2020, INT C LEARNING REPRE; Lee J, 2019, PR MACH LEARN RES, V97; Li J, 2011, DIGEST LIVER DIS, V43, P40, DOI 10.1016/j.dld.2010.05.013; Li X, 2019, CANCER MED-US, V8, P5327, DOI 10.1002/cam4.2426; Li XX, 2019, LECT NOTES COMPUT SC, V11768, P485, DOI 10.1007/978-3-030-32254-0_54; Li Y, 2019, METHODS, V166, P4, DOI 10.1016/j.ymeth.2019.04.008; Liu Q, 2016, J HEMATOL ONCOL, V9, DOI 10.1186/s13045-016-0339-1; Liu XP, 2016, NUCLEIC ACIDS RES, V44, DOI 10.1093/nar/gkw772; Liu ZX, 2019, NATURE, V568, P249, DOI 10.1038/s41586-019-1041-6; Luo D., 2020, Advances in neural information processing sys-tems, V33, P19620; Ma BS, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103761; Ma Y, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P723, DOI 10.1145/3292500.3330982; McGranahan N, 2015, SCI TRANSL MED, V7, DOI 10.1126/scitranslmed.aaa1408; Mesquita, 2020, ADV NEURAL INF PROCE, P2220; Morris C, 2019, AAAI CONF ARTIF INTE, P4602; Nulsen J, 2021, GENOME MED, V13, DOI 10.1186/s13073-021-00830-0; Peng W, 2022, BRIEF BIOINFORM, V23, DOI 10.1093/bib/bbab432; Pham VVH, 2021, BIOINFORMATICS, V37, P3285, DOI 10.1093/bioinformatics/btab262; Pham VVH, 2020, BIOINFORMATICS, V36, pI583, DOI 10.1093/bioinformatics/btaa797; Pope PE, 2019, PROC CVPR IEEE, P10764, DOI 10.1109/CVPR.2019.01103; Privat M, 2020, INT J MED SCI, V17, P2799, DOI 10.7150/ijms.43101; Ranjan E, 2020, AAAI CONF ARTIF INTE, V34, P5470; Roy S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60740-w; Schulte-Sasse R, 2021, NAT MACH INTELL, V3, P513, DOI 10.1038/s42256-021-00325-y; Sherman BT, 2022, NUCLEIC ACIDS RES, V50, pW216, DOI 10.1093/nar/gkac194; Skrede OJ, 2020, LANCET, V395, P350, DOI 10.1016/S0140-6736(19)32998-8; Sondka Z, 2018, NAT REV CANCER, V18, P696, DOI 10.1038/s41568-018-0060-1; Su X, 2024, IEEE T NEUR NET LEAR, V35, P4682, DOI 10.1109/TNNLS.2021.3137396; Sudhakar M, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-021-04015-y; Supplitt S, 2021, INT J MOL SCI, V22, DOI 10.3390/ijms22031422; Tanvir RB, 2020, IEEE INT C BIOINFORM, P1813, DOI 10.1109/BIBM49941.2020.9313242; Taube JM, 2018, MODERN PATHOL, V31, P214, DOI 10.1038/modpathol.2017.156; Velickovic P., 2017, Stat, V1050, P10; Pham VVH, 2021, THERANOSTICS, V11, P5553, DOI 10.7150/thno.52670; Wang J, 2021, BRIEF BIOINFORM, V22, P1984, DOI 10.1093/bib/bbz167; Wang T, 2021, NUCLEIC ACIDS RES, V49, pD1289, DOI 10.1093/nar/gkaa1033; Wang X, 2021, 35 C NEURAL INFORM P; Wang X, 2023, IEEE T PATTERN ANAL, V45, P2297, DOI 10.1109/TPAMI.2022.3170302; Wei PJ, 2020, FRONT GENET, V11, DOI 10.3389/fgene.2020.00377; Wei TD, 2021, NONLINEAR DYNAM, V103, P1733, DOI 10.1007/s11071-021-06208-6; Wen Y, 2021, FRONT PHARMACOL, V12, DOI 10.3389/fphar.2021.645862; Wu M, 2010, CANCER-AM CANCER SOC, V116, P2768, DOI 10.1002/cncr.25181; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Xin XL, 2022, APPL MATH COMPUT, V412, DOI 10.1016/j.amc.2021.126537; Xu YH, 2023, IEEE T COMPUT SOC SY, V10, P602, DOI 10.1109/TCSS.2022.3169219; Xu ZL, 2021, NONLINEAR ANAL-HYBRI, V42, DOI 10.1016/j.nahs.2021.101088; Yang H, 2017, BIOINFORMATICS, V33, P483, DOI 10.1093/bioinformatics/btw662; Yang MQ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2058, DOI 10.1145/3442381.3449929; Ying Rex, 2019, Adv Neural Inf Process Syst, V32, P9240; Ying R, 2018, ADV NEUR IN, V31; Yuan H, 2020, ICLR; Yuan H, 2021, PR MACH LEARN RES, V139; Zhang JH, 2017, NUCLEIC ACIDS RES, V45, DOI 10.1093/nar/gkx089; Zhang L, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P3098, DOI 10.1145/3366423.3380083; Zhang MJ, 2018, ROUTL CONTEMP CHINA, P1, DOI 10.1109/ICNSC.2018.8361272; Zhang T, 2021, BIOINFORMATICS, V37, P4477, DOI 10.1093/bioinformatics/btab477; Zhou Y, 2022, ACM T INTEL SYST TEC, V13, DOI 10.1145/3495161	88	1	1	1	13	SPRINGER HEIDELBERG	HEIDELBERG	TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY	2199-4536	2198-6053		COMPLEX INTELL SYST	COMPLEX INTELL. SYST.	FEB	2024	10	1					111	128		10.1007/s40747-023-01068-6	http://dx.doi.org/10.1007/s40747-023-01068-6		JUL 2023	18	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HN7S4		gold			2024-09-18	WOS:001032507000003
J	Singh, G; Kamalja, A; Patil, R; Karwa, A; Tripathi, A; Chavan, P				Singh, Gaurav; Kamalja, Anushka; Patil, Rohit; Karwa, Ashutosh; Tripathi, Akansha; Chavan, Pallavi			A comprehensive assessment of artificial intelligence applications for cancer diagnosis	ARTIFICIAL INTELLIGENCE REVIEW			English	Article						Artificial intelligence; Cancer detection; Cancer diagnosis; Prediction; Deep learning; Machine learning; Image Processing	GASTRIC-CANCER; CLASSIFICATION	Artificial intelligence (AI) is being used increasingly to detect fatal diseases such as cancer. The potential reduction in human error, rapid diagnosis, and consistency of judgment are the primary motives for using these applications. Artificial Neural Networks and Convolution Neural Networks are popular AI techniques being increasingly used in diagnosis. Numerous academics have explored and evaluated AI methods used in the detection of various cancer types for comparison and analysis. This study presents a thorough evaluation of the AI techniques used in cancer detection based on extensively researched studies and research trials published on the subject. The manuscript offers a thorough evaluation and comparison of the AI methods applied to the detection of five primary cancer types: breast cancer, lung cancer, colorectal cancer, prostate cancer, skin cancer, and digestive cancer. To determine how well these models compare with medical professionals' judgments, the opinions of developed models and of experts are compared and provided in this paper.	[Singh, Gaurav; Kamalja, Anushka; Patil, Rohit; Karwa, Ashutosh; Tripathi, Akansha; Chavan, Pallavi] DY Patil Deemed Be Univ, Ramrao Adik Inst Technol, Informat Technol, Sect 7, Nerul Navi Mumbai 400706, Maharashtra, India		Chavan, P (corresponding author), DY Patil Deemed Be Univ, Ramrao Adik Inst Technol, Informat Technol, Sect 7, Nerul Navi Mumbai 400706, Maharashtra, India.	gaurav.s.140102@gmail.com; anushkakamalja@gmail.com; rohitpatil07122001@gmail.com; ashutoshkarwa18@gmail.com; akanshatripathi2601@gmail.com; pallavichavan11@gmail.com						Adegun A, 2021, ARTIF INTELL REV, V54, P811, DOI 10.1007/s10462-020-09865-y; Ali MS, 2021, MACH LEARN APPL, V5, DOI 10.1016/j.mlwa.2021.100036; Bindu G, 2006, PROG ELECTROMAGN RES, V58, P149, DOI 10.2528/PIER05081802; Das K, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph182413409; Dong JS, 2020, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.01629; Goel R., 2015, Int J Comput Appl, V112, P975; Goyal H, 2020, J CLIN MED, V9, DOI 10.3390/jcm9103313; Harmon SA, 2019, DIAGN INTERV RADIOL, V25, P183, DOI 10.5152/dir.2019.19125; Hekler A, 2019, EUR J CANCER, V120, P114, DOI 10.1016/j.ejca.2019.07.019; Hirasawa T, 2018, GASTRIC CANCER, V21, P653, DOI 10.1007/s10120-018-0793-2; Ho CW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-06264-x; Injadat M, 2021, ARTIF INTELL REV, V54, P3299, DOI 10.1007/s10462-020-09948-w; Kanimozhi T., 2016, Singaporean J. Sci. Res. (SJSR) J. Sel. Areas Microelectron. (JSAM), V8, P35; Kim HE, 2020, LANCET DIGIT HEALTH, V2, pE138, DOI 10.1016/S2589-7500(20)30003-0; Lorenzovici N, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11030514; Mustafa M., 2016, IOSR J Dental Med Sci, V15, P94, DOI DOI 10.9790/0853-15100494101; Naderan M., 2021, Syst Res Inf Technol, V2021, P98; Niu PH, 2020, WORLD J GASTROENTERO, V26, DOI 10.3748/wjg.v26.i36.5408; Pandiangan T, 2019, AT INDONES, V45, P9, DOI 10.17146/aij.2019.860; Patel K, 2022, ARTIF INTELL REV, V55, P3747, DOI 10.1007/s10462-021-10084-2; Perincheri S, 2021, MODERN PATHOL, V34, P1588, DOI 10.1038/s41379-021-00794-x; Rodriguez-Ruiz A, 2019, JNCI-J NATL CANCER I, V111, P916, DOI 10.1093/jnci/djy222; Saba T, 2020, J INFECT PUBLIC HEAL, V13, P1274, DOI 10.1016/j.jiph.2020.06.033; Sathykumar K., 2020, Cureus, V12, P1; Sharif HU., 2021, Int J Res Appl Sci Eng Technol, V09, P1121, DOI [10.22214/ijraset.2021.38582, DOI 10.22214/IJRASET.2021.38582]; Shastry KA, 2022, ARTIF INTELL REV, V55, P2641, DOI 10.1007/s10462-021-10074-4; Sitarz R, 2018, CANCER MANAG RES, V10, P239, DOI 10.2147/CMAR.S149619; Sokouti M, 2020, ARTIF INTELL REV, V53, P3287, DOI 10.1007/s10462-019-09764-x; Suzuki H, 2021, DIGEST ENDOSC, V33, P254, DOI 10.1111/den.13897; Tang DH, 2020, EBIOMEDICINE, V62, DOI 10.1016/j.ebiom.2020.103146; Utomo Chandra Prasetyo, 2014, International Journal of Advanced Research in Artificial Intelligence, V3, P10; Van Booven DJ, 2021, RES REP UROL, V13, P31, DOI 10.2147/RRU.S268596; Viscaino M, 2021, WORLD J GASTROENTERO, V27, P6399, DOI 10.3748/wjg.v27.i38.6399; Watanabe AT, 2019, J DIGIT IMAGING, V32, P625, DOI 10.1007/s10278-019-00192-5; Yu CR, 2022, ARTIF INTELL REV, V55, P323, DOI 10.1007/s10462-021-10034-y	35	0	0	2	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0269-2821	1573-7462		ARTIF INTELL REV	Artif. Intell. Rev.	JUN 20	2024	57	7							179	10.1007/s10462-024-10783-6	http://dx.doi.org/10.1007/s10462-024-10783-6			52	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	UX0R6		hybrid			2024-09-18	WOS:001251251200002
J	Mahmood, T; Wahid, A; Hong, JS; Kim, SG; Park, KR				Mahmood, Tahir; Wahid, Abdul; Hong, Jin Seong; Kim, Seung Gu; Park, Kang Ryoung			A novel convolution transformer-based network for histopathology-image classification using adaptive convolution and dynamic attention	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE			English	Article						Artificial intelligence; Deep learning; Convolution transformer -based network; Histopathology; Renal cell carcinoma	TEXTURE CLASSIFICATION	Renal cell carcinoma (RCC), which is the primary subtype of kidney cancer, is among the leading causes of cancer. Recent breakthroughs in computer vision, particularly deep learning, have revolutionized the analysis of histopathology images, thus providing potential solutions for tasks such as the grading of renal cell carcinoma. Nevertheless, the multitude of available neural network architectures and the absence of systematic evaluations render it challenging to identify optimal models and training configurations for distinct histopathology classification tasks. Hence, we propose a novel hybrid model that effectively combines the advantages of vision transformers and convolutional neural networks. The proposed method, which is named the renal cancer grading network, comprises two essential components: an adaptive convolution (AC) block and a dynamic attention (DA) block. The AC block emphasizes efficient feature extraction and spatial representation learning via intelligently designed convolutional operations. The DA block, which is constructed on the features of the AC block, is a crucial module for histopathology-image classification. It introduces a dynamic attention mechanism and employs a transformer encoder to refine learned representations. Experiments were conducted on four publicly available histopathology datasets: RCC dataset of Kasturba medical college (KMC), colorectal cancer histology (CRCH), break cancer histology (BreakHis) and colon cancer histopathology dataset (CCH). The proposed method demonstrated an accuracy of 90.62%, precision of 91.23%, recall of 90.63%, and a weighted harmonic mean of precision and recall (F1-score) of 90.92 on the KMC dataset. Similarly, the proposed method demonstrates consistent accuracy (weighted average F1-score of 99%) on the CRCH dataset, recognition rate of 88.30% on the BreakHis dataset, and an accuracy of 99.7% on CCH dataset. These results confirm that our method outperforms the state-of-the-art methods, thus demonstrating its effectiveness and robustness across various datasets.	[Mahmood, Tahir; Wahid, Abdul; Hong, Jin Seong; Kim, Seung Gu; Park, Kang Ryoung] Dongguk Univ, Div Elect & Elect Engn, 30 Pildong Ro 1 Gil, Seoul 04620, South Korea	Dongguk University	Park, KR (corresponding author), Dongguk Univ, Div Elect & Elect Engn, 30 Pildong Ro 1 Gil, Seoul 04620, South Korea.	parkgr@dongguk.edu	Kim, SeungGu/KSM-6269-2024	Wahid, Abdul/0009-0007-5101-8859	National Research Foundation of Korea (NRF) - Ministry of Science and ICT (MSIT) through the Basic Science Research Program [NRF-2022R1F1A1064291]; MSIT, Korea, under the Information Technology Research Center (ITRC) support program [IITP-2024-2020-0-01789]	National Research Foundation of Korea (NRF) - Ministry of Science and ICT (MSIT) through the Basic Science Research Program(National Research Foundation of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); MSIT, Korea, under the Information Technology Research Center (ITRC) support program(Ministry of Science & ICT (MSIT), Republic of Korea)	This research was supported partially by both the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (MSIT) through the Basic Science Research Program (NRF-2022R1F1A1064291) and by the MSIT, Korea, under the Information Technology Research Center (ITRC) support program (IITP-2024-2020-0-01789) supervised by the Institute for Information & Communications Technology Planning & Evaluation (IITP) .	Aatresh AA, 2021, INT J COMPUT ASS RAD, V16, P1549, DOI 10.1007/s11548-021-02410-4; Abu Haeyeh Y, 2022, BIOENGINEERING-BASEL, V9, DOI 10.3390/bioengineering9090423; Ahmed S, 2024, IEEE COMMUN SURV TUT, V26, P461, DOI 10.1109/COMST.2023.3334269; [Anonymous], STUDENTS T TEST; Azar AT, 2014, COMPUT METH PROG BIO, V113, P465, DOI 10.1016/j.cmpb.2013.11.004; Babita, 2024, EXPERT SYST APPL, V249, DOI 10.1016/j.eswa.2024.123569; Cervantes J, 2020, NEUROCOMPUTING, V408, P189, DOI 10.1016/j.neucom.2019.10.118; Chanchal AK, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-31275-7; COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155; d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830; Dai Z, 2021, ADV NEUR IN, V34; Dilshad N, 2024, IEEE INTERNET THINGS, V11, P13467, DOI 10.1109/JIOT.2023.3336931; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Du ZB, 2020, BIOMARK RES, V8, DOI 10.1186/s40364-020-00195-3; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675; Howard AG, 2017, Arxiv, DOI arXiv:1704.04861; Guha T, 2014, IEEE T MULTIMEDIA, V16, P980, DOI 10.1109/TMM.2014.2306175; Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957; Hamilton NA, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-110; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hatamizadeh A, 2023, INT C MACH LEARN, P12633, DOI [10.48550/arXiv.2206.09959, DOI 10.48550/ARXIV.2206.09959]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He QQ, 2023, COMPUT BIOL MED, V155, DOI 10.1016/j.compbiomed.2023.106629; He T, 2020, IEEE J BIOMED HEALTH, V24, P1762, DOI 10.1109/JBHI.2019.2949601; Hong SK, 2011, BJU INT, V107, P409, DOI 10.1111/j.1464-410X.2010.09561.x; Huang Y, 2017, IEEE J BIOMED HEALTH, V21, P1625, DOI 10.1109/JBHI.2017.2691738; Javed S, 2019, IEEE INT CONF COMP V, P342, DOI 10.1109/ICCVW.2019.00045; Javed S, 2020, IEEE T IMAGE PROCESS, V29, P9204, DOI 10.1109/TIP.2020.3023795; Javed S, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101696; Jiang Y, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214587; Kather JN, 2019, PLOS MED, V16, DOI 10.1371/journal.pmed.1002730; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Kaur J, 2023, COMPUT J, V66, P2011, DOI 10.1093/comjnl/bxac059; Khan MN, 2024, ACCIDENT ANAL PREV, V197, DOI 10.1016/j.aap.2024.107457; Lal Shyam, 2024, Multimedia Tools and Applications, V83, P60583, DOI 10.1007/s11042-023-17895-1; Lee DD, 2001, ADV NEUR IN, V13, P556; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Macenko M, 2009, I S BIOMED IMAGING, P1107, DOI 10.1109/ISBI.2009.5193250; Mahmood T, 2023, J KING SAUD UNIV-COM, V35, P740, DOI 10.1016/j.jksuci.2023.01.013; Mahmood T, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10111909; Maurya R, 2024, BIOMED SIGNAL PROCES, V94, DOI 10.1016/j.bspc.2024.106258; Mehta S, 2022, Arxiv, DOI arXiv:2110.02178; Nalisnik M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-15092-3; nvidia, Introducing the GeForce GTX 1070 Graphics Card: Gaming Perfected; Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623; Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27; RCG-Net, 2023, Renal-cancer-histopathology-classification; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Sarkar R, 2018, IEEE T IMAGE PROCESS, V27, P749, DOI 10.1109/TIP.2017.2763829; Schulz S, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.788740; Shafi S, 2023, DIAGN PATHOL, V18, DOI 10.1186/s13000-023-01375-z; Sirinukunwattana K, 2016, IEEE T MED IMAGING, V35, P1196, DOI 10.1109/TMI.2016.2525803; Song Y, 2013, LECT NOTES COMPUT SC, V8150, P452, DOI 10.1007/978-3-642-40763-5_56; Spanhol FA, 2016, IEEE T BIO-MED ENG, V63, P1455, DOI 10.1109/TBME.2015.2496264; Srinivas U, 2014, IEEE T MED IMAGING, V33, P1163, DOI 10.1109/TMI.2014.2306173; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Togaçar M, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106810; Togaçar M, 2020, PHYSICA A, V545, DOI 10.1016/j.physa.2019.123592; Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010; Vu TH, 2016, IEEE T MED IMAGING, V35, P738, DOI 10.1109/TMI.2015.2493530; Wessels F, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0272656; Xu Y, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1685-x; Xu Y, 2014, MED IMAGE ANAL, V18, P591, DOI 10.1016/j.media.2014.01.010; Yadav A., 2024, Int. J. Intell. Syst. Appl. Eng., V12, P689; Yang K, 2024, CMC-COMPUT MATER CON, V78, P393, DOI 10.32604/cmc.2023.044994; Yang Z, 2007, NEUROCOMPUTING, V71, P363, DOI 10.1016/j.neucom.2006.11.023; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zheng QY, 2023, CANCERS, V15, DOI 10.3390/cancers15123198; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	71	0	0	6	6	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0952-1976	1873-6769		ENG APPL ARTIF INTEL	Eng. Appl. Artif. Intell.	SEP	2024	135								108824	10.1016/j.engappai.2024.108824	http://dx.doi.org/10.1016/j.engappai.2024.108824		JUN 2024	15	Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Automation & Control Systems; Computer Science; Engineering	WG2D2		hybrid			2024-09-18	WOS:001253641800001
J	Eu, CY; Tang, TB; Lin, CH; Lee, LH; Lu, CK				Eu, Chin Yii; Tang, Tong Boon; Lin, Cheng-Hung; Lee, Lok Hua; Lu, Cheng-Kai			Automatic Polyp Segmentation in Colonoscopy Images Using a Modified Deep Convolutional Encoder-Decoder Architecture	SENSORS			English	Article						colorectal cancer; computer-aided diagnosis (CAD); SegNet Visual Geometry Group-19 (VGG-19); convolutional neural network (CNN); polyp segmentation		Colorectal cancer has become the third most commonly diagnosed form of cancer, and has the second highest fatality rate of cancers worldwide. Currently, optical colonoscopy is the preferred tool of choice for the diagnosis of polyps and to avert colorectal cancer. Colon screening is time-consuming and highly operator dependent. In view of this, a computer-aided diagnosis (CAD) method needs to be developed for the automatic segmentation of polyps in colonoscopy images. This paper proposes a modified SegNet Visual Geometry Group-19 (VGG-19), a form of convolutional neural network, as a CAD method for polyp segmentation. The modifications include skip connections, 5 x 5 convolutional filters, and the concatenation of four dilated convolutions applied in parallel form. The CVC-ClinicDB, CVC-ColonDB, and ETIS-LaribPolypDB databases were used to evaluate the model, and it was found that our proposed polyp segmentation model achieved an accuracy, sensitivity, specificity, precision, mean intersection over union, and dice coefficient of 96.06%, 94.55%, 97.56%, 97.48%, 92.3%, and 95.99%, respectively. These results indicate that our model performs as well as or better than previous schemes in the literature. We believe that this study will offer benefits in terms of the future development of CAD tools for polyp segmentation for colorectal cancer diagnosis and management. In the future, we intend to embed our proposed network into a medical capsule robot for practical usage and try it in a hospital setting with clinicians.	[Eu, Chin Yii; Tang, Tong Boon; Lee, Lok Hua; Lu, Cheng-Kai] Univ Teknol PETRONAS, Dept Elect & Elect Engn, Seri Iskandar 32610, Perak, Malaysia; [Lin, Cheng-Hung] Yuan Ze Univ, Res Ctr, Dept Elect Engn & Biomed Engn, Jhongli 32003, Taiwan	Universiti Teknologi Petronas; Yuan Ze University	Lu, CK (corresponding author), Univ Teknol PETRONAS, Dept Elect & Elect Engn, Seri Iskandar 32610, Perak, Malaysia.	chin_19000297@utp.edu.my; tongboon.tang@utp.edu.my; chlin@saturn.yzu.edu.tw; lee.lok_24987@utp.edu.my; chengkai.lu@utp.edu.my	Lin, Cheng-Hung/V-5553-2019; Tang, Tong Boon/JFS-4526-2023	LEE, LOK HUA/0000-0002-9762-7226; Tang, Tong Boon/0000-0002-5721-6828; Lin, Cheng-Hung/0000-0001-8373-2271; Lu, ChengKai/0000-0002-5819-0754	Ministry of Higher Education, Malaysia [FRGS/1/2020/TK0/UTP/02/23]; Ministry of Science and Technology, Taiwan [110-2221-E-155-013]	Ministry of Higher Education, Malaysia(Ministry of Education, Malaysia); Ministry of Science and Technology, Taiwan(Ministry of Science and Technology, Taiwan)	This work was supported by the Ministry of Higher Education, Malaysia, under Grant FRGS/1/2020/TK0/UTP/02/23. It was also partially funded by the Ministry of Science and Technology, Taiwan, under Grant 110-2221-E-155-013.	Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197; [Anonymous], P 3 INT C LEARNING R; [Anonymous], MAJORITY CANC CASES; [Anonymous], Tests to Detect Colorectal Cancer and Polyps; [Anonymous], 2015, arXiv: Learning; Azimi SM, 2019, IEEE T GEOSCI REMOTE, V57, P2920, DOI 10.1109/TGRS.2018.2878510; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bagheri M, 2019, IEEE ENG MED BIO, P6742, DOI [10.1109/EMBC.2019.8856793, 10.1109/embc.2019.8856793]; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361; Cao Yu, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P2349; Dutta S, 2018, 2018 INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, SIGNAL PROCESSING AND NETWORKING (WISPNET); Guo XD, 2019, MED PHYS, V46, P5666, DOI 10.1002/mp.13865; Hamaguchi R, 2018, IEEE WINT CONF APPL, P1442, DOI 10.1109/WACV.2018.00162; Han CY, 2019, IEEE ACCESS, V7, P43369, DOI 10.1109/ACCESS.2019.2908685; Huang CH, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP); Kang J, 2019, IEEE ACCESS, V7, P26440, DOI 10.1109/ACCESS.2019.2900672; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar P, 2018, IEEE IMAGE PROC, P3503, DOI 10.1109/ICIP.2018.8451295; Li QL, 2017, 2017 10TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI); Liew WS, 2021, COMPUT METH PROG BIO, V206, DOI 10.1016/j.cmpb.2021.106114; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Liu M, 2019, IEEE ACCESS, V7, P75058, DOI [10.1109/access.2019.2921027, 10.1109/ACCESS.2019.2921027]; Meng J, 2020, OPEN LIFE SCI, V15, P588, DOI 10.1515/biol-2020-0055; Mo X, 2018, INT C PATT RECOG, P3929, DOI 10.1109/ICPR.2018.8545174; Nguyen NQ, 2019, IEEE ACCESS, V7, P33795, DOI 10.1109/ACCESS.2019.2904094; Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002; Sánchez-Peralta LF, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8081316; Sasmal P, 2018, INT CONF CIRC SYST S, P201, DOI 10.1109/ICSIGSYS.2018.8372666; Sengar N, 2016, 2016 39TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P529, DOI 10.1109/TSP.2016.7760936; Shin Y, 2018, IEEE ACCESS, V6, P56007, DOI 10.1109/ACCESS.2018.2872717; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Sornapudi S, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9122404; Wickstrom K, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101619; Wittenberg Thomas, 2019, Current Directions in Biomedical Engineering, V5, P231, DOI 10.1515/cdbme-2019-0059; Yu F., 2015, arXiv preprint arXiv: 1506. 03365; Yu JY, 2019, I C DATA ENGIN WORKS, P306, DOI 10.1109/ICDEW.2019.00010; Zhang L, 2017, COMM COM INF SC, V723, P707, DOI 10.1007/978-3-319-60964-5_62; Zhang RK, 2018, PATTERN RECOGN, V83, P209, DOI 10.1016/j.patcog.2018.05.026; Zheng YL, 2018, IEEE ENG MED BIO, P4142, DOI 10.1109/EMBC.2018.8513337	41	4	4	2	21	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		1424-8220		SENSORS-BASEL	Sensors	AUG	2021	21	16							5630	10.3390/s21165630	http://dx.doi.org/10.3390/s21165630			24	Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation	Science Citation Index Expanded (SCI-EXPANDED)	Chemistry; Engineering; Instruments & Instrumentation	UH6WJ	34451072	gold, Green Published			2024-09-18	WOS:000690068100001
C	Wang, YH; Nguyen, PA; Islam, M; Li, YC; Yang, HC		OhnoMachado, L; Seroussi, B		Wang, Yu-Hsiang; Nguyen, Phung-Anh; Islam, Mohaimenul; Li, Yu-Chuan; Yang, Hsuan-Chia			Development of Deep Learning Algorithm for Detection of Colorectal Cancer in EHR Data	MEDINFO 2019: HEALTH AND WELLBEING E-NETWORKS FOR ALL	Studies in Health Technology and Informatics		English	Proceedings Paper	17th World Congress of Medical and Health Informatics (MEDINFO)	AUG 25-30, 2019	Int Med Informat Assoc, Lyon, FRANCE	French Assoc Med Informat	Int Med Informat Assoc	Colorectal Neoplasms; Algorithms; Electronic Health Records		We aimed to develop a deep learning model for the prediction of the risk of advanced colorectal cancer in Taiwanese adults. We collected data of 58152 patients from the Taiwan National Health Insurance database from 1999 to 2013. All patients' comorbidities and medications history were included in the development of the convolution neural network (CNN) model. We also used 3-year medical data of all patients before the diagnosed colorectal cancer (CRC) as the dimensional time in the model. The area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were computed to measure the performance of the model. The results showed the mean (SD) of AUC of the model was 0.922 (0.004). Moreover, the performance of the model observed the sensitivity of 0.837, specificity of 0.867, and 0.532 for PPV value. Our study utilized CNN to develop a prediction model for CRC, based on non-image and multi-dimensional medical records.	[Wang, Yu-Hsiang] Taipei Med Univ, Coll Med, Taipei, Taiwan; [Nguyen, Phung-Anh; Islam, Mohaimenul; Li, Yu-Chuan; Yang, Hsuan-Chia] Taipei Med Univ, Int Ctr Hlth Informat Technol, Taipei, Taiwan; [Li, Yu-Chuan] Taipei Med Univ, Coll Med Sci & Technol, Taipei, Taiwan	Taipei Medical University; Taipei Medical University; Taipei Medical University	Nguyen, PA (corresponding author), Taipei Med Univ, Coll Med Sci & Technol, Int Ctr Hlth Informat Technol, 250 Wuxing Str, Taipei 11031, Taiwan.	alexnthhp@tmu.edu.tw	Wang, Yu Hsiang/HLH-2514-2023; Islam, Md.Mohaimenul Islam/AAF-4058-2019; li, JACK/HHC-6755-2022; Le, Nguyen Quoc Khanh/H-2057-2017	Yang, Hsuan-Chia/0000-0001-9198-0697; Wang, Yu-Hsiang/0000-0001-7747-4456; Islam, Md.Mohaimenul/0000-0001-6026-2748	Ministry of Science and Technology (MOST) [106-2634-F-038 -001 CC2]	Ministry of Science and Technology (MOST)(Ministry of Science and Technology, China)	This research was sponsored in part by Ministry of Science and Technology (MOST) 107-2634-F-038-002-, and Ministry of Science and Technology (MOST) 106-2634-F-038 -001 CC2. We would like to thank Mr. Chia-Wei Liang for his assistant in this study.	[Anonymous], ADADELTA: An Adaptive Learning Rate Method; Chen BK, 2013, BMC PUBLIC HEALTH, V13, DOI 10.1186/1471-2458-13-551; Gründner J, 2018, STUD HEALTH TECHNOL, V247, P101, DOI 10.3233/978-1-61499-852-5-101; Hsieh MH, 2018, J CLIN MED, V7, DOI 10.3390/jcm7090277; Iqbal U, 2015, MEDICINE, V94, DOI 10.1097/MD.0000000000000483; Naylor CD, 2018, JAMA-J AM MED ASSOC, V320, P1099, DOI 10.1001/jama.2018.11103; Nguyen PA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082401; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wahl B, 2018, BMJ GLOB HEALTH, V3, DOI 10.1136/bmjgh-2018-000798; Wang YW, 2018, J FORMOS MED ASSOC, V117, P358, DOI 10.1016/j.jfma.2017.09.010; Yang H.-J., 2018, GASTROINTESTINAL END	11	23	24	0	10	IOS PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	0926-9630	1879-8365	978-1-64368-003-3; 978-1-64368-002-6	STUD HEALTH TECHNOL			2019	264						438	441		10.3233/SHTI190259	http://dx.doi.org/10.3233/SHTI190259			4	Computer Science, Interdisciplinary Applications; Health Care Sciences & Services; Medical Informatics	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Health Care Sciences & Services; Medical Informatics	BP9MI	31437961				2024-09-18	WOS:000569653400088
C	Hou, WT; Huang, HL; Peng, Q; Yu, RS; Yu, LQ; Wang, LS		Wang, L; Dou, Q; Fletcher, PT; Speidel, S; Li, S		Hou, Wentai; Huang, Helong; Peng, Qiong; Yu, Rongshan; Yu, Lequan; Wang, Liansheng			Spatial-Hierarchical Graph Neural Network with Dynamic Structure Learning for Histological Image Classification	MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION, MICCAI 2022, PT II	Lecture Notes in Computer Science		English	Proceedings Paper	25th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)	SEP 18-22, 2022	Singapore, SINGAPORE	MICCAI Soc		Histological image classification; Graph neural network; Dynamic structure; Hierarchical representation; Vision transformer	COLORECTAL-CANCER	Graph neural network (GNN) has achieved tremendous success in histological image classification, as it can explicitly model the notion and interaction of different biological entities (e.g., cell, tissue and etc.). However, the potential of GNN has not been fully unleashed for histological image analysis due to (1) the fixed design mode of graph structure and (2) the insufficient interactions between multi-level entities. In this paper, we proposed a novel spatial-hierarchical GNN framework (SHGNN) equipped with a dynamic structure learning (DSL) module for effective histological image classification. Compared with traditional GNNs, the proposed framework has two compelling characteristics. First, the DSL module integrates the positional attribute and semantic representation of entities to learn the adjacency relationship of them during the training process. Second, the proposed SHGNN can extract rich and discriminative features by mining the spatial features of different entities via graph convolutions and aggregating the semantic of multi-level entities via a vision transformer (ViT) based interaction mechanism. We evaluate the proposed framework on our collected colorectal cancer staging (CRCS) dataset and the public breast carcinoma subtyping (BRAGS) dataset. Experimental results demonstrate that our proposed method yield superior classification results compared to state-of-the-arts.	[Hou, Wentai] Xiamen Univ, Sch Informat, Informat & Commun Engn Dept, Xiamen, Peoples R China; [Huang, Helong; Peng, Qiong; Yu, Rongshan; Wang, Liansheng] Xiamen Univ, Sch Informat, Dept Comp Sci, Xiamen, Peoples R China; [Yu, Lequan] Univ Hong Kong, Dept Stat & Actuarial Sci, Pok Fu Lam, Hong Kong, Peoples R China	Xiamen University; Xiamen University; University of Hong Kong	Wang, LS (corresponding author), Xiamen Univ, Sch Informat, Dept Comp Sci, Xiamen, Peoples R China.	houwt@stu.xmu.edu.cn; hlhuang@stu.xmu.edu.cn; qpeng@stu.xmu.edu.cn; rsyu@xmu.edu.cn; lqyu@hku.hk; lswang@xmu.edu.cn	Wang, Liansheng/HGA-8949-2022; YU, Rongshan/AAF-3353-2021; huang, helong/HPD-2057-2023; Yu, Lequan/U-5377-2019	Yu, Lequan/0000-0002-9315-6527	Ministry of Science and Technology of the People's Republic of China [2021ZD0201900, 2021ZD0201903]	Ministry of Science and Technology of the People's Republic of China(Ministry of Science and Technology, China)	This work was supported by Ministry of Science and Technology of the People's Republic of China (2021ZD0201900) (2021ZD0201903).	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Anklin V, 2021, LECT NOTES COMPUT SC, V12902, P636, DOI 10.1007/978-3-030-87196-3_59; [Anonymous], 2020, ARXIV; Ba LJ., 2016, arXiv; Bai J, 2019, BIOMED RES INT, V2019, DOI 10.1155/2019/1065652; Brancati N, 2021, Arxiv, DOI arXiv:2111.04740; Cai Z., 2021, BMC Plant Biol, V21, P1, DOI [DOI 10.1186/S12870-021-02931-9/FIGURES/8, 10.1186/S12870-021-02931-9/FIGURES/8]; Chen RJ, 2022, IEEE T MED IMAGING, V41, P757, DOI [10.1109/TITS.2020.3030218, 10.1109/TMI.2020.3021387]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fey M., 2019, arXiv preprint arXiv, V1903, P02428; Graham S, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101563; Hamilton WL, 2017, ADV NEUR IN, V30; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou WT, 2021, MED IMAGE ANAL, V72, DOI 10.1016/j.media.2021.102092; Jaume G., 2011, P MICCAI WORKSHOP CO, V156, P117; Jia ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1324; Kiesslich R, 2004, GASTROENTEROLOGY, V127, P706, DOI 10.1053/j.gastro.2004.06.050; Li YX, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-019-2979-y; Li YJ, 2017, Arxiv, DOI [arXiv:1511.05493, 10.48550/arXiv.1511.05493, DOI 10.48550/ARXIV.1511.05493]; Lu WQ, 2020, IEEE COMPUT SOC CONF, P1049, DOI 10.1109/CVPRW50498.2020.00138; Nowacki TM, 2015, DIGEST DIS SCI, V60, P492, DOI 10.1007/s10620-014-3373-2; Pati P, 2022, MED IMAGE ANAL, V75, DOI 10.1016/j.media.2021.102264; Raju Ashwin, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P529, DOI 10.1007/978-3-030-59722-1_51; Shi XY, 2013, BIOCHEM BIOPH RES CO, V435, P282, DOI 10.1016/j.bbrc.2013.04.063; Sterlacci W., 2021, Multidisciplinary Treatment of Colorectal Cancer, P263, DOI [10.1007/978-3-030-58846-528, DOI 10.1007/978-3-030-58846-528]; Vaswani A, 2017, ADV NEUR IN, V30; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Yao XH, 2020, CELL RES, V30, P541, DOI 10.1038/s41422-020-0318-5; Zhou J, 2021, Arxiv, DOI [arXiv:1812.08434, DOI 10.48550/ARXIV.1812.08434]; Zhou Y., 2019, IEEE T ROBOT, P1	30	8	9	2	8	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	0302-9743	1611-3349	978-3-031-16434-7; 978-3-031-16433-0	LECT NOTES COMPUT SC			2022	13432						181	191		10.1007/978-3-031-16434-7_18	http://dx.doi.org/10.1007/978-3-031-16434-7_18			11	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Radiology, Nuclear Medicine & Medical Imaging	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Radiology, Nuclear Medicine & Medical Imaging	BU0DF					2024-09-18	WOS:000867288800018
J	Gessert, N; Bengs, M; Wittig, L; Droemann, D; Keck, T; Schlaefer, A; Ellebrecht, DB				Gessert, Nils; Bengs, Marcel; Wittig, Lukas; Droemann, Daniel; Keck, Tobias; Schlaefer, Alexander; Ellebrecht, David B.			Deep transfer learning methods for colon cancer classification in confocal laser microscopy images	INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY			English	Article						Colon cancer; Confocal laser microscopy; Transfer learning; Convolution neural network	CONVOLUTIONAL NEURAL-NETWORKS; PERITONEAL CARCINOMATOSIS; SKIN; CT	Purpose The gold standard for colorectal cancer metastases detection in the peritoneum is histological evaluation of a removed tissue sample. For feedback during interventions, real-time in vivo imaging with confocal laser microscopy has been proposed for differentiation of benign and malignant tissue by manual expert evaluation. Automatic image classification could improve the surgical workflow further by providing immediate feedback. Methods We analyze the feasibility of classifying tissue from confocal laser microscopy in the colon and peritoneum. For this purpose, we adopt both classical and state-of-the-art convolutional neural networks to directly learn from the images. As the available dataset is small, we investigate several transfer learning strategies including partial freezing variants and full fine-tuning. We address the distinction of different tissue types, as well as benign and malignant tissue. Results We present a thorough analysis of transfer learning strategies for colorectal cancer with confocal laser microscopy. In the peritoneum, metastases are classified with an AUC of 97.1, and in the colon the primarius is classified with an AUC of 73.1. In general, transfer learning substantially improves performance over training from scratch. We find that the optimal transfer learning strategy differs for models and classification tasks. Conclusions We demonstrate that convolutional neural networks and transfer learning can be used to identify cancer tissue with confocal laser microscopy. We show that there is no generally optimal transfer learning strategy and model as well as task-specific engineering is required. Given the high performance for the peritoneum, even with a small dataset, application for intraoperative decision support could be feasible.	[Gessert, Nils; Bengs, Marcel; Schlaefer, Alexander] Hamburg Univ Technol, Inst Med Technol, Hamburg, Germany; [Wittig, Lukas; Droemann, Daniel] Univ Med Ctr Schleswig Holstein, Dept Pulmol, Lubeck, Germany; [Keck, Tobias; Ellebrecht, David B.] Univ Med Ctr Schleswig Holstein, Dept Surg, Lubeck, Germany	Hamburg University of Technology; University of Kiel; Schleswig Holstein University Hospital; University of Kiel; Schleswig Holstein University Hospital	Gessert, N (corresponding author), Hamburg Univ Technol, Inst Med Technol, Hamburg, Germany.	nils.gessert@tuhh.de	Gessert, Nils/AAV-6412-2020; Keck, Tobias/AFH-4936-2022; Schlaefer, Alexander/HPE-1451-2023	Schlaefer, Alexander/0000-0001-9201-8854; Gessert, Nils/0000-0001-6325-5092				Aubreville M, 2019, INT J COMPUT ASS RAD, V14, P31, DOI 10.1007/s11548-018-1836-1; Aubreville M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-12320-8; Bengio Y., 2012, P ICML WORKSHOP UNSU, P17, DOI DOI 10.1109/IJCNN.2011.6033302; De Bree E, 2004, J SURG ONCOL, V86, P64, DOI 10.1002/jso.20049; Dromain C, 2008, ABDOM IMAGING, V33, P87, DOI 10.1007/s00261-007-9211-7; Ellebrecht DB, 2019, SURG ENDOSC, V33, P1811, DOI 10.1007/s00464-018-6457-9; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Franko J, 2012, J CLIN ONCOL, V30, P263, DOI 10.1200/JCO.2011.37.1039; Gessert N, 2019, BILDVERARBEITUNG MED; Gessert N, 2019, IEEE T MED IMAGING, V38, P426, DOI 10.1109/TMI.2018.2865659; Goceri Evgin, 2017, International Conferences on Computer Graphics, Visualization, Computer, Vision and image Processing 2017 and Big Data Analytics, Data Mining and Computational Intelligence 2017. Proceedings, P305; González-Moreno S, 2009, CANCER J, V15, P184, DOI 10.1097/PPO.0b013e3181a58ec3; He K., 2016, PROC IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010; Hong JS, 2017, IEEE ENG MED BIO, P2892, DOI 10.1109/EMBC.2017.8037461; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Iafrate F, 2012, ABDOM IMAGING, V37, P616, DOI 10.1007/s00261-011-9804-z; Ioffe S., 2015, P INT C MACH LEARN L, DOI [10.48550/arXiv.1502.03167, DOI 10.48550/ARXIV.1502.03167]; Ishigami S, 2014, WORLD J SURG ONCOL, V12, DOI 10.1186/1477-7819-12-350; Izadyyazdanabadi M, 2018, LECT NOTES COMPUT SC, V11071, P300, DOI 10.1007/978-3-030-00934-2_34; Izadyyazdanabadi M, 2018, FRONT ONCOL, V8, DOI 10.3389/fonc.2018.00240; Izadyyazdanabadi M, 2018, J VIS COMMUN IMAGE R, V54, P10, DOI 10.1016/j.jvcir.2018.04.004; Izadyyazdanabadi M, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254902; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Low R N, 2000, J Magn Reson Imaging, V12, P269, DOI 10.1002/1522-2586(200008)12:2<269::AID-JMRI9>3.3.CO;2-7; Nair G., 2010, ICML, P807, DOI 10.1123/jab.2016-0355; Niederer RL, 2007, BRIT J OPHTHALMOL, V91, P1165, DOI 10.1136/bjo.2006.112656; RAJADHYAKSHA M, 1995, J INVEST DERMATOL, V104, P946, DOI 10.1111/1523-1747.ep12606215; Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516044442, 10.1146/annurev-bioeng-071516-044442]; Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; SZEGEDY C, 2016, PROC CVPR IEEE, P2818, DOI [DOI 10.1109/CVPR.2016.308, 10.1109/CVPR.2016.308]; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302; Verwaal VJ, 2005, ANN SURG ONCOL, V12, P65, DOI 10.1007/s10434-004-1167-z; Wiltgen M, 2016, MICROSCOPY ANAL; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yosinski Jason, 2014, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1411.1792	39	21	25	1	26	SPRINGER HEIDELBERG	HEIDELBERG	TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY	1861-6410	1861-6429		INT J COMPUT ASS RAD	Int. J. Comput. Assist. Radiol. Surg.	NOV	2019	14	11			SI		1837	1845		10.1007/s11548-019-02004-1	http://dx.doi.org/10.1007/s11548-019-02004-1			9	Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging; Surgery	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Radiology, Nuclear Medicine & Medical Imaging; Surgery	JM2DC	31129859				2024-09-18	WOS:000496030000003
J	Luo, P; Ding, YL; Lei, XJ; Wu, FX				Luo, Ping; Ding, Yulian; Lei, Xiujuan; Wu, Fang-Xiang			deepDriver: Predicting Cancer Driver Genes Based on Somatic Mutations Using Deep Convolutional Neural Networks	FRONTIERS IN GENETICS			English	Article						deep learning; convolutional neural networks; driver gene prediction; cancer mutations; gene similarity network	BREAST-CANCER; PROLIFERATION; PROGRESSION; RNA	With the advances in high-throughput technologies, millions of somatic mutations have been reported in the past decade. Identifying driver genes with oncogenic mutations from these data is a critical and challenging problem. Many computational methods have been proposed to predict driver genes. Among them, machine learning-based methods usually train a classifier with representations that concatenate various types of features extracted from different kinds of data. Although successful, simply concatenating different types of features may not be the best way to fuse these data. We notice that a few types of data characterize the similarities of genes, to better integrate them with other data and improve the accuracy of driver gene prediction, in this study, a deep learning-based method (deepDriver) is proposed by performing convolution on mutation-based features of genes and their neighbors in the similarity networks. The method allows the convolutional neural network to learn information within mutation data and similarity networks simultaneously, which enhances the prediction of driver genes. deepDriver achieves AUC scores of 0.984 and 0.976 on breast cancer and colorectal cancer, which are superior to the competing algorithms. Further evaluations of the top 10 predictions also demonstrate that deepDriver is valuable for predicting new driver genes.	[Luo, Ping; Ding, Yulian; Wu, Fang-Xiang] Univ Saskatchewan, Div Biomed Engn, Saskatoon, SK, Canada; [Lei, Xiujuan] Shaanxi Normal Univ, Sch Comp Sci, Xian, Shaanxi, Peoples R China; [Wu, Fang-Xiang] Hainan Normal Univ, Sch Math & Stat, Haikou, Hainan, Peoples R China; [Wu, Fang-Xiang] Univ Saskatchewan, Dept Mech Engn, Saskatoon, SK, Canada; [Wu, Fang-Xiang] Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK, Canada	University of Saskatchewan; Shaanxi Normal University; Hainan Normal University; University of Saskatchewan; University of Saskatchewan	Wu, FX (corresponding author), Univ Saskatchewan, Div Biomed Engn, Saskatoon, SK, Canada.; Wu, FX (corresponding author), Hainan Normal Univ, Sch Math & Stat, Haikou, Hainan, Peoples R China.; Wu, FX (corresponding author), Univ Saskatchewan, Dept Mech Engn, Saskatoon, SK, Canada.; Wu, FX (corresponding author), Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK, Canada.	faw341@mail.usask.ca		Wu, Fang-Xiang/0000-0002-4593-9332	Natural Science and Engineering Research Council of Canada (NSERC); China Scholarship Council (CSC)	Natural Science and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); China Scholarship Council (CSC)(China Scholarship Council)	This work is supported in part by Natural Science and Engineering Research Council of Canada (NSERC) and China Scholarship Council (CSC).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; AlDubayan SH, 2018, AM J HUM GENET, V102, P401, DOI 10.1016/j.ajhg.2018.01.018; [Anonymous], IEEE ACM T COMPUTATI; [Anonymous], 2011, ARXIV11043889; [Anonymous], AACR ANN M 2017; Bailey MH, 2018, CELL, V173, P371, DOI [10.1016/j.cell.2018.02.060, 10.1016/j.cell.2018.07.034]; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Cao J, 2016, MEDICINE, V95, DOI 10.1097/MD.0000000000002496; Chatr-aryamontri A, 2017, NUCLEIC ACIDS RES, V45, pD369, DOI 10.1093/nar/gkw1102; Cheng FX, 2016, BRIEF BIOINFORM, V17, P642, DOI 10.1093/bib/bbv068; Choi MR, 2015, APMIS, V123, P65, DOI 10.1111/apm.12309; Chollet Francois, 2015, Keras; Cornen S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0081843; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Davoli T, 2013, CELL, V155, P948, DOI 10.1016/j.cell.2013.10.011; Dees ND, 2012, GENOME RES, V22, P1589, DOI 10.1101/gr.134635.111; Forbes SA, 2017, NUCLEIC ACIDS RES, V45, pD777, DOI 10.1093/nar/gkw1121; Friedrich T, 2016, EBIOMEDICINE, V8, P159, DOI 10.1016/j.ebiom.2016.05.003; Gala K, 2018, ONCOGENE, V37, P4692, DOI 10.1038/s41388-018-0273-5; Gonzalez-Perez A, 2013, NAT METHODS, V10, P1081, DOI [10.1038/NMETH.2642, 10.1038/nmeth.2642]; Gonzalez-Perez A, 2012, NUCLEIC ACIDS RES, V40, DOI 10.1093/nar/gks743; Grossman RL, 2016, NEW ENGL J MED, V375, P1109, DOI 10.1056/NEJMp1607591; Guo WF, 2018, BIOINFORMATICS, V34, P1893, DOI 10.1093/bioinformatics/bty006; Haeger SM, 2016, ONCOGENE, V35, P577, DOI 10.1038/onc.2015.112; He P, 2018, INT J ONCOL, V52, P1305, DOI 10.3892/ijo.2018.4284; Honda K, 2015, CELL BIOSCI, V5, DOI 10.1186/s13578-015-0031-0; Hou JP, 2014, GENOME MED, V6, DOI 10.1186/s13073-014-0056-8; Kechagioglou P, 2014, ANTICANCER RES, V34, P1387; Kikuchi S, 2005, CLIN CANCER RES, V11, P2954, DOI 10.1158/1078-0432.CCR-04-2206; Koo BH, 2007, INT J CANCER, V121, P1710, DOI 10.1002/ijc.22882; Kumar P, 2009, NAT PROTOC, V4, P1073, DOI 10.1038/nprot.2009.86; Lawrence MS, 2014, NATURE, V505, P495, DOI 10.1038/nature12912; Lee S, 2012, CANCER RES, V72, P4574, DOI 10.1158/0008-5472.CAN-12-0636; Li S, 2018, J CELL PHYSIOL, V233, P6679, DOI 10.1002/jcp.26325; Meriggi F, 2014, REV RECENT CLIN TRIA, V9, P8, DOI 10.2174/1568026614666140423121525; Mularoni L, 2016, GENOME BIOL, V17, DOI 10.1186/s13059-016-0994-0; Pécuchet N, 2017, ONCOTARGET, V8, P23831, DOI 10.18632/oncotarget.6379; Prasad TSK, 2009, NUCLEIC ACIDS RES, V37, pD767, DOI 10.1093/nar/gkn892; Prévostel C, 2017, EUR J CANCER, V86, P150, DOI 10.1016/j.ejca.2017.08.037; Rao RC, 2015, NAT REV CANCER, V15, P334, DOI 10.1038/nrc3929; Redig AJ, 2016, CLIN CANCER RES, V22, P3148, DOI 10.1158/1078-0432.CCR-15-2377; Reimand J, 2013, MOL SYST BIOL, V9, DOI 10.1038/msb.2012.68; Rubio-Perez C, 2015, CANCER CELL, V27, P382, DOI 10.1016/j.ccell.2015.02.007; Sanz-Pamplona R, 2015, CLIN CANCER RES, V21, P4709, DOI 10.1158/1078-0432.CCR-15-0159; Tamborero D, 2013, BIOINFORMATICS, V29, P2238, DOI 10.1093/bioinformatics/btt395; Tokheim CJ, 2016, P NATL ACAD SCI USA, V113, P14330, DOI 10.1073/pnas.1616440113; Uusitalo E, 2017, BRIT J CANCER, V116, P211, DOI 10.1038/bjc.2016.403; Velmurugan KR, 2017, ONCOGENE, V36, P6383, DOI 10.1038/onc.2017.256; Vogelstein B, 2013, SCIENCE, V339, P1546, DOI 10.1126/science.1235122; Wang HY, 2017, CELL PHYSIOL BIOCHEM, V41, P2221, DOI 10.1159/000475637; Wang XW, 2014, WORLD J GASTROENTERO, V20, P4178, DOI 10.3748/wjg.v20.i15.4178; Wong WC, 2011, BIOINFORMATICS, V27, P2147, DOI 10.1093/bioinformatics/btr357; Yates B, 2017, NUCLEIC ACIDS RES, V45, pD619, DOI 10.1093/nar/gkw1033; Yu J, 2016, ONCOGENE, V35, P187, DOI 10.1038/onc.2015.72; Yu YF, 2018, J CANCER, V9, P2953, DOI 10.7150/jca.25542; Zheng XB, 2017, CANCER RES, V77, DOI 10.1158/1538-7445.AM2017-4849	56	60	66	0	30	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND		1664-8021		FRONT GENET	Front. Genet.	JAN 29	2019	10								13	10.3389/fgene.2019.00013	http://dx.doi.org/10.3389/fgene.2019.00013			12	Genetics & Heredity	Science Citation Index Expanded (SCI-EXPANDED)	Genetics & Heredity	HJ5AS	30761181	Green Published, gold			2024-09-18	WOS:000457191100001
J	Jiang, JW; Xie, QR; Cheng, Z; Cai, JQ; Xia, T; Yang, H; Yang, B; Peng, H; Bai, XS; Yan, MQ; Li, X; Zhou, J; Huang, X; Wang, L; Long, HY; Wang, PX; Chu, YP; Zeng, FW; Zhang, XQ; Wang, GY; Zeng, FX				Jiang, Jiawei; Xie, Qianrong; Cheng, Zhuo; Cai, Jianqiang; Xia, Tian; Yang, Hang; Yang, Bo; Peng, Hui; Bai, Xuesong; Yan, Mingque; Li, Xue; Zhou, Jun; Huang, Xuan; Wang, Liang; Long, Haiyan; Wang, Pingxi; Chu, Yanpeng; Zeng, Fan-Wei; Zhang, Xiuqin; Wang, Guangyu; Zeng, Fanxin			AI based colorectal disease detection using real-time screening colonoscopy	PRECISION CLINICAL MEDICINE			English	Article						artificial intelligence (AI); colorectal disease; real-time colonoscopy	MISS RATE; TASK-FORCE; CANCER; POLYPS; CLASSIFICATION; POLYPECTOMY; DIAGNOSIS	Colonoscopy is an effective tool for early screening of colorectal diseases. However, the application of colonoscopy in distinguishing different intestinal diseases still faces great challenges of efficiency and accuracy. Here we constructed and evaluated a deep convolution neural network (CNN) model based on 117 055 images from 16 004 individuals, which achieved a high accuracy of 0.933 in the validation dataset in identifying patients with polyp, colitis, colorectal cancer (CRC) from normal. The proposed approach was further validated on multi-center real-time colonoscopy videos and images, which achieved accurate diagnostic performance on detecting colorectal diseases with high accuracy and precision to generalize across external validation datasets. The diagnostic performance of the model was further compared to the skilled endoscopists and the novices. In addition, our model has potential in diagnosis of adenomatous polyp and hyperplastic polyp with an area under the receiver operating characteristic curve of 0.975. Our proposed CNN models have potential in assisting clinicians in making clinical decisions with efficiency during application.	[Jiang, Jiawei; Xie, Qianrong; Yang, Hang; Li, Xue; Zhou, Jun; Wang, Pingxi; Chu, Yanpeng; Zeng, Fan-Wei; Zeng, Fanxin] Dazhou Cent Hosp, Dept Clin Res Ctr, Dazhou 635000, Peoples R China; [Jiang, Jiawei] Eidgenoss Tech Hsch Zurich, Dept Comp Sci, CH-999034 Zurich, Switzerland; [Cheng, Zhuo; Yang, Bo; Bai, Xuesong; Yan, Mingque] Dazhou Cent Hosp, Digest Endoscopy Ctr, Dazhou 635000, Peoples R China; [Cai, Jianqiang] Chinese Acad Med Sci & Peking Union Med Coll, Canc Hosp, Natl Canc Ctr, Dept Hepatobiliary Surg, Beijing 100730, Peoples R China; [Xia, Tian] Natl Ctr Biomed Anal, Beijing 100850, Peoples R China; [Peng, Hui] Huazhong Agr Univ, Coll Informat, Wuhan 430070, Peoples R China; [Huang, Xuan] Capital Med Univ, Beijing Chao Yang Hosp, Dept Ophthalmol, Med Res Ctr, Beijing 100020, Peoples R China; [Wang, Liang] Dazhou Cent Hosp, Informat Dept, Dazhou 635000, Peoples R China; [Long, Haiyan] Quxian Peoples Hosp, Digest Endoscopy Ctr, Dazhou 635000, Peoples R China; [Zhang, Xiuqin] Peking Univ, Inst Mol Med, Beijing 100871, Peoples R China; [Wang, Guangyu] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China; [Zeng, Fanxin] Sichuan Univ Arts & Sci, Dept Med, Dazhou 635000, Peoples R China	Chinese Academy of Medical Sciences - Peking Union Medical College; Peking Union Medical College; Cancer Institute & Hospital - CAMS; Huazhong Agricultural University; Capital Medical University; Peking University; Beijing University of Posts & Telecommunications; Sichuan University of Arts & Science	Zeng, FX (corresponding author), Dazhou Cent Hosp, Dept Clin Res Ctr, Dazhou 635000, Peoples R China.; Zhang, XQ (corresponding author), Peking Univ, Inst Mol Med, Beijing 100871, Peoples R China.; Wang, GY (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.; Zeng, FX (corresponding author), Sichuan Univ Arts & Sci, Dept Med, Dazhou 635000, Peoples R China.	zhangxq@pku.edu.cn; guangyu.wang24@gmail.com; zengfx@pku.edu.cn	Peng, Hui/ABB-8324-2021; Cai, Jun/GLS-4538-2022; Tan, Wei/KBB-7333-2024; Bai, Xuesong/AAO-4028-2020	Zhang, Xiuqin/0000-0001-8692-7146	National Natural Science Foundation of China [81902861, 32000485]; Xinglin Scholars" Scientific Research Project Fund of Chengdu University of Traditional Chinese Medicine [YYZX2019012]; Scientific Research Fund of Technology Bureau in Dazhou [17YYJC0004]; Key Research and Development Project Fund of Science and Technology Bureau in Dazhou, Sichuan Province [20ZDYF0001]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Xinglin Scholars" Scientific Research Project Fund of Chengdu University of Traditional Chinese Medicine; Scientific Research Fund of Technology Bureau in Dazhou; Key Research and Development Project Fund of Science and Technology Bureau in Dazhou, Sichuan Province	This study was funded by the National Natural Science Foundation of China (Grant No. 81902861 to F.Z. and 32000485 to X.H.), "Xinglin Scholars" Scientific Research Project Fund of Chengdu University of Traditional Chinese Medicine (Grant No. YYZX2019012 to F.Z.), the Scientific Research Fund of Technology Bureau in Dazhou (Grant No. 17YYJC0004 to F.-W.Z.), the Key Research and Development Project Fund of Science and Technology Bureau in Dazhou, Sichuan Province (Grant No. 20ZDYF0001 to F.-W.Z.). We express our deepest appreciation to J.Z, Y.C. for organizing the raw data and G.Y. for the revising the manuscript.	Abadi M, 2015, TENSORFLOW LARGE SCA; Chen PJ, 2018, GASTROENTEROLOGY, V154, P568, DOI 10.1053/j.gastro.2017.10.010; Citarda F, 2001, GUT, V48, P812, DOI 10.1136/gut.48.6.812; Das N, 2020, NEURAL NETWORKS, V128, P47, DOI 10.1016/j.neunet.2020.05.003; FEARON ER, 1990, CELL, V61, P759, DOI 10.1016/0092-8674(90)90186-I; Guo XD, 2019, MED PHYS, V46, P5666, DOI 10.1002/mp.13865; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heresbach D, 2008, ENDOSCOPY, V40, P284, DOI 10.1055/s-2007-995618; Horie Y, 2019, GASTROINTEST ENDOSC, V89, P25, DOI 10.1016/j.gie.2018.07.037; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Jiménez-Sánchez A, 2020, INT J COMPUT ASS RAD, V15, P847, DOI 10.1007/s11548-020-02150-x; Kermany DS, 2018, CELL, V172, P1122, DOI 10.1016/j.cell.2018.02.010; Li L, 2020, GASTRIC CANCER, V23, P126, DOI 10.1007/s10120-019-00992-2; Liang GB, 2020, COMPUT METH PROG BIO, V187, DOI 10.1016/j.cmpb.2019.06.023; Lu LQ, 2020, PEERJ, V8, DOI 10.7717/peerj.8668; Machida H, 2004, ENDOSCOPY, V36, P1094, DOI 10.1055/s-2004-826040; Maeda Y, 2019, GASTROINTEST ENDOSC, V89, P408, DOI 10.1016/j.gie.2018.09.024; Manna C, 2013, REPROD BIOMED ONLINE, V26, P42, DOI 10.1016/j.rbmo.2012.09.015; Mori Y, 2018, ANN INTERN MED, V169, P357, DOI 10.7326/M18-0249; MULLER B, 1986, ZBL CHIR, V111, P1091; MUTO T, 1975, CANCER, V36, P2251, DOI 10.1002/cncr.2820360944; Nowacki TM, 2015, DIGEST DIS SCI, V60, P492, DOI 10.1007/s10620-014-3373-2; Pasha SF, 2009, GASTROINTEST ENDOSC, V69, pAB363, DOI 10.1016/j.gie.2009.03.1079; Qi XF, 2019, MED IMAGE ANAL, V52, P185, DOI 10.1016/j.media.2018.12.006; Rajkomar A, 2018, NPJ DIGIT MED, V1, DOI 10.1038/s41746-018-0029-1; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Siu AL, 2016, ANN INTERN MED, V164, P279, DOI 10.7326/M15-2886; Than M, 2015, ANN GASTROENTEROL, V28, P94; Topol EJ, 2019, NAT MED, V25, P44, DOI 10.1038/s41591-018-0300-7; Urban G, 2018, GASTROENTEROLOGY, V155, P1069, DOI 10.1053/j.gastro.2018.06.037; van Rijn JC, 2006, AM J GASTROENTEROL, V101, P343, DOI 10.1111/j.1572-0241.2006.00390.x; VOGELSTEIN B, 1988, NEW ENGL J MED, V319, P525, DOI 10.1056/NEJM198809013190901; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Winawer SJ, 2006, CA-CANCER J CLIN, V56, P143, DOI 10.3322/canjclin.56.3.143; WINAWER SJ, 1993, NEW ENGL J MED, V329, P1977, DOI 10.1056/NEJM199312303292701; Yu LQ, 2017, IEEE T MED IMAGING, V36, P994, DOI 10.1109/TMI.2016.2642839; Zhang K, 2020, CELL, V181, P1423, DOI 10.1016/j.cell.2020.04.045; Zhang XH, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1457-4	38	3	3	0	2	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	2096-5303	2516-1571		PRECIS CLIN MED	Precis. Clin. Med.	JUN	2021	4	2					109	118		10.1093/pcmedi/pbab013	http://dx.doi.org/10.1093/pcmedi/pbab013		MAY 2021	10	Medicine, Research & Experimental	Emerging Sources Citation Index (ESCI)	Research & Experimental Medicine	WG6QN	35694157	Green Published, gold			2024-09-18	WOS:000707119200005
C	Guo, YB; Matuszewski, BJ		Tremeau, A; Farinella, GM; Braz, J		Guo, Yun Bo; Matuszewski, Bogdan J.			GIANA Polyp Segmentation with Fully Convolutional Dilation Neural Networks	VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4			English	Proceedings Paper	14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)	FEB 25-27, 2019	Prague, CZECH REPUBLIC			Fully Convolutional Neural Networks; Dilation Convolution; Polyp Segmentation; Video Colonoscopy; Segmentation Quality	VALIDATION	Polyp detection and segmentation in colonoscopy images plays an important role in early detection of colorectal cancer. The paper describes methodology adopted for the EndoVisSub2017/2018 Gastrointestinal Image ANAlysis - (GIANA) polyp segmentation sub-challenges. The developed segmentation algorithms are based on the fully convolutional neural network (FCNN) model. Two novel variants of the FCNN have been investigated, implemented and evaluated. The first one, combines the deep residual network and the dilation kernel layers within the fully convolutional network framework. The second proposed architecture is based on the U-net network augmented by the dilation kernels and "squeeze and extraction" units. The proposed architectures have been evaluated against the well-known FCN8 model. The paper describes the adopted evaluation metrics and presents the results on the GIANA dataset. The proposed methods produced competitive results, securing the first place for the SD and HD image segmentation tasks at the 2017 GIANA challenge and the second place for the SD images at the 2018 GIANA challenge.	[Guo, Yun Bo; Matuszewski, Bogdan J.] Univ Cent Lancashire, Sch Engn, Comp Vis & Machine Learning CVML, Res Grp, Preston, Lancs, England	University of Central Lancashire	Guo, YB (corresponding author), Univ Cent Lancashire, Sch Engn, Comp Vis & Machine Learning CVML, Res Grp, Preston, Lancs, England.			Guo, Yunbo/0000-0001-6804-4884				Alexandre LA, 2008, INT CONF BIOMED, P38, DOI 10.1109/BMEI.2008.246; [Anonymous], 2016, P INT C LEARN REPR; [Anonymous], 2018, ARXIV180200368; [Anonymous], 2015, PROC CVPR IEEE; [Anonymous], 2010, JMLR WORKSHOP C P; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Bernal J, 2013, IEEE ENG MED BIO, P7350, DOI 10.1109/EMBC.2013.6611256; Breier M., 2011, P 15 INT STUD C EL E, V2011; Breier M, 2011, PROC SPIE, V7963, DOI 10.1117/12.877986; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Gross S., 2009, Algorithmen - Systeme - Anwendungen Proceedings des Workshops, Bildverarbeitung fur die Medizin, P252, DOI 10.1007/978-3-540-93860-6_51; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hernandez-Garcia A., 2018, ARXIV180207042V3; Hu J, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1321, DOI 10.1145/3038912.3052685; Hwang S., 2007, IM PROC 2007 ICIP 20, V2; Iakovidis DK, 2005, COMP MED SY, P575, DOI 10.1109/CBMS.2005.6; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Q, 2017, INT J PHOTOENERGY, V2017, DOI [10.1109/TPWRS.2017.2712697, 10.1155/2017/8107073]; Park S., 2015, Polyp detection in colonoscopy videos using deeply-learned hierarchical features; Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189; Ribeiro E, 2016, COMP MED SY, P253, DOI 10.1109/CBMS.2016.39; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shen J. K., 2005, IM PROC 2005 ICIP 20, V3; Tajbakhsh N, 2014, I S BIOMED IMAGING, P97, DOI 10.1109/ISBI.2014.6867818; Tajbakhsh N, 2014, LECT NOTES COMPUT SC, V8674, P179, DOI 10.1007/978-3-319-10470-6_23; Tajbakhsh N, 2013, LECT NOTES COMPUT SC, V8198, P53, DOI 10.1007/978-3-642-41083-3_7; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Zhang L, 2017, COMM COM INF SC, V723, P707, DOI 10.1007/978-3-319-60964-5_62; Zhang RK, 2017, IEEE J BIOMED HEALTH, V21, P41, DOI 10.1109/JBHI.2016.2635662; Zhang Y, 2008, MEDIVIS 2008: FIFTH INTERNATIONAL CONFERENCE BIOMEDICAL VISUALIZATION - INFORMATION VISUALIZATION IN MEDICAL AND BIOMEDICAL INFORMATICS, PROCEEDINGS, P71, DOI 10.1109/MediVis.2008.12; Zhang Y, 2009, IEEE IMAGE PROC, P2993, DOI 10.1109/ICIP.2009.5414505	35	36	37	1	7	SCITEPRESS	SETUBAL	AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL			978-989-758-354-4				2019							632	641		10.5220/0007698806320641	http://dx.doi.org/10.5220/0007698806320641			10	Computer Science, Software Engineering; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Imaging Science & Photographic Technology	BP9RY		Green Accepted, Green Submitted, hybrid			2024-09-18	WOS:000570779500073
J	Luo, ZH; Feng, JJ; Cai, N; Wang, XD; Liao, JC; Li, QQ; Peng, FQ; Chen, CW				Luo, Zhihao; Feng, Jianjun; Cai, Nian; Wang, Xiaodan; Liao, Jiacheng; Li, Quanqing; Peng, Fuqiang; Chen, Chuanwen			Relation extraction for colorectal cancer via deep learning with entity-aware feature orthogonal decomposition	EXPERT SYSTEMS WITH APPLICATIONS			English	Article						Colorectal cancer; Pathological reports; Relation extraction; Two-stream feature extraction; Feature orthogonal decomposition; Stage-wise cross entropy		Relation extraction is significant for text structuring of colorectal cancer (CRC) pathological reports to facilitate doctors' disease diagnoses. Although many relation extraction methods have been extensively studied for various natural language processing applications, they cannot be well transferred to be applied for CRC pathological reports since CRC pathological reports have some unique characteristics. To this end, a deep learning framework is designed in this paper to extract entity relations in CRC pathological reports, which is based on an encoder-decoder architecture with entity-aware feature orthogonal decomposition. Specifically, to effectively extract semantic features of long and short entities, a two-stream encoder is designed based on an edge-aware convolutional neural network and a dimension-aware dilated convolution residual network. To alleviate the influence of the blending of subject-object features, entity-aware feature orthogonal decomposition is designed to decompose the extracted semantic features into three types, i.e. subject features, object features and subject-object shared features. A stage-wise cross entropy loss is proposed to well train the network. Comparison experiments indicated that our designed network performs well on CRC pathological texts with the performance of 92.3% F1 score, 93.1% Precision, and 91.5% Recall, outperforming the existing relation extraction models.	[Luo, Zhihao; Feng, Jianjun; Cai, Nian] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China; [Liao, Jiacheng] Foshan Talkiin Technol Co LTD, 5th Floor,Bldg 1,Wanbang Commercial Plaza,Daliang, Foshan, Peoples R China; [Wang, Xiaodan; Li, Quanqing; Peng, Fuqiang; Chen, Chuanwen] Guangzhou Huayinkang Med Grp Co Ltd, Guangzhou 510006, Peoples R China	Guangdong University of Technology	Cai, N (corresponding author), Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.	2112203076@mail2.gdut.edu.cn; 3122002009@mail2.gdut.edu.cn; cainian@gdut.edu.cn; wangxiaodan@huayinlab.com; liaojiacheng@talkiin.cn; liquanqing@huayinlab.com; pengfuqiang@huayinlab.com; liaojiacheng@talkiin.cn			The 308-Program for Clinical Research of Sun Yat-Sen University Cancer Center [308-2019-01-01]; National Natural Science Foundation of China [82172019]	The 308-Program for Clinical Research of Sun Yat-Sen University Cancer Center; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by 308-Program for Clinical Research of Sun Yat-Sen University Cancer Center [Grant No. 308-2019-01-01] and the National Natural Science Foundation of China [Grant No. 82172019] .	Araujo S E, 2001, Rev Hosp Clin Fac Med Sao Paulo, V56, P25; Barresi V., 2015, Histological grading in colorectal cancer: new insights and perspectives; Bhasuran B, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200699; Bovee J., 2020, Soft tissue and bone tumours; Bundschus M, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-207; Chang YC, 2013, J BIOMED INFORM, V46, pS54, DOI 10.1016/j.jbi.2013.09.007; Chen KBY, 2021, CURR ONCOL, V28, P5356, DOI 10.3390/curroncol28060447; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Eberts M, 2020, FRONT ARTIF INTEL AP, V325, P2006, DOI 10.3233/FAIA200321; Egner JR., 2010, JAMA-J AM MED ASSOC, V304, P1726, DOI 10.1001/jama.2010.1525; El Din KS, 2020, BMC CANCER, V20, DOI 10.1186/s12885-020-06766-9; Hanauer DA, 2015, J AM MED INFORM ASSN, V22, pE219, DOI 10.1093/jamia/ocu036; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Hu Q, 2021, COMPUT ELECTR ENG, V91, DOI 10.1016/j.compeleceng.2021.107019; Jelier R, 2005, BIOINFORMATICS, V21, P2049, DOI 10.1093/bioinformatics/bti268; Sahu SK, 2016, Arxiv, DOI arXiv:1606.09370; Liu RB, 2021, Arxiv, DOI arXiv:2108.07886; Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101; Mahulae PS, 2023, J PUBLIC HEALTH-UK, V46, pe226, DOI 10.1093/pubmed/fdad223; Miao Q., 2012, P 26 PAC AS C LANG I, P99; Murphy CC, 2024, NAT REV GASTRO HEPAT, V21, P25, DOI 10.1038/s41575-023-00841-9; Neilson LJ, 2015, FRONTLINE GASTROENTE, V6, P117, DOI 10.1136/flgastro-2015-100565; Pan HF, 2022, BMC PUBLIC HEALTH, V22, DOI 10.1186/s12889-022-14274-7; Ren FL, 2022, WSDM'22: PROCEEDINGS OF THE FIFTEENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P824, DOI 10.1145/3488560.3498409; Rex DK, 2019, CLIN GASTROENTEROL H, V17, P1428, DOI 10.1016/j.cgh.2018.09.040; Shang YM, 2022, AAAI CONF ARTIF INTE, P11285; Siegel Rebecca L, 2024, CA Cancer J Clin, V74, P12, DOI 10.3322/caac.21820; Sun WY, 2013, J AM MED INFORM ASSN, V20, P806, DOI 10.1136/amiajnl-2013-001628; Sun ZJ, 2023, IEEE ACCESS, V11, P127422, DOI 10.1109/ACCESS.2023.3331504; Tang RX, 2023, EXPERT SYST APPL, V229, DOI 10.1016/j.eswa.2023.120441; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wan Lu, 2023, 2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI), P1212, DOI 10.1109/PRAI59366.2023.10331969; Wang Y., 2020, arXiv; Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041; Wei ZP, 2020, Arxiv, DOI arXiv:1909.03227; [吴宗友 Wu Zongyou], 2021, [计算机研究与发展, Journal of Computer Research and Development], V58, P513; Yin MW, 2019, J BIOMED INFORM, V98, DOI 10.1016/j.jbi.2019.103289; Zaheer M., 2020, Adv. Neural Inf. Process. Syst, V33, P17283, DOI DOI 10.5555/3495724.3497174; Zhang YL, 2020, IEEE ACCESS, V8, P95947, DOI 10.1109/ACCESS.2020.2995739; Zhang ZJ, 2023, COMPLEX INTELL SYST, V9, P5235, DOI 10.1007/s40747-023-01004-8; Zhang Z, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13074447; Zhao K, 2021, KNOWL-BASED SYST, V219, DOI 10.1016/j.knosys.2021.106888; Zhao XY, 2023, IEEE T KNOWL DATA EN, V35, P7953, DOI 10.1109/TKDE.2022.3161584; Zheng HY, 2021, Arxiv, DOI arXiv:2106.09895; Zheng SC, 2017, Arxiv, DOI [arXiv:1706.05075, DOI 10.48550/ARXIV.1706.05075]	45	0	0	2	2	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0957-4174	1873-6793		EXPERT SYST APPL	Expert Syst. Appl.	DEC 15	2024	258								125188	10.1016/j.eswa.2024.125188	http://dx.doi.org/10.1016/j.eswa.2024.125188			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Operations Research & Management Science	E3N7V					2024-09-18	WOS:001302114700001
J	Schuler, JPS; Romani Also, S; Puig, D; Rashwan, H; Abdel-Nasser, M				Schwarz Schuler, Joao Paulo; Romani Also, Santiago; Puig, Domenec; Rashwan, Hatem; Abdel-Nasser, Mohamed			An Enhanced Scheme for Reducing the Complexity of Pointwise Convolutions in CNNs for Image Classification Based on Interleaved Grouped Filters without Divisibility Constraints	ENTROPY			English	Article						EfficientNet; deep learning; computer vision; image classification; convolutional neural network; DCNN; grouped convolution; pointwise convolution; data analysis; network optimization; parameter reduction; parallel branches; channel interleaving	NEURAL-NETWORK	In image classification with Deep Convolutional Neural Networks (DCNNs), the number of parameters in pointwise convolutions rapidly grows due to the multiplication of the number of filters by the number of input channels that come from the previous layer. Existing studies demonstrated that a subnetwork can replace pointwise convolutional layers with significantly fewer parameters and fewer floating-point computations, while maintaining the learning capacity. In this paper, we propose an improved scheme for reducing the complexity of pointwise convolutions in DCNNs for image classification based on interleaved grouped filters without divisibility constraints. The proposed scheme utilizes grouped pointwise convolutions, in which each group processes a fraction of the input channels. It requires a number of channels per group as a hyperparameter Ch. The subnetwork of the proposed scheme contains two consecutive convolutional layers K and L, connected by an interleaving layer in the middle, and summed at the end. The number of groups of filters and filters per group for layers K and L is determined by exact divisions of the original number of input channels and filters by Ch. If the divisions were not exact, the original layer could not be substituted. In this paper, we refine the previous algorithm so that input channels are replicated and groups can have different numbers of filters to cope with non exact divisibility situations. Thus, the proposed scheme further reduces the number of floating-point computations (11%) and trainable parameters (10%) achieved by the previous method. We tested our optimization on an EfficientNet-B0 as a baseline architecture and made classification tests on the CIFAR-10, Colorectal Cancer Histology, and Malaria datasets. For each dataset, our optimization achieves a saving of 76%, 89%, and 91% of the number of trainable parameters of EfficientNet-B0, while keeping its test classification accuracy.	[Schwarz Schuler, Joao Paulo; Romani Also, Santiago; Puig, Domenec; Rashwan, Hatem; Abdel-Nasser, Mohamed] Univ Rovira & Virgili, Dept Engn Informat & Matemat, Tarragona 43007, Spain; [Abdel-Nasser, Mohamed] Aswan Univ, Elect Engn Dept, Elect & Commun Engn Sect, Aswan 81528, Egypt	Universitat Rovira i Virgili; Egyptian Knowledge Bank (EKB); Aswan University	Schuler, JPS (corresponding author), Univ Rovira & Virgili, Dept Engn Informat & Matemat, Tarragona 43007, Spain.	joaopaulo.schwarz@estudiants.urv.cat	Romani, Santiago/AAA-3957-2019; Rashwan, Hatem/P-5760-2016; Abdel-Nasser, Mohamed/H-4321-2015	Abdel-Nasser, Mohamed/0000-0002-1074-2441; Puig, Domenec/0000-0002-0562-4205; Romani, Santiago/0000-0001-6673-9615; Rashwan, Hatem A./0000-0001-5421-1637	Spanish Government [PID2019-105789RB-I00]	Spanish Government(Spanish Government)	The Spanish Government partly supported this research through Project PID2019-105789RB-I00.	[Anonymous], 2016, TensorFlow: large-scale machine learning on heterogeneous distributed systems; Baykal C., 2019, P INT C LEARNING REP; Cheng Y, 2015, IEEE I CONF COMP VIS, P2857, DOI 10.1109/ICCV.2015.327; Chollet, KERAS; Denil M, 2013, NIPS; Glorot X, 2010, P 13 INT C ART INT S, V9, P249, DOI DOI 10.1109/LGRS.2016.2565705; Han S., 2016, P INT C LEARNING REP; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ioannou Y, 2017, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2017.633; Kahatapitiya K, 2021, IEEE WINT CONF APPL, P1409, DOI 10.1109/WACV48630.2021.00145; Kather J.N., 2016, Collection of textures in colorectal cancer histology; Krizhevsky A., 2009, Learning multiple layers of features from tiny images, V1, P4; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; LeCun Y, 1989, NEURIPS, V2; Liebenwein L., 2020, ICLR; Liebenwein L., 2021, Proceedings of Machine Learning and Systems, V3, P93, DOI 0.48550/arXiv.2103.030142,3,6; Lin M., 2014, ARXIV; Rajaraman S, 2018, PEERJ, V6, DOI 10.7717/peerj.4568; Ramachandran P, 2017, ARXIV; REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schuler J., 2021, GROUPED POINTWISE CO, P383, DOI [10.3233/FAIA210158, DOI 10.3233/FAIA210158]; Schuler J.P.S., 2022, MENDEL, V28, P23; Schuler J.P.S., 2021, K CAI NEURAL API; Shahbazi A, 2020, J ASIAN EARTH SCI, V202, DOI 10.1016/j.jseaes.2020.104541; Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58; Sun K., 2018, P BMVC, P1; Szegedy C., 2015, IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594, 10.1109/cvpr.2015.7298594, 10.48550/arXiv.1409.4842]; Tan MX, 2019, PR MACH LEARN RES, V97; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Wang XJ, 2019, PROC CVPR IEEE, P9041, DOI 10.1109/CVPR.2019.00926; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang WZ, 2019, IET IMAGE PROCESS, V13, P779, DOI 10.1049/iet-ipr.2018.6191; Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang T., 2017, ARXIV; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhuang ZW, 2018, ADV NEUR IN, V31	42	4	4	0	4	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		1099-4300		ENTROPY-SWITZ	Entropy	SEP	2022	24	9							1264	10.3390/e24091264	http://dx.doi.org/10.3390/e24091264			15	Physics, Multidisciplinary	Science Citation Index Expanded (SCI-EXPANDED)	Physics	4S5ST	36141151	gold, Green Published			2024-09-18	WOS:000857501400001
J	Hwang, M; Wang, D; Jiang, WC; Pan, X; Fu, DL; Hwang, KS; Ding, KF				Hwang, Maxwell; Wang, Da; Jiang, Wei-Cheng; Pan, Xiang; Fu, Dongliang; Hwang, Kao-Shing; Ding, Kefeng			An Adaptive Regularization Approach to Colonoscopic Polyp Detection Using a Cascaded Structure of Encoder-Decoders	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS			English	Article						Convolution neural networks; Encoder-decoder networks; Fuzzy logic	COLORECTAL-CANCER; MISS RATE; PARTICIPATION; PREVENTION; INCREASES; VISION	This research aims to segment colonoscopic images by automatically extracting polyp features by exploiting the strengths of convolution neural networks (CNN). The proposed model employs deep learning and adaptive regularization techniques. The model is structurally composed of two cascaded encoder-decoder networks, each of which is constructed by four CNN layers and two full connection layers. The front model is built on backpropagation learning for segmenting a colonoscopic polyp image. The output images from the precedent hetero-encoder are regarded as corrupted labeled images, especially during the time period close to the end of learning, and are selectively fed into the successive auto-encoder for denoising learning to enhance its discriminative power and relieve the problem of a lack of labeled data for medical image tasks. The performance of the proposed model can be further improved by a simple fuzzy logic approach setting the regularization parameter in the loss function. The proposed method utilizes features learned from some open medical datasets and our own collected dataset. The performance of the proposed architecture is compared with a state-of-the-art network. The evaluation shows the performances of the proposed method are consistent across all the datasets and often outperform the state-of-art model.	[Hwang, Maxwell; Wang, Da; Pan, Xiang; Fu, Dongliang; Ding, Kefeng] Zhejiang Univ, Sch Med, Affiliated Hosp 2, Dept Colorectal Surg, Hangzhou, Zhejiang, Peoples R China; [Hwang, Maxwell; Wang, Da; Pan, Xiang; Fu, Dongliang; Ding, Kefeng] Zhejiang Univ, Affiliated Hosp 2, China Natl Minist Educ, Canc Inst,Key Lab Canc Prevent & Intervent, Hangzhou, Zhejiang, Peoples R China; [Hwang, Maxwell; Wang, Da; Pan, Xiang; Fu, Dongliang; Ding, Kefeng] Zhejiang Univ, Affiliated Hosp 2, Key Lab Mol Biol Med Sci, Sch Med, Hangzhou, Zhejiang, Peoples R China; [Jiang, Wei-Cheng] Tunghai Univ, Dept Elect Engn, Taichung, Taiwan; [Hwang, Kao-Shing] Natl Sun Yat Sen Univ, Dept Elect Engn, Kaohsiung, Taiwan	Zhejiang University; Zhejiang University; Zhejiang University; Tunghai University; National Sun Yat Sen University	Ding, KF (corresponding author), Zhejiang Univ, Sch Med, Affiliated Hosp 2, Dept Colorectal Surg, Hangzhou, Zhejiang, Peoples R China.; Ding, KF (corresponding author), Zhejiang Univ, Affiliated Hosp 2, China Natl Minist Educ, Canc Inst,Key Lab Canc Prevent & Intervent, Hangzhou, Zhejiang, Peoples R China.; Ding, KF (corresponding author), Zhejiang Univ, Affiliated Hosp 2, Key Lab Mol Biol Med Sci, Sch Med, Hangzhou, Zhejiang, Peoples R China.	himax26@126.com; wangda0618@zju.edu.cn; jiangwc@thu.edu.tw; panx@zju.edu.cn; 3120102932@zju.edu.cn; hwang@ccu.edu.tw; dingkefeng@zju.edu.cn	Hwang, Kao-Shing/AAD-2644-2020		Key Technology Research and Development Program of Zhejiang Province [2017C03017]; National Natural Science Foundation of China [81672916, LQ17H160008]; National Key R&D Program of China [2017YFC0908200]	Key Technology Research and Development Program of Zhejiang Province; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China	This work was supported in part by the Key Technology Research and Development Program of Zhejiang Province (2017C03017), the National Natural Science Foundation of China (81672916) and (LQ17H160008), and the National Key R&D Program of China (2017YFC0908200).	Ahn SB, 2012, GUT LIVER, V6, P64, DOI 10.5009/gnl.2012.6.1.64; Aslanian HR, 2013, AM J GASTROENTEROL, V108, P166, DOI 10.1038/ajg.2012.237; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bae SH, 2015, IEEE T MED IMAGING, V34, P2379, DOI 10.1109/TMI.2015.2434398; Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042; Brenner H, 2014, GASTROENTEROLOGY, V146, P709, DOI 10.1053/j.gastro.2013.09.001; Buchner AM, 2011, GASTROINTEST ENDOSC, V73, P1223, DOI 10.1016/j.gie.2011.01.060; Chatfield K., 2014, RETURN DEVIL DETAILS; Coimbra MT, 2006, IEEE T CIRC SYST VID, V16, P628, DOI 10.1109/TCSVT.2006.873158; El Khatib A, 2015, IEEE ENG MED BIO, P2669, DOI 10.1109/EMBC.2015.7318941; Iwahori Y, 2015, PROCEDIA COMPUT SCI, V60, P730, DOI 10.1016/j.procs.2015.08.226; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee CK, 2011, GASTROINTEST ENDOSC, V74, P1094, DOI 10.1016/j.gie.2011.06.033; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Lin HF, 2019, INT J FUZZY SYST, V21, P1026, DOI 10.1007/s40815-018-00604-8; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mahmud N, 2015, GASTROENTEROL REP, V3, P179, DOI 10.1093/gastro/gov027; Pan W, 2019, INT J FUZZY SYST, V21, P95, DOI 10.1007/s40815-018-0535-y; Ranzato M., 2007, 2007 IEEE C COMP VIS, P1, DOI [DOI 10.1109/CVPR.2007.383157, 10.1109/CVPR.2007.383157]; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roth HR, 2015, I S BIOMED IMAGING, P101, DOI 10.1109/ISBI.2015.7163826; Shin Y, 2017, IEEE ENG MED BIO, P3277, DOI 10.1109/EMBC.2017.8037556; Silva J, 2013, IEEE ENG MED BIO, P5711, DOI 10.1109/EMBC.2013.6610847; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302; Urban G, 2018, GASTROENTEROLOGY, V155, P1069, DOI 10.1053/j.gastro.2018.06.037; WINAWER SJ, 1993, NEW ENGL J MED, V329, P1977, DOI 10.1056/NEJM199312303292701; Yuan YX, 2016, IEEE T AUTOM SCI ENG, V13, P529, DOI 10.1109/TASE.2015.2395429; Zauber AG, 2012, NEW ENGL J MED, V366, P687, DOI 10.1056/NEJMoa1100370; Zhang RK, 2017, IEEE J BIOMED HEALTH, V21, P41, DOI 10.1109/JBHI.2016.2635662	32	5	5	0	12	SPRINGER HEIDELBERG	HEIDELBERG	TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY	1562-2479	2199-3211		INT J FUZZY SYST	Int. J. Fuzzy Syst.	OCT	2019	21	7					2091	2101		10.1007/s40815-019-00694-y	http://dx.doi.org/10.1007/s40815-019-00694-y			11	Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Information Systems	Science Citation Index Expanded (SCI-EXPANDED)	Automation & Control Systems; Computer Science	JI6HB					2024-09-18	WOS:000493567300009
J	Li, X; Li, W; Tao, R				Li, Xiang; Li, Wei; Tao, Ran			Staged Detection-Identification Framework for Cell Nuclei in Histopathology Images	IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT			English	Article						Feature extraction; Detectors; Cancer; Morphology; Sensitivity; Convolution; Shape; Deep convolutional neural network; histopathology image; nucleus detection; pattern recognition	NEURAL-NETWORKS; CLASSIFICATION; SEGMENTATION; SYSTEM	Histopathology image is an important basis for pathologists to evaluate disease at the cellular level and colon cancer tissue sections usually contain many different types of nuclei, which should be automatically detected and identified. However, the detection and the identification of cell nuclei are challenging tasks due to the complex tissue structure and the diversity of nuclear morphology. In this paper, a staged detection-identification framework is proposed for cell nuclei in colon cancer histopathology images. First, nuclei positions are detected by a position of interest network, which encodes context-aware representation on input image and decodes features on proximity map. Meanwhile, a cascade residual fusion block is presented to enhance the detection performance during the decoding process. Second, a multicropping network is developed to identify the detected cell nuclei. For reducing the impact of uncertainty, a multicropping module is designed for effectively capturing contextual feature contents around the center of a nucleus. The proposed detection-identification framework is evaluated on an available colorectal adenocarcinoma images data set, which has 100 images including more than 20 000 marked nuclei. Compared with state-of-the-art methods, the proposed approach demonstrates excellent performance with better prediction scores.	[Li, Xiang; Li, Wei] Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China; [Tao, Ran] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China; [Tao, Ran] Beijing Key Lab Fract Signals & Syst, Beijing 100081, Peoples R China	Beijing University of Chemical Technology; Beijing Institute of Technology	Li, W (corresponding author), Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China.	liwei089@ieee.org; rantao@bit.edu.cn	LI, WEI/ABD-5001-2021		Beijing Natural Science Foundation [4172043]; Beijing Nova Program [Z171100001117050]; Research Fund for Basic Researches in Central Universities [PYBZ1831]	Beijing Natural Science Foundation(Beijing Natural Science Foundation); Beijing Nova Program(Beijing Municipal Science & Technology Commission); Research Fund for Basic Researches in Central Universities	This work was supported in part by the Beijing Natural Science Foundation under Grant 4172043, in part by the Beijing Nova Program under Grant Z171100001117050, and in part by the Research Fund for Basic Researches in Central Universities under Grant PYBZ1831.	[Anonymous], SPIE MED IMAG; [Anonymous], 2014, J. Cancer Ther., DOI DOI 10.4236/jct.2014.513127; [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123; Basavanhally Ajay, 2011, J Pathol Inform, V2, pS1, DOI 10.4103/2153-3539.92027; Bouchard J, 2017, IEEE T INSTRUM MEAS, V66, P2505, DOI 10.1109/TIM.2017.2666458; Chen ZJ, 2018, IEEE SENS J, V18, P6360, DOI 10.1109/JSEN.2018.2844252; Ciresan DC, 2012, IEEE IJCNN; Cosatto E, 2008, INT C PATT RECOG, P672; De Vito L, 2017, IEEE T INSTRUM MEAS, V66, P2502, DOI 10.1109/TIM.2017.2733318; Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Z, 2013, IEEE T INSTRUM MEAS, V62, P889, DOI 10.1109/TIM.2013.2246917; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Irshad Humayun, 2014, IEEE Rev Biomed Eng, V7, P97, DOI 10.1109/RBME.2013.2295804; King DB, 2015, ACS SYM SER, V1214, P1; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar Rajesh, 2015, J Med Eng, V2015, P457906, DOI 10.1155/2015/457906; Kuse Manohar, 2011, J Pathol Inform, V2, pS2, DOI 10.4103/2153-3539.92028; Li MG, 2008, IEEE T INSTRUM MEAS, V57, P1221, DOI 10.1109/TIM.2007.915443; Li YX, 2017, IEEE T MED IMAGING, V36, P1561, DOI 10.1109/TMI.2017.2672702; Madabhushi A, 2016, MED IMAGE ANAL, V33, P170, DOI 10.1016/j.media.2016.06.037; Malon Christopher D, 2013, J Pathol Inform, V4, P9, DOI 10.4103/2153-3539.112694; Mualla F, 2014, I S BIOMED IMAGING, P927, DOI 10.1109/ISBI.2014.6868023; Munirah MA, 2011, ROM J MORPHOL EMBRYO, V52, P669; Murthy V, 2017, IEEE WINT CONF APPL, P834, DOI 10.1109/WACV.2017.98; Nguyen Kien, 2011, J Pathol Inform, V2, pS3, DOI 10.4103/2153-3539.92030; Osowski S, 2009, IEEE T INSTRUM MEAS, V58, P2159, DOI 10.1109/TIM.2008.2006726; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Saha M, 2018, IEEE T IMAGE PROCESS, V27, P2189, DOI 10.1109/TIP.2018.2795742; Sakaki K, 2013, IEEE T BIO-MED ENG, V60, P3113, DOI 10.1109/TBME.2013.2268387; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shrestha S, 2015, IEEE T INSTRUM MEAS, V64, P2453, DOI 10.1109/TIM.2015.2415013; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Sirinukunwattana K, 2016, IEEE T MED IMAGING, V35, P1196, DOI 10.1109/TMI.2016.2525803; Sirinukunwattana K, 2015, PROC SPIE, V9420, DOI 10.1117/12.2082010; Su H, 2014, IEEE ACM T COMPUT BI, V11, P714, DOI 10.1109/TCBB.2013.151; Su TY, 2018, IEEE SENS J, V18, P6857, DOI 10.1109/JSEN.2018.2850940; Targ S., 2016, RESNET RESNET GEN RE; Wang HB, 2014, PROC SPIE, V9041, DOI 10.1117/12.2043902; Xie YP, 2015, LECT NOTES COMPUT SC, V9351, P358, DOI 10.1007/978-3-319-24574-4_43; Xiong XC, 2005, Sixth International Conference on Software Engineerng, Artificial Intelligence, Networking and Parallel/Distributed Computing and First AICS International Workshop on Self-Assembling Wireless Networks, Proceedings, P82; Xu J, 2016, IEEE T MED IMAGING, V35, P119, DOI 10.1109/TMI.2015.2458702; Yu YH, 2017, INFORMATION, V8, DOI 10.3390/info8030091; Zhu RK, 2017, IEEE INT C BIOINF BI, P51, DOI [10.1109/BIBE.2017.00016, 10.1109/BIBE.2017.00-79]	45	13	13	0	19	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	0018-9456	1557-9662		IEEE T INSTRUM MEAS	IEEE Trans. Instrum. Meas.	JAN	2020	69	1					183	193		10.1109/TIM.2019.2894044	http://dx.doi.org/10.1109/TIM.2019.2894044			11	Engineering, Electrical & Electronic; Instruments & Instrumentation	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Instruments & Instrumentation	JW0ZD					2024-09-18	WOS:000502787500020
C	Wang, A; Wu, M; Qi, H; Shi, H; Chen, JH; Chen, YR; Luo, XB			IEEE	Wang, Ao; Wu, Ming; Qi, Hao; Shi, Hong; Chen, Jianhua; Chen, Yinran; Luo, Xiongbiao			PYRAMID TRANSFORMER DRIVEN MULTIBRANCH FUSION FOR POLYP SEGMENTATION IN COLONOSCOPIC VIDEO IMAGES	2023 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP	IEEE International Conference on Image Processing ICIP		English	Proceedings Paper	30th IEEE International Conference on Image Processing (ICIP)	OCT 08-11, 2023	Kuala Lumpur, MALAYSIA	IEEE, Inst Elect & Elect Engineers, Signal Proc Soc		Polyp segmentation; vision transformer; convolutional neural networks; colorectal cancer; colonoscopy		Colonoscopic polyp segmentation is essential and valuable to early diagnosis and treatment of colorectal cancer. It remains challenging to accurately extract these polyps due to their small sizes, irregular shapes, image artifacts, and illumination variations. This work proposes a new encoder-decoder architecture called pyramid transformer driven multibranch fusion to precisely segment different types of colorectal polyps during colonoscopy. Specifically, our architecture employs a simple, convolution-free pyramid transformer as its encoder that is a flexible and powerful feature extractor. Next, a multibranch fusion decoder is employed to reserve the detailed appearance information and fuse semantic global cues, which can deal with blurred polyp edges caused by nonuniform illumination and the shaky colonoscope. Additionally, a hybrid spatial-frequency loss function is introduced for accurate training. We evaluate our proposed architecture on colonoscopic polyp images with four types of polyps with different pathological features, with the experimental results showing that our architecture significantly outperforms other deep learning models. Particularly, our method improves the average dice similarity and intersection over union to 90.7% and 0.848, respectively.	[Wang, Ao; Wu, Ming; Qi, Hao; Chen, Yinran; Luo, Xiongbiao] Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China; [Luo, Xiongbiao] Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen 361102, Peoples R China; [Shi, Hong; Chen, Jianhua] Fujian Med Univ, Fujian Canc Hosp, Canc Hosp, Fuzhou 350014, Peoples R China	Xiamen University; Xiamen University; Fujian Medical University	Luo, XB (corresponding author), Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China.; Luo, XB (corresponding author), Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen 361102, Peoples R China.; Shi, H (corresponding author), Fujian Med Univ, Fujian Canc Hosp, Canc Hosp, Fuzhou 350014, Peoples R China.		Luo, Xiong/P-4343-2016		National Natural Science Foundation of China [61971367]; Natural Science Foundation of Fujian Province of China [2020J01004]; Fujian Provincial Technology Innovation Joint Funds [2019Y9091]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Fujian Province of China(Natural Science Foundation of Fujian Province); Fujian Provincial Technology Innovation Joint Funds	This work was supported partly by the National Natural Science Foundation of China under Grant 61971367, the Natural Science Foundation of Fujian Province of China under Grant 2020J01004, and the Fujian Provincial Technology Innovation Joint Funds under Grant 2019Y9091.	[Anonymous], 2021, P 29 ACM INT C MULT, DOI DOI 10.1109/SNPDWINTER52325.2021.00044; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Guo XQ, 2021, IEEE T MED IMAGING, V40, P1134, DOI 10.1109/TMI.2020.3046843; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsiang Huang C., 2021, ARXIV210107172; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Jiang LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13899, DOI 10.1109/ICCV48922.2021.01366; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Van Leemput K, 2001, IEEE T MED IMAGING, V20, P677, DOI 10.1109/42.938237; Vaswani A, 2017, ADV NEUR IN, V30; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1	15	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-7281-9835-4	IEEE IMAGE PROC			2023							2350	2354		10.1109/ICIP49359.2023.10223054	http://dx.doi.org/10.1109/ICIP49359.2023.10223054			5	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BW1IU					2024-09-18	WOS:001106821002088
J	Fu, JH; Lin, SL; Zhou, PH; Guo, Y; Wang, YY				Fu, Junhu; Lin, Shengli; Zhou, Pinghong; Guo, Yi; Wang, Yuanyuan			M<SUP>3</SUP>ResU-Net: a deep residual network for multi-center colorectal polyp segmentation based on multi-scale learning and attention mechanism	PHYSICS IN MEDICINE AND BIOLOGY			English	Article						colorectal polyp segmentation; multi-center datasets; attention mechanism; multi-scale learning	CANCER STATISTICS; VALIDATION	Colorectal polyps are considered as an important precursor of colorectal cancer (CRC) in clinical diagnosis. A network automatically and accurately segmenting polyps can recognize, locate and finally help to remove polyps, greatly reducing the misdiagnosis rate. Although many neural networks for polyp segmentation have been proposed, there still exist some difficulties including the diversity of image backgrounds, the jelly effect, and the various shapes and sizes of different polyps. These factors lead to the segmentation accuracy remaining to be improved. In this paper, we propose M(3)ResU-Net including multi-scale learning and attention mechanisms, aiming to segment multi-center colorectal polyps. First, we implement the contrast limited adaptive histogram equalization (CLAHE) and data augmentation for multi-center data. Then, channel and spatial attention mechanisms are introduced to focus on polyp features and suppress interference features. Finally, in order to balance small target segmentation and the acquisition of global information, multi-scale learning with dilated convolutions is employed. We compared other five polyp segmentation methods on three publicly available datasets. In single-center experiments, M(3)ResU-Net reaches a Dice similarity coefficient (DSC) exceeding that of the best compared method by over 2%. In various multi-center experiments, M(3)ResU-Net all achieves a DSC over 0.8. The results demonstrate that M(3)ResU-Net is capable of assisting clinicians in polyp segmentation in the field of colonoscopy, which provides important and reliable support to improve diagnostic efficiency.	[Fu, Junhu; Guo, Yi; Wang, Yuanyuan] Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China; [Lin, Shengli] Fudan Univ, Zhongshan Hosp, Endoscopy Ctr, Shanghai, Peoples R China; [Lin, Shengli] Fudan Univ, Zhongshan Hosp, Endoscopy Res Inst, Shanghai, Peoples R China; [Zhou, Pinghong] Shanghai Collaborat Innovat Ctr Endoscopy, Shanghai, Peoples R China	Fudan University; Fudan University; Fudan University	Guo, Y; Wang, YY (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.	guoyi@fudan.edu.cn; yywang@fudan.edu.cn	Wang, Yuan/HHC-1520-2022		National Natural Science Foundation of China [61871135, 81830058, 82000623]; Science and Technology Commission of Shanghai Municipality [20DZ1100104, 20DZ1100102]; Zhongshan Hospital Innovation Fund [2020ZSCX06]; Zhongshan Hospital Outstanding Youth Fund [2021ZSYQ12]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Commission of Shanghai Municipality(Science & Technology Commission of Shanghai Municipality (STCSM)); Zhongshan Hospital Innovation Fund; Zhongshan Hospital Outstanding Youth Fund	This work was supported by the National Natural Science Foundation of China (Grants 61871135, 81830058 and 82000623), the Science and Technology Commission of Shanghai Municipality (Grants 20DZ1100104 and 20DZ1100102), Zhongshan Hospital Innovation Fund (Grant 2020ZSCX06) and Zhongshan Hospital Outstanding Youth Fund (Grant 2021ZSYQ12).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Campos GFC, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0445-4; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen WQ, 2016, CA-CANCER J CLIN, V66, P115, DOI 10.3322/caac.21338; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Figueiredo IN, 2019, BIOMED SIGNAL PROCES, V53, DOI 10.1016/j.bspc.2019.101577; Guo XQ, 2021, IEEE T MED IMAGING, V40, P1134, DOI 10.1109/TMI.2020.3046843; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Keras Chollet F., 2018, Astrophys. Source Code Libr. ascl:1806.022; Larsen IK., 2016, CANC NORWAY 2015 CAN, V2016; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Poorneshwaran JM, 2019, IEEE ENG MED BIO, P7201, DOI [10.1109/EMBC.2019.8857958, 10.1109/embc.2019.8857958]; Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002; Sánchez-Peralta LF, 2020, ARTIF INTELL MED, V108, DOI 10.1016/j.artmed.2020.101923; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Siegel RL, 2017, CA-CANCER J CLIN, V67, P177, DOI 10.3322/caac.21395; Srivastava A, 2022, IEEE J BIOMED HEALTH, V26, P2252, DOI 10.1109/JBHI.2021.3138024; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Yang C, 2021, IEEE J BIOMED HEALTH, V25, P3886, DOI 10.1109/JBHI.2021.3077271; Yang XY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038011; Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1	34	6	6	4	44	IOP Publishing Ltd	BRISTOL	TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND	0031-9155	1361-6560		PHYS MED BIOL	Phys. Med. Biol.	OCT 21	2022	67	20							205005	10.1088/1361-6560/ac92bb	http://dx.doi.org/10.1088/1361-6560/ac92bb			16	Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Radiology, Nuclear Medicine & Medical Imaging	5D4HO	36113443				2024-09-18	WOS:000864905200001
C	Tan, JX; Gao, YF; Cao, WG; Pomeroy, M; Zhang, S; Huo, YM; Li, LH; Liang, ZR			IEEE	Tan, Jiaxing; Gao, Yongfeng; Cao, Weiguo; Pomeroy, Marc; Zhang, Shu; Huo, Yumei; Li, Lihong; Liang, Zhengrong			GLCM-CNN: Gray Level Co-occurrence Matrix based CNN Model for Polyp Diagnosis	2019 IEEE EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL & HEALTH INFORMATICS (BHI)			English	Proceedings Paper	IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)	MAY 19-22, 2019	Univ Illinois Chicago, Chicago, IL	IEEE, IEEE, Engn Med & Biol Soc, GeorgiaTech, Wallace H Coulter Dept Biomed Engn, Emory Univ, G Tec, Univ Illinois Urbana Champaign, Healthcare Engn Syst Ctr, Starkey Hearing Technologies, AMIA, NSF	Univ Illinois Chicago	Deep Learning; Polyp Diagnosis; GLCM; Medical Imaging; CT scan analysis	CANCER	The accurate identification of malignant polyp on colon CT is critical for the early detection of colorectal cancer, which also offers patients the best chance of cure. Deep learning based methods, especially convolution neural network (CNN) based methods, have been proposed for computer-aided polyp diagnosis due to CNN's strength in feature learning. However, most of the current CNN models focus on the 2D information or use multiple 2D slices as a 2.5D model input, which does not consider the 3D spatial information. In this work, we propose a CNN based 3D polyp diagnosis method. The proposed method encodes the 3D information into a multi-dimensional gray-level co-occurrence tensor. Each dimension represents one sampling view in the 3D space and 13 dimensions are used in this work. This model takes advantage of the co-occurrence matrix which is a good texture indicator to differentiate the tissue textures between benign and malignant. Additionally, our proposed method solves the problem of input size selection due to huge variants of polyp size and could be extended to other applications. Experiment results demonstrated that our method achieves an AUC of 0.93, which outperforms 2D (AUC 0.57) and 3D (AUC 0.72) convolution neural network solutions and the current state-of-the-art method (AUC 0.86).	[Tan, Jiaxing; Huo, Yumei; Li, Lihong] CUNY, New York, NY 10021 USA; [Gao, Yongfeng; Cao, Weiguo; Pomeroy, Marc; Zhang, Shu; Liang, Zhengrong] SUNY Stony Brook, Sch Med, Dept Radiol, Stony Brook, NY 11794 USA	City University of New York (CUNY) System; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Liang, ZR (corresponding author), SUNY Stony Brook, Sch Med, Dept Radiol, Stony Brook, NY 11794 USA.	Jerome.Liang@sunysb.edu			NIH of the National Cancer Institute [CA206171, CA220004]	NIH of the National Cancer Institute	This work was supported in part by NIH grant #CA206171 and #CA220004 of the National Cancer Institute.	American Cancer Society, 2018, FACTS FIG 2018; Anirudh R., 2016, LUNG NODULE DETECTIO, V9785; [Anonymous], IEEE J BIOMEDICAL HL; Byers T, 1997, CA-CANCER J CLIN, V47, P154, DOI 10.3322/canjclin.47.3.154; Ferlay J, 2015, INT J CANCER, V136, pE359, DOI 10.1002/ijc.29210; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hu YF, 2016, IEEE T MED IMAGING, V35, P1522, DOI 10.1109/TMI.2016.2518958; Jaffrey K, 2003, ESA'03: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON EMBEDDED SYSTEMS AND APPLICATIONS, P3; Liu H, 2017, PROCEEDINGS OF 2017 IEEE 2ND INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P1, DOI [10.1109/INTMAG.2017.8007847, 10.1109/ITNEC.2017.8284747]; Rathore S, 2013, IEEE ACM T COMPUT BI, V10, P545, DOI 10.1109/TCBB.2013.84; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Setio AAA, 2016, IEEE T MED IMAGING, V35, P1160, DOI 10.1109/TMI.2016.2536809; TAN J, 2018, J X-RAY SCI TECHNOL, P1	13	5	5	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-7281-0848-3				2019										10.1109/bhi.2019.8834585	http://dx.doi.org/10.1109/bhi.2019.8834585			4	Engineering, Biomedical; Medical Informatics	Conference Proceedings Citation Index - Science (CPCI-S)	Engineering; Medical Informatics	BO2RU					2024-09-18	WOS:000508002200081
J	Gilabert, P; Vitrià, J; Laiz, P; Malagelada, C; Watson, A; Wenzek, H; Segui, S				Gilabert, Pere; Vitria, Jordi; Laiz, Pablo; Malagelada, Carolina; Watson, Angus; Wenzek, Hagen; Segui, Santi			Artificial intelligence to improve polyp detection and screening time in colon capsule endoscopy	FRONTIERS IN MEDICINE			English	Article						colon capsule endoscopy; artificial intelligence; screening time; polyp detection; colorectal cancer prevention	COLORECTAL-CANCER; FUTURE	Colon Capsule Endoscopy (CCE) is a minimally invasive procedure which is increasingly being used as an alternative to conventional colonoscopy. Videos recorded by the capsule cameras are long and require one or more experts' time to review and identify polyps or other potential intestinal problems that can lead to major health issues. We developed and tested a multi-platform web application, AI-Tool, which embeds a Convolution Neural Network (CNN) to help CCE reviewers. With the help of artificial intelligence, AI-Tool is able to detect images with high probability of containing a polyp and prioritize them during the reviewing process. With the collaboration of 3 experts that reviewed 18 videos, we compared the classical linear review method using RAPID Reader Software v9.0 and the new software we present. Applying the new strategy, reviewing time was reduced by a factor of 6 and polyp detection sensitivity was increased from 81.08 to 87.80%.	[Gilabert, Pere; Vitria, Jordi; Laiz, Pablo; Segui, Santi] Univ Barcelona, Dept Matemat & Informat, Barcelona, Spain; [Malagelada, Carolina] Univ Hosp Vall dHebron, Digest Syst Res Unit, Barcelona, Spain; [Malagelada, Carolina] Univ Autonoma Barcelona, Dept Med, Barcelona, Spain; [Watson, Angus] NHS Highland, Dept Colorectal Surg, Raigmore Hosp, Inverness, Scotland; [Wenzek, Hagen] CorporateHlth Int ApS, Odense, Denmark	University of Barcelona; Hospital Universitari Vall d'Hebron; Autonomous University of Barcelona	Gilabert, P (corresponding author), Univ Barcelona, Dept Matemat & Informat, Barcelona, Spain.	pere.gilabert@ub.edu; angus.watson@nhs.scot	Segui, Santi/E-4860-2010; Gilabert, Pere/HQZ-4479-2023; Malagelada, Carolina/F-3743-2016	Gilabert, Pere/0000-0002-0597-853X; Malagelada, Carolina/0000-0001-7097-1492; watson, angus/0000-0003-1556-0097; Wenzek, Hagen/0000-0002-5395-4567				Ahmad OF, 2020, TECH INNOVAT GASTROI, V22, P80, DOI 10.1016/j.tgie.2019.150636; Ahmed M, 2020, GASTROENTEROL RES, V13, P1, DOI 10.14740/gr1239; Aoki T, 2020, DIGEST ENDOSC, V32, P585, DOI 10.1111/den.13517; Beg S, 2020, GASTROINTEST ENDOSC, V91, P1322, DOI 10.1016/j.gie.2020.01.026; Bond JH, 2003, ENDOSCOPY, V35, P27, DOI 10.1055/s-2003-36410; Byrne MF, 2019, GASTROINTEST ENDOSC, V89, P195, DOI 10.1016/j.gie.2018.08.017; Zammit SC, 2021, EXPERT REV GASTROENT, V15, P127, DOI 10.1080/17474124.2021.1840351; Farnbacher MJ, 2014, SCAND J GASTROENTERO, V49, P339, DOI 10.3109/00365521.2013.865784; Goran L, 2018, WORLD J GASTRO ENDOS, V10, P184, DOI 10.4253/wjge.v10.i9.184; Günther U, 2012, INT J COLORECTAL DIS, V27, P521, DOI 10.1007/s00384-011-1347-9; Guo XQ, 2019, LECT NOTES COMPUT SC, V11764, P293, DOI 10.1007/978-3-030-32239-7_33; Hausmann J, 2019, INT J COLORECTAL DIS, V34, P1857, DOI 10.1007/s00384-019-03393-0; Ismail MS, 2022, BMC GASTROENTEROL, V22, DOI 10.1186/s12876-021-02081-0; Jin EH, 2020, GASTROENTEROLOGY, V158, P2169, DOI [10.1053/j.gastro.2020.02.036, 10.1053/j.gastro.2020.02.03]; de Jonge L, 2021, LANCET GASTROENTEROL, V6, P304, DOI 10.1016/S2468-1253(21)00003-0; Koulaouzidis A, 2021, THER ADV GASTROINTES, V14, DOI 10.1177/26317745211001983; Laghi L, 2021, LANCET GASTROENTEROL, V6, P425, DOI 10.1016/S2468-1253(21)00098-4; Laiz P, 2020, COMPUT MED IMAG GRAP, V86, DOI 10.1016/j.compmedimag.2020.101794; Lauby-Secretan B, 2016, NEW ENGL J MED, V375, P794, DOI 10.1056/NEJMsr1606602; Levin TR, 2018, GASTROENTEROLOGY, V155, P1383, DOI 10.1053/j.gastro.2018.07.017; Loveday C, 2021, GUT, V70, P1053, DOI 10.1136/gutjnl-2020-321650; Maieron A, 2004, ENDOSCOPY, V36, P864, DOI 10.1055/s-2004-825852; Mascarenhas M, 2022, ENDOSC INT OPEN, V10, pE171, DOI 10.1055/a-1675-1941; Muehlematter UJ, 2021, LANCET DIGIT HEALTH, V3, pE195, DOI 10.1016/S2589-7500(20)30292-2; Noorda R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74668-8; O'Sullivan DE, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-76294-w; Rahim T, 2020, COMPUT MED IMAG GRAP, V85, DOI 10.1016/j.compmedimag.2020.101767; Rondonotti E, 2020, ENDOSC INT OPEN, V08, pE1220, DOI 10.1055/a-1210-4830; Saurin JC, 2016, CLIN ENDOSC, V49, P26, DOI 10.5946/ce.2016.49.1.26; Shiotani A, 2012, J CLIN GASTROENTEROL, V46, pE92, DOI 10.1097/MCG.0b013e31824fff94; Sung H, 2021, CA-CANCER J CLIN, V71, P209, DOI 10.3322/caac.21660; Topol EJ, 2019, NAT MED, V25, P44, DOI 10.1038/s41591-018-0300-7; Valle L, 2014, WORLD J GASTROENTERO, V20, P9828, DOI 10.3748/wjg.v20.i29.9828; Vuik FER, 2021, ENDOSCOPY, V53, P815, DOI 10.1055/a-1308-1297; Yang YJ, 2020, CLIN ENDOSC, V53, P387, DOI 10.5946/ce.2020.133; Ye PF, 2020, CANCERS, V12, DOI 10.3390/cancers12061408; Yuan YX, 2020, IEEE T AUTOM SCI ENG, V17, P574, DOI 10.1109/TASE.2019.2936645; Yuan YX, 2017, MED PHYS, V44, P1379, DOI 10.1002/mp.12147; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	39	5	5	1	7	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND		2296-858X		FRONT MED-LAUSANNE	Front. Med.	OCT 13	2022	9								1000726	10.3389/fmed.2022.1000726	http://dx.doi.org/10.3389/fmed.2022.1000726			8	Medicine, General & Internal	Science Citation Index Expanded (SCI-EXPANDED)	General & Internal Medicine	5T2PO	36314009	Green Submitted, gold, Green Published			2024-09-18	WOS:000875715500001
J	Jain, S; Atale, R; Gupta, A; Mishra, U; Seal, A; Ojha, A; Jaworek-Korjakowska, J; Krejcar, O				Jain, Samir; Atale, Rohan; Gupta, Anubhav; Mishra, Utkarsh; Seal, Ayan; Ojha, Aparajita; Jaworek-Korjakowska, Joanna; Krejcar, Ondrej			CoInNet: A Convolution-Involution Network With a Novel Statistical Attention for Automatic Polyp Segmentation	IEEE TRANSACTIONS ON MEDICAL IMAGING			English	Article						Polyp segmentation; deep neural network; statistical feature attention; wireless capsule endoscopy; boundary learning		Polyps are very common abnormalities in human gastrointestinal regions. Their early diagnosis may help in reducing the risk of colorectal cancer. Vision-based computer-aided diagnostic systems automatically identify polyp regions to assist surgeons in their removal. Due to their varying shape, color, size, texture, and unclear boundaries, polyp segmentation in images is a challenging problem. Existing deep learning segmentation models mostly rely on convolutional neural networks that have certain limitations in learning the diversity in visual patterns at different spatial locations. Further, they fail to capture inter-feature dependencies. Vision transformer models have also been deployed for polyp segmentation due to their powerful global feature extraction capabilities. But they too are supplemented by convolution layers for learning contextual local information. In the present paper, a polyp segmentation model CoInNet is proposed with a novel feature extraction mechanism that leverages the strengths of convolution and involution operations and learns to highlight polyp regions in images by considering the relationship between different feature maps through a statistical feature attention unit. To further aid the network in learning polyp boundaries, an anomaly boundary approximation module is introduced that uses recursively fed feature fusion to refine segmentation results. It is indeed remarkable that even tiny-sized polyps with only 0.01% of an image area can be precisely segmented by CoInNet. It is crucial for clinical applications, as small polyps can be easily overlooked even in the manual examination due to the voluminous size of wireless capsule endoscopy videos. CoInNet outperforms thirteen state-of-the-art methods on five benchmark polyp segmentation datasets.	[Jain, Samir; Atale, Rohan; Gupta, Anubhav; Mishra, Utkarsh; Seal, Ayan; Ojha, Aparajita] PDPM Indian Inst Informat Technol Design & Mfg, Jabalpur 482005, India; [Jaworek-Korjakowska, Joanna] AGH Univ Krakow, Dept Automat Control & Robot, PL-30059 Krakow, Poland; [Jaworek-Korjakowska, Joanna] AGH Univ Krakow, Ctr Excellence Artificial Intelligence, PL-30059 Krakow, Poland; [Krejcar, Ondrej] Univ Hradec Kralove, Fac Informat & Management, Ctr Basic & Appl Sci, Hradec Kralove 50003, Czech Republic; [Krejcar, Ondrej] Univ Teknol Malaysia, Malaysia Japan Int Inst Technol MJIIT, Kuala Lumpur 54100, Malaysia	Indian Institute of Information Technology Design & Manufacturing, Jabalpur; AGH University of Krakow; AGH University of Krakow; University of Hradec Kralove; Universiti Teknologi Malaysia	Seal, A (corresponding author), PDPM Indian Inst Informat Technol Design & Mfg, Jabalpur 482005, India.	samirjain@iiitdmj.ac.in; 21mcsa07@iiitdmj.ac.in; 21mcsa03@iiitdmj.ac.in; 22mcsa10@iiitdmj.ac.in; ayanseal30@ieee.org; aojha@iiitdmj.ac.in; jaworek@agh.edu.pl; ondrej.krejcar@uhk.cz	Jaworek-Korjakowska, Joanna/W-3601-2017; Krejcar, Ondrej/A-8639-2008; Seal, Ayan/AAI-1929-2020; Ojha, Aparajita/Q-3902-2016	Ojha, Aparajita/0000-0003-1567-8378; Seal, Ayan/0000-0002-9939-2926; Jaworek-Korjakowska, Joanna/0000-0003-0146-8652	University of Hradec Kralove, Faculty of Informatics and Management, Czech Republic [UHKFIM-GE-2023]; Program "Excellence Initiative Research University" for the AGH University, Krakow; ARTIQ Project [ARTIQ/0004/2021]	University of Hradec Kralove, Faculty of Informatics and Management, Czech Republic; Program "Excellence Initiative Research University" for the AGH University, Krakow; ARTIQ Project	This work was supported in part by the project "Smart Solutions in Ubiquitous Computing Environments", Grant Agency of Excellence (under ID: UHKFIM-GE-2023),University of Hradec Kralove, Faculty of Informatics and Management, Czech Republic; and in part by the Program "Excellence Initiative Research University" for the AGH University, Krakow, and the ARTIQ Project, under Grant ARTIQ/0004/2021.	Abraham N, 2019, I S BIOMED IMAGING, P683, DOI 10.1109/ISBI.2019.8759329; Banik D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3015607; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361; Cai LH, 2022, LECT NOTES COMPUT SC, V13434, P629, DOI 10.1007/978-3-031-16440-8_60; Chavan T, 2022, LECT NOTES COMPUT SC, V13263, P283, DOI 10.1007/978-3-031-09342-5_27; Chen J., 2021, PREPRINT, DOI [DOI 10.48550/ARXIV.2102.04306, 10.48550/arxiv.2102.04306]; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013; Dong B, 2024, Arxiv, DOI arXiv:2108.06932; Fan DP, 2018, Arxiv, DOI arXiv:1805.10421; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Guo YB, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070069; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang XH, 2023, IEEE T MED IMAGING, V42, P1484, DOI 10.1109/TMI.2022.3230943; Iakovidis DK, 2018, IEEE T MED IMAGING, V37, P2196, DOI 10.1109/TMI.2018.2837002; Jain S., 2021, Communications in Computer and Information Science, P538; Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14; Jin Y, 2023, VISUAL COMPUT, V39, P4819, DOI 10.1007/s00371-022-02630-y; Li D, 2021, PROC CVPR IEEE, P12316, DOI 10.1109/CVPR46437.2021.01214; Li ZH, 2024, IEEE T MED IMAGING, V43, P96, DOI 10.1109/TMI.2023.3291719; Liu AR, 2022, IEEE WINT CONF APPL, P1706, DOI 10.1109/WACV51458.2022.00177; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Long JAT, 2015, Arxiv, DOI arXiv:1411.4038; Mahmood S, 2021, TECH INNOVAT GASTROI, V23, P328, DOI 10.1016/j.tige.2021.06.007; Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018; Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/crv52889.2021.00032, 10.1109/CRV52889.2021.00032]; Qiu Z., 2022, arXiv; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Shi JH, 2023, IEEE T CIRC SYST VID, V33, P30, DOI 10.1109/TCSVT.2022.3197643; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Srivastava A, 2022, IEEE J BIOMED HEALTH, V26, P2252, DOI 10.1109/JBHI.2021.3138024; Ta N, 2023, MULTIMEDIA SYST, V29, P3041, DOI 10.1007/s00530-022-00900-2; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Vaswani A, 2017, ADV NEUR IN, V30; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang JC, 2023, IEEE T MED IMAGING, V42, P1735, DOI 10.1109/TMI.2023.3236037; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Xiao Hongxin, 2022, 2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE), P184, DOI 10.1109/AEMCSE55572.2022.00045; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]; Yue GH, 2022, IEEE J BIOMED HEALTH, V26, P4090, DOI 10.1109/JBHI.2022.3173948; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhao XQ, 2023, Arxiv, DOI arXiv:2303.10894; Zhao XQ, 2021, LECT NOTES COMPUT SC, V12901, P120, DOI 10.1007/978-3-030-87193-2_12; Zhou T, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109555; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]	52	6	6	17	44	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	0278-0062	1558-254X		IEEE T MED IMAGING	IEEE Trans. Med. Imaging	DEC	2023	42	12					3987	4000		10.1109/TMI.2023.3320151	http://dx.doi.org/10.1109/TMI.2023.3320151			14	Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	AY5W1	37768798				2024-09-18	WOS:001122030500041
J	Zhang, RK; Zheng, YL; Poon, CCY; Shen, DG; Lau, JYW				Zhang, Ruikai; Zheng, Yali; Poon, Carmen C. Y.; Shen, Dinggang; Lau, James Y. W.			Polyp detection during colonoscopy using a regression-based convolutional neural network with a tracker	PATTERN RECOGNITION			English	Article						Smart cancer screening; Therapeutic endoscopy; Endoscopic Informatics; Body Sensor Network; Deep Learning; Health Informatics	MOLECULAR PATHOLOGICAL EPIDEMIOLOGY; MISS RATE; COLORECTAL-CANCER; CLASSIFICATION; PATHWAYS; CAPSULE	A computer-aided detection (CAD) tool for locating and detecting polyps can help reduce the chance of missing polyps during colonoscopy. Nevertheless, state-of-the-art algorithms were either computationally complex or suffered from low sensitivity and therefore unsuitable to be used in real clinical setting. In this paper, a novel regression-based Convolutional Neural Network (CNN) pipeline is presented for polyp detection during colonoscopy. The proposed pipeline was constructed in two parts: 1) to learn the spatial features of colorectal polyps, a fast object detection algorithm named ResYOLO was pre-trained with a large non-medical image database and further fine-tuned with colonoscopic images extracted from videos; and 2) temporal information was incorporated via a tracker named Efficient Convolution Operators (ECO) for refining the detection results given by ResYOLO. Evaluated on 17,574 frames extracted from 18 endoscopic videos of the AsuMayoDB, the proposed method was able to detect frames with polyps with a precision of 88.6%, recall of 71.6% and processing speed of 6.5 frames per second, i.e. the method can accurately locate polyps in more frames and at a faster speed compared to existing methods. In conclusion, the proposed method has great potential to be used to assist endoscopists in tracking polyps during colonoscopy. (C) 2018 Elsevier Ltd. All rights reserved.	[Zhang, Ruikai; Zheng, Yali; Poon, Carmen C. Y.; Lau, James Y. W.] Chinese Univ Hong Kong, Dept Surg, Hong Kong, Hong Kong, Peoples R China; [Shen, Dinggang] Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA; [Shen, Dinggang] Univ N Carolina, BRIC, Chapel Hill, NC 27599 USA; [Shen, Dinggang] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea	Chinese University of Hong Kong; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina; University of North Carolina Chapel Hill; Korea University	Poon, CCY (corresponding author), Chinese Univ Hong Kong, Dept Surg, Hong Kong, Hong Kong, Peoples R China.; Shen, DG (corresponding author), Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA.; Shen, DG (corresponding author), Univ N Carolina, BRIC, Chapel Hill, NC 27599 USA.	rzhang@surgery.cuhk.edu.hk; ylzheng@surgery.cuhk.edu.hk; cpoon@surgery.cuhk.edu.hk; dgshen@med.unc.edu; laujyw@surgery.cuhk.edu.hk	Zhang, Ruikai/W-9848-2019; Lau, James/O-2612-2016; Shen, Dinggang/ABF-6812-2020; Poon, Carmen/B-4616-2011	Poon, Carmen/0000-0001-7717-4752	General Research Fund [GRF/14202417]; Innovation and Technology Fund [ITF/337/16FP]	General Research Fund; Innovation and Technology Fund	This project is supported in part by General Research Fund (GRF/14202417) and Innovation and Technology Fund (ITF/337/16FP).	Afridi MJ, 2018, PATTERN RECOGN, V73, P65, DOI 10.1016/j.patcog.2017.07.019; [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707; Bae SH, 2015, IEEE T MED IMAGING, V34, P2379, DOI 10.1109/TMI.2015.2434398; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042; Bishehsari F, 2014, WORLD J GASTROENTERO, V20, P6055, DOI 10.3748/wjg.v20.i20.6055; Colussi D, 2013, INT J MOL SCI, V14, P16365, DOI 10.3390/ijms140816365; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056; Ferlay J, 2015, INT J CANCER, V136, pE359, DOI 10.1002/ijc.29210; Häfner M, 2009, PATTERN RECOGN, V42, P1180, DOI 10.1016/j.patcog.2008.07.012; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heresbach D, 2008, ENDOSCOPY, V40, P284, DOI 10.1055/s-2007-995618; Jiao ZC, 2018, PATTERN RECOGN, V75, P292, DOI 10.1016/j.patcog.2017.07.008; Kaminski MF, 2012, ENDOSCOPY, V44, P695, DOI 10.1055/s-0032-1306895; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Leung BHK, 2017, IEEE T BIO-MED ENG, V64, P1106, DOI 10.1109/TBME.2016.2591060; Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959; Mori Y, 2017, ENDOSCOPY, V49, P813, DOI 10.1055/s-0043-109430; Nawarathna R, 2014, NEUROCOMPUTING, V144, P70, DOI 10.1016/j.neucom.2014.02.064; Ogino S, 2016, EPIDEMIOLOGY, V27, P602, DOI 10.1097/EDE.0000000000000471; Ogino S, 2011, GUT, V60, P397, DOI 10.1136/gut.2010.217182; Park S., 2015, Polyp detection in colonoscopy videos using deeply-learned hierarchical features; Park S. Y., 2016, MED IMAGING 2016 COP; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Roth HR, 2016, IEEE T MED IMAGING, V35, P1170, DOI 10.1109/TMI.2015.2482920; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516044442, 10.1146/annurev-bioeng-071516-044442]; Tajbakhsh N, 2017, PATTERN RECOGN, V63, P476, DOI 10.1016/j.patcog.2016.09.029; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Tajbakhsh N, 2015, I S BIOMED IMAGING, P79, DOI 10.1109/ISBI.2015.7163821; van Rijn JC, 2006, AM J GASTROENTEROL, V101, P343, DOI 10.1111/j.1572-0241.2006.00390.x; Wang Y, 2014, IEEE J BIOMED HEALTH, V18, P1379, DOI 10.1109/JBHI.2013.2285230; Yu LQ, 2017, IEEE J BIOMED HEALTH, V21, P65, DOI 10.1109/JBHI.2016.2637004; Yuan ZJ, 2017, PROC SPIE, V10133, DOI 10.1117/12.2254671; Zhang RK, 2017, IEEE J BIOMED HEALTH, V21, P41, DOI 10.1109/JBHI.2016.2635662	38	108	118	2	80	ELSEVIER SCI LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND	0031-3203	1873-5142		PATTERN RECOGN	Pattern Recognit.	NOV	2018	83						209	219		10.1016/j.patcog.2018.05.026	http://dx.doi.org/10.1016/j.patcog.2018.05.026			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GR0BL	31105338	Green Accepted, Green Published			2024-09-18	WOS:000442172200016
J	Yue, GH; Li, SY; Zhou, TW; Wang, MH; Du, JF; Jiang, QP; Gao, W; Wang, TF; Lv, J				Yue, Guanghui; Li, Siying; Zhou, Tianwei; Wang, Miaohui; Du, Jingfeng; Jiang, Qiuping; Gao, Wei; Wang, Tianfu; Lv, Jun			Adaptive Context Exploration Network for Polyp Segmentation in Colonoscopy Images	IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE			English	Article						Image segmentation; Feature extraction; Colonoscopy; Shape; Decoding; Convolution; Annotations; Convolutional neural network; polyp segmentation; colonoscopy image; context exploration	COLORECTAL-CANCER; VALIDATION	Recently, automatic and accurate polyp segmentation has become an emerging yet challenging issue. Although convolutional neural networks (CNNs) exhibit a promising future modality to address this issue, most CNN-based methods highly require extensive labeled data. Unfortunately, there is a lack of large-scale public colorectal polyp segmentation datasets in the clinical community and academia. In this study, we construct a new benchmark dataset, which includes 2163 colonoscopy images and their pixel-wise annotations. Moreover, for intelligent polyp segmentation, we propose a novel adaptive context exploration network (ACENet). Our ACENet follows an encoder-decoder architecture and consists of two key modules, i.e., an attentional atrous spatial pyramid pooling (AASPP) module and an adaptive context extraction (ACE) module. The AASPP fuses semantic features from the encoder, and generates the global guidance information for the following decoder. The ACE captures multi-scale features and aggregates them by a branch-wise attention mechanism. Benefiting from these two modules, our ACENet is capable of adaptively exploring the context features to locate and detect the polyp regions effectively. Extensive experiments on the collected dataset and four publicly available datasets show that the proposed ACENet achieves superior performance on five evaluation metrics over three mainstream categories of the state-of-the-art methods.	[Yue, Guanghui; Li, Siying; Wang, Tianfu] Shenzhen Univ, Sch Biomed Engn, Hlth Sci Ctr, Shenzhen 518060, Peoples R China; [Zhou, Tianwei] Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China; [Wang, Miaohui] Shenzhen Univ, Guangdong Key Lab Intelligent format Proc, Shenzhen 518060, Peoples R China; [Du, Jingfeng] Shenzhen Univ, Dept Gastroenterol & Hepatol, Gen Hosp, Shenzhen 518060, Peoples R China; [Jiang, Qiuping] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China; [Gao, Wei] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China; [Gao, Wei] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Lv, Jun] Yantai Univ, Sch Comp & Control Engn, Yantai 264000, Peoples R China	Shenzhen University; Shenzhen University; Shenzhen University; Shenzhen University; Ningbo University; Peking University; Peng Cheng Laboratory; Yantai University	Zhou, TW (corresponding author), Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.	yueguanghui@szu.edu.cn; 2070246077@email.szu.edu.cn; tianwei@szu.edu.cn; wang.miaohui@gmail.com; djfjms1231@qq.com; jiangqiuping@nbu.edu.cn; gaowei262@pku.edu.cn; wang.miaohui@gmail.com; ljdream0710@pku.edu.cn	Lyu, Jun/HLQ-3356-2023; Zhou, Tianwei/GSI-8460-2022; Jiang, Qiuping/AAL-8273-2020	Qiuping, Jiang/0000-0002-6025-9343	Guangdong Basic and Applied Basic Research Foundation [2021A1515011348, 2019A1515111205, 2019A1515110401]; Shenzhen Science andTechnology Program [RCBS20200714114920379]; Natural Science Foundation of Shenzhen [JCYJ20190808145011259]; National Natural Science Foundation of China [62001302, 62071309, 62103286]; Tencent "Rhinoceros Birds" -Scientific Research Foundation for Young Teachers of Shenzhen University; Social Science Youth Foundation of Ministry of Education of China [21YJC630181]; Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University [VRLAB2021C05]	Guangdong Basic and Applied Basic Research Foundation; Shenzhen Science andTechnology Program; Natural Science Foundation of Shenzhen; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Tencent "Rhinoceros Birds" -Scientific Research Foundation for Young Teachers of Shenzhen University; Social Science Youth Foundation of Ministry of Education of China; Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University	This work was supported in part by Guangdong Basic and Applied Basic Research Foundation underGrants 2021A1515011348, 2019A1515111205, and 2019A1515110401, in part by Shenzhen Science andTechnology Program under Grant RCBS20200714114920379, in part by the Natural Science Foundation of Shenzhen under Grant JCYJ20190808145011259, in part by the National Natural Science Foundation of China under Grants 62001302, 62071309, and 62103286, in part by Tencent "Rhinoceros Birds" -Scientific Research Foundation for Young Teachers of Shenzhen University, in part by Social Science Youth Foundation of Ministry of Education of China under Grant 21YJC630181, and in part by Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University under Grant VRLAB2021C05.	Ameling S., 2009, P BILDV MED ALG SYST, P346; Banik D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3015607; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Q, 2022, IEEE T EM TOP COMP I, V6, P1190, DOI 10.1109/TETCI.2021.3051910; Cheng MJ, 2021, LECT NOTES COMPUT SC, V12901, P720, DOI 10.1007/978-3-030-87193-2_68; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Goyal M, 2020, IEEE TETCI, V4, P728, DOI 10.1109/TETCI.2018.2866254; Heinrich MP, 2019, MED IMAGE ANAL, V54, P1, DOI 10.1016/j.media.2019.02.006; Hwang S., 2007, 2007 IEEE INT C IMAG, V2, DOI DOI 10.1109/ICIP.2007.4379193; Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Jiafu Zhong, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P285, DOI 10.1007/978-3-030-59725-2_28; Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Lee SG, 2023, IEEE T EM TOP COMP I, V7, P319, DOI 10.1109/TETCI.2021.3132382; Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin T.Y., 2017, P IEEE C COMP VIS PA, P2117, DOI [DOI 10.1109/CVPR.2017.106, 10.1109/CVPR.2017.106]; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959; Prasath VBS, 2017, J IMAGING, V3, DOI 10.3390/jimaging3010001; Qu XY, 2022, IEEE T EM TOP COMP I, V6, P580, DOI 10.1109/TETCI.2021.3070713; Rabeneck L, 2003, AM J GASTROENTEROL, V98, P471, DOI 10.1016/S0002-9270(02)05928-2; Rex DK, 2017, GASTROINTEST ENDOSC, V86, P18, DOI 10.1016/j.gie.2017.04.003; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shunjie Dong, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12264), P98, DOI 10.1007/978-3-030-59719-1_10; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Ul Ain Q, 2021, IEEE TETCI, V5, P554, DOI 10.1109/TETCI.2020.2983426; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Wang Ruxin, 2022, Med Image Anal, V78, P102395, DOI 10.1016/j.media.2022.102395; Wang S, 2021, IEEE J BIOMED HEALTH, V25, P514, DOI 10.1109/JBHI.2020.2997760; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Wickstrom K, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101619; Wu HS, 2021, AAAI CONF ARTIF INTE, V35, P2916; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Xinzi Sun, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P851, DOI 10.1109/ICMLA.2019.00148; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Yap MH, 2021, COMPUT BIOL MED, V135, DOI 10.1016/j.compbiomed.2021.104596; Yeung M, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104815; Yin Zenong, 2022, Public Health Nutr, P1, DOI [10.1109/ICIT48603.2022.10002826, 10.1017/S1368980022002439]; Yu F., 2016, 4 INT C LEARN REPR I; Zhou WJ, 2022, IEEE T EM TOP COMP I, V6, P957, DOI 10.1109/TETCI.2021.3118043; Zhou WJ, 2022, IEEE T EM TOP COMP I, V6, P593, DOI 10.1109/TETCI.2021.3097393; Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609; Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526	56	13	13	7	45	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2471-285X			IEEE T EM TOP COMP I	IEEE Trans. Emerg. Top. Comput. Intell.	APR	2023	7	2					487	499		10.1109/TETCI.2022.3193677	http://dx.doi.org/10.1109/TETCI.2022.3193677		AUG 2022	13	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	J0ZS7					2024-09-18	WOS:000840486600001
J	Mahmud, T; Paul, B; Fattah, SA				Mahmud, Tanvir; Paul, Bishmoy; Fattah, Shaikh Anowarul			PolypSegNet: A modified encoder-decoder architecture for automated polyp segmentation from colonoscopy images	COMPUTERS IN BIOLOGY AND MEDICINE			English	Article						Polyp segmentation; Colorectal cancer; Colonoscopy; Computer-aided diagnosis; Neural network	ADENOMA DETECTION; VALIDATION; DIAGNOSIS; NETWORK; CANCER	Colorectal cancer has become one of the major causes of death throughout the world. Early detection of Polyp, an early symptom of colorectal cancer, can increase the survival rate to 90%. Segmentation of Polyp regions from colonoscopy images can facilitate the faster diagnosis. Due to varying sizes, shapes, and textures of polyps with subtle visible differences with the background, automated segmentation of polyps still poses a major challenge towards traditional diagnostic methods. Conventional Unet architecture and some of its variants have gained much popularity for its automated segmentation though having several architectural limitations that result in sub-optimal performance. In this paper, an encoder-decoder based modified deep neural network architecture is proposed, named as PolypSegNet, to overcome several limitations of traditional architectures for very precise automated segmentation of polyp regions from colonoscopy images. For achieving more generalized representation at each scale of both the encoder and decoder module, several sequential depth dilated inception (DDI) blocks are integrated into each unit layer for aggregating features from different receptive areas utilizing depthwise dilated convolutions. Different scales of contextual information from all encoder unit layers pass through the proposed deep fusion skip module (DFSM) to generate skip interconnection with each decoder layer rather than separately connecting different levels of encoder and decoder. For more efficient reconstruction in the decoder module, multi-scale decoded feature maps generated at various levels of the decoder are jointly optimized in the proposed deep reconstruction module (DRM) instead of only considering the decoded feature map from final decoder layer. Extensive experimentations on four publicly available databases provide very satisfactory performance with mean five-fold cross-validation dice scores of 91.52% in CVC-ClinicDB database, 92.8% in CVC-ColonDB database, 88.72% in Kvasir-SEG database, and 84.79% in ETIS-Larib database. The proposed network provides very accurate segmented polyp regions that will expedite the diagnosis of polyps even in challenging conditions.	[Mahmud, Tanvir; Paul, Bishmoy; Fattah, Shaikh Anowarul] BUET, Dept EEE, ECE Bldg, Dhaka 1205, Bangladesh	Bangladesh University of Engineering & Technology (BUET)	Fattah, SA (corresponding author), BUET, Dept EEE, ECE Bldg, Dhaka 1205, Bangladesh.	tanvirmahmud@eee.buet.ac.bd; paul.bish98@gmail.com; fattah@eee.buet.ac.bd	Mahmud, Tanvir/AAU-9382-2020	Paul, Bishmoy/0000-0002-1021-7882; Mahmud, Tanvir/0000-0003-0529-2826				Abraham N, 2019, I S BIOMED IMAGING, P683, DOI 10.1109/ISBI.2019.8759329; [Anonymous], 2020, J PEST SCI, DOI DOI 10.1007/S10340-020-01281-Z; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP); Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dawwas MF, 2014, NEW ENGL J MED, V370, P2539, DOI [10.1056/NEJMc1405329, 10.1056/NEJMoa1309086]; Guo YB, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P632, DOI 10.5220/0007698806320641; Guo YB, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070069; Hashemi S.R., 2018, ARXIV1803, P11078; Hassan C, 2020, GUT, V69, P799, DOI 10.1136/gutjnl-2019-319914; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025; Iglesias A., 2020, NEUROCOMPUTING; Jemal A, 2010, CA-CANCER J CLIN, V60, P277, DOI [10.3322/caac.21254, 10.3322/caac.20073]; Jha D, 2020, Doubleu-net: A deep convolutional neural network for medical image segmentation; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Kaminski MF, 2017, GASTROENTEROLOGY, V153, P98, DOI 10.1053/j.gastro.2017.04.006; Kang J, 2019, IEEE ACCESS, V7, P26440, DOI 10.1109/ACCESS.2019.2900672; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lou GC, 2014, TURK J GASTROENTEROL, V25, P182, DOI 10.5152/tjg.2014.4664; Mou L, 2020, IEEE T MED IMAGING, V39, P1392, DOI 10.1109/TMI.2019.2950051; Prasath VBS, 2017, J IMAGING, V3, DOI 10.3390/jimaging3010001; Qadir H. A., 2019, 2019 13 INT S MED IN, P1; Qadir HA, 2019, IEEE ACCESS, V7, P169537, DOI 10.1109/ACCESS.2019.2954675; Rex DK, 2000, GASTROINTEST ENDOSC, V51, P33, DOI 10.1016/S0016-5107(00)70383-X; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rutter CM, 2012, CANCER CAUSE CONTROL, V23, P289, DOI 10.1007/s10552-011-9878-5; Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; van Wijk C, 2010, IEEE T MED IMAGING, V29, P688, DOI 10.1109/TMI.2009.2031323; Wang LS, 2019, IEEE ACCESS, V7, P44676, DOI 10.1109/ACCESS.2019.2908386; Wang P, 2020, LANCET GASTROENTEROL, V5, P343, DOI [10.1016/S2468-1253(19)30411-x, 10.1016/S2468-1253(19)30411-X]; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Wang R., 2020, ARXIV200500966; Yao JH, 2004, IEEE T MED IMAGING, V23, P1344, DOI 10.1109/TMI.2004.826941; Yao JH, 2007, MED PHYS, V34, P1655, DOI 10.1118/1.2717411; Yu F., 2015, arXiv preprint arXiv: 1506. 03365; Yuan YX, 2018, IEEE J BIOMED HEALTH, V22, P1250, DOI 10.1109/JBHI.2017.2734329; Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609	43	52	56	2	32	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0010-4825	1879-0534		COMPUT BIOL MED	Comput. Biol. Med.	JAN	2021	128								104119	10.1016/j.compbiomed.2020.104119	http://dx.doi.org/10.1016/j.compbiomed.2020.104119			13	Biology; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Mathematical & Computational Biology	Science Citation Index Expanded (SCI-EXPANDED)	Life Sciences & Biomedicine - Other Topics; Computer Science; Engineering; Mathematical & Computational Biology	PN6FH	33254083				2024-09-18	WOS:000604572200002
J	Kim, HR; Kim, KJ; Lim, KT; Choi, DH				Kim, Hwa-Rang; Kim, Kwang-Ju; Lim, Kil-Taek; Choi, Doo-Hyun			Histological Image Segmentation and Classification Using Entropy-Based Convolutional Module	IEEE ACCESS			English	Article						Deep learning; Image segmentation; Electronic countermeasures; Convolution; Channel coding; Communication systems; Solid modeling; Segmentation; deep learning; information theory; Shannon entropy; colorectal cancer image		As the powerful performance of deep learning has been proven, many computer vision researchers have applied deep learning methods to their works as a breakthrough that could not be achieved with conventional computer vision algorithms. Particularly in pathological image analysis, deep learning plays an important role because some diagnosis requires a considerable cost or much time. In a recent, convolutional neural network (CNN)-based deep learning models have shown meaningful results in pathological image analysis, reducing time and cost. However, existing CNN-based segmentation models perform the same convolution operation for all channels of a feature map. It could be an inefficient operation according to information theory. We propose (Shannon) entropy-based convolutional module (ECM) for efficient convolutional operation in terms of a communication system. The fundamental coding manner of a communication system based on information theory is to allocate fewer bits for data showing the high probability of occurrence, and vice versa. Following up this coding manner, a feature is divided into dominant and recessive features according to the channel importance calculated from the channel attention module, and a heavy operation is conducted on the recessive feature and a light operation is conducted on the dominant feature. This operating manner can make a network perform efficient calculations and improve its performance. Furthermore, our proposed module is a portable unit, thus it can be a replacement of any convolution without modification of the whole architecture. To the best of our knowledge, our proposed module is the first trial to mimic the coding manner of information theory. The models equipped with our proposed module outperform the original models achieving 0.855 of F1 score and 0.832 of Jaccard score on colorectal cancer (CRC) image data-set.	[Kim, Hwa-Rang; Choi, Doo-Hyun] Kyungpook Natl Univ, Grad Sch Elect & Elect Engn, Daegu 41566, South Korea; [Kim, Hwa-Rang; Kim, Kwang-Ju; Lim, Kil-Taek] Elect & Telecommun Res Inst, Daegu 42994, South Korea	Kyungpook National University (KNU); Electronics & Telecommunications Research Institute - Korea (ETRI)	Choi, DH (corresponding author), Kyungpook Natl Univ, Grad Sch Elect & Elect Engn, Daegu 41566, South Korea.; Kim, KJ (corresponding author), Elect & Telecommun Res Inst, Daegu 42994, South Korea.	kwangju@etri.re.kr; dhc@ee.knu.ac.kr		Kim, KwangJu/0000-0001-8458-4506; Doo-Hyun, Choi/0000-0002-4950-8863	Electronics and Telecommunications Research Institute (ETRI) Grant through the Korean Government (Development of ICT Convergence Technology for Daegu-GyeongBuk Regional Industry) [21ZD1100]; Seoul National University Hospital through the Korea Health Technology Research and Development Project by the Korea Health Industry Development Institute (KHIDI) by the Ministry of Health and Welfare, Republic of Korea [HI18C0316]	Electronics and Telecommunications Research Institute (ETRI) Grant through the Korean Government (Development of ICT Convergence Technology for Daegu-GyeongBuk Regional Industry); Seoul National University Hospital through the Korea Health Technology Research and Development Project by the Korea Health Industry Development Institute (KHIDI) by the Ministry of Health and Welfare, Republic of Korea	This work was supported in part by the Electronics and Telecommunications Research Institute (ETRI) Grant through the Korean Government (Development of ICT Convergence Technology for Daegu-GyeongBuk Regional Industry) under Grant 21ZD1100, and in part by the Seoul National University Hospital through the Korea Health Technology Research and Development Project by the Korea Health Industry Development Institute (KHIDI) by the Ministry of Health and Welfare, Republic of Korea, under Grant HI18C0316.	[Anonymous], P 3 INT C LEARNING R; Araújo RJ, 2019, LECT NOTES COMPUT SC, V11764, P93, DOI 10.1007/978-3-030-32239-7_11; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7; Eavani H, 2014, LECT NOTES COMPUT SC, V8675, P193, DOI 10.1007/978-3-319-10443-0_25; Han SH, 2007, ETRI J, V29, P641, DOI 10.4218/etrij.07.0106.0316; Hartley RVL, 1928, BELL SYST TECH J, V7, P535, DOI 10.1002/j.1538-7305.1928.tb01236.x; He K., 2017, IEEE I CONF COMP VIS, P2961, DOI DOI 10.1109/ICCV.2017.322; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]; Kingma D. P., 2017, arXiv; Kingma D. P., 2013, CORR; Li C., 2021, SOCIAL NETW COMPUT S, V2, P1; Li C, 2020, ARTIF INTELL REV, V53, P4821, DOI 10.1007/s10462-020-09808-7; Li C, 2019, IEEE ACCESS, V7, P90378, DOI 10.1109/ACCESS.2019.2924467; Liang-Chieh C., 2015, ARXIV14127062, P1; Liu Y., 2019, P INT C MED IM COMP, P193; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Macenko M, 2009, I S BIOMED IMAGING, P1107, DOI 10.1109/ISBI.2009.5193250; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Nouranian S, 2013, LECT NOTES COMPUT SC, V8150, P173, DOI 10.1007/978-3-642-40763-5_22; Qi KH, 2019, LECT NOTES COMPUT SC, V11766, P247, DOI 10.1007/978-3-030-32248-9_28; Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shannon C.E., 2001, MOBILE COMPUTING COM, V5, P3, DOI [DOI 10.1145/584091.584093, DOI 10.1002/J.1538-7305.1948.TB00917.X, 10.1002/j.1538-7305.1948.tb01338.x, DOI 10.1002/J.1538-7305.1948.TB01338.X]; Simpson IJA, 2013, LECT NOTES COMPUT SC, V8150, P10, DOI 10.1007/978-3-642-40763-5_2; Sun CH, 2020, BIOCYBERN BIOMED ENG, V40, P1535, DOI 10.1016/j.bbe.2020.09.008; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vahadane A, 2016, IEEE T MED IMAGING, V35, P1962, DOI 10.1109/TMI.2016.2529665; Wang B, 2019, LECT NOTES COMPUT SC, V11764, P84, DOI 10.1007/978-3-030-32239-7_10; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Wu YC, 2019, LECT NOTES COMPUT SC, V11764, P264, DOI 10.1007/978-3-030-32239-7_30; Xu J, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2020.101835; Xu J, 2016, NEUROCOMPUTING, V191, P214, DOI 10.1016/j.neucom.2016.01.034; Xue D, 2020, IEEE ACCESS, V8, P104603, DOI 10.1109/ACCESS.2020.2999816; Yin PS, 2019, LECT NOTES COMPUT SC, V11764, P129, DOI 10.1007/978-3-030-32239-7_15; Yokota F, 2013, LECT NOTES COMPUT SC, V8150, P190, DOI 10.1007/978-3-642-40763-5_24; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou XM, 2020, IEEE ACCESS, V8, P90931, DOI 10.1109/ACCESS.2020.2993788; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]	45	1	1	1	9	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2169-3536			IEEE ACCESS	IEEE Access		2021	9						90964	90976		10.1109/ACCESS.2021.3091578	http://dx.doi.org/10.1109/ACCESS.2021.3091578			13	Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Telecommunications	TJ8GN		gold			2024-09-18	WOS:000673713200001
C	Zhou, YN; Graham, S; Koohbanani, NA; Shaban, M; Heng, PA; Rajpoot, N			IEEE	Zhou, Yanning; Graham, Simon; Koohbanani, Navid Alemi; Shaban, Muhammad; Heng, Pheng-Ann; Rajpoot, Nasir			CGC-Net: Cell Graph Convolutional Network for Grading of Colorectal Cancer Histology Images	2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW)	IEEE International Conference on Computer Vision Workshops		English	Proceedings Paper	IEEE/CVF International Conference on Computer Vision (ICCV)	OCT 27-NOV 02, 2019	Seoul, SOUTH KOREA	IEEE, IEEE Comp Soc, CVF			NEURAL-NETWORK; CLASSIFICATION	Colorectal cancer (CRC) grading is typically carried out by assessing the degree of gland formation within histology images. To do this, it is important to consider the over-all tissue micro-environment by assessing the cell-level information along with the morphology of the gland. However, current automated methods for CRC grading typically utilise small image patches and therefore fail to incorporate the entire tissue micro-architecture for grading purposes. To overcome the challenges of CRC grading, we present a novel cell-graph convolutional neural network (CGC-Net) that converts each large histology image into a graph, where each node is represented by a nucleus within the original image and cellular interactions are denoted as edges between these nodes according to node similarity. The CGC-Net utilises nuclear appearance features in addition to the spatial location of nodes to further boost the performance of the algorithm. To enable nodes to fuse multi-scale information, we introduce Adaptive GraphSage, which is a graph convolution technique that combines multi-level features in a data-driven way. Furthermore, to deal with redundancy in the graph, we propose a sampling technique that removes nodes in areas of dense nuclear activity. We show that modeling the image as a graph enables us to effectively consider a much larger image (around 16 x larger) than traditional patch-based approaches and model the complex structure of the tissue micro-environment. We construct cell graphs with an average of over 3,000 nodes on a large CRC histology image dataset and report state-of-the-art results as compared to recent patch-based as well as contextual patch-based techniques, demonstrating the effectiveness of our method.	[Zhou, Yanning; Heng, Pheng-Ann] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Graham, Simon; Koohbanani, Navid Alemi; Shaban, Muhammad; Rajpoot, Nasir] Univ Warwick, Dept Comp Sci, TIA Lab, Coventry, W Midlands, England; [Zhou, Yanning] Univ Warwick, Tissue Image Analyt TIA Lab, Coventry, W Midlands, England	Chinese University of Hong Kong; University of Warwick; University of Warwick	Zhou, YN (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	ynzhou@cse.cuhk.edu.hk; s.graham.1@warwick.ac.uk; n.alemi-koohbanani@warwick.ac.uk; m.shaban@warwick.ac.uk; pheng@cse.cuhk.edu.hk; n.m.rajpoot@warwick.ac.uk	Koohbanani, Navid/AAQ-1440-2020; Shaban, Muhammad/AEV-0783-2022	Shaban, Muhammad/0000-0001-6115-1505; Rajpoot, Nasir/0000-0002-4706-1308	UK Medical Research Council [MR/P015476/1]; Hong Kong Research Grants Council under General Research Fund [14225616]; Warwick Global Partnership Fund (GPF); Warwick; CUHK; MRC [MR/P015476/1] Funding Source: UKRI	UK Medical Research Council(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Hong Kong Research Grants Council under General Research Fund(Hong Kong Research Grants Council); Warwick Global Partnership Fund (GPF); Warwick; CUHK(Chinese University of Hong Kong); MRC(UK Research & Innovation (UKRI)Medical Research Council UK (MRC))	This work was supported in part by the UK Medical Research Council grant#MR/P015476/1 and the Hong Kong Research Grants Council under General Research Fund (Project No. 14225616). The authors are also grateful to the Warwick Global Partnership Fund (GPF) for funding the collaboration between Warwick and CUHK.	[Anonymous], 2019, INT C MACH LEARN; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, ARXIV170302442; [Anonymous], 2017, P 2017 IEEE C COMP V; [Anonymous], 2016, INT C MACH LEARN; [Anonymous], 2016, ADV NEURAL INFORM PR; [Anonymous], 2017, ARXIV170404861; [Anonymous], 2018, 32 AAAI C ART INT; [Anonymous], 2016, INT JOINT C NEUR NET; [Anonymous], 2014, 2 INT C LEARNING REP; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90; [Anonymous], 2018, INT C MACH LEARN; Araújo T, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177544; Awan R, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16516-w; Bilgin CC, 2010, DATA MIN KNOWL DISC, V20, P416, DOI 10.1007/s10618-009-0153-2; Bilgin Cemal Cagatay, 2007, ENG MED BIOL SOC; Chen Zhaomin, 2019, PROCEEDINGS OF THE I; Compton CC, 2000, ARCH PATHOL LAB MED, V124, P979; Coudray N, 2018, NAT MED, V24, P1559, DOI 10.1038/s41591-018-0177-5; Cruz-Roa A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196828; Demir C, 2005, BIOINFORMATICS, V21, P7, DOI 10.1093/bioinformatics/bti1100; Diamond J, 2004, HUM PATHOL, V35, P1121, DOI 10.1016/j.humpath.2004.05.010; Dundar MM, 2011, IEEE T BIO-MED ENG, V58, P1977, DOI 10.1109/TBME.2011.2110648; Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193; Fey M., 2019, ICLR 2019, P1; Gecer B, 2018, PATTERN RECOGN, V84, P345, DOI 10.1016/j.patcog.2018.07.022; Gori Marco, 2005, P 2005 IEEE INT JOIN; Graham S, 2019, MED IMAGE ANAL, V52, P199, DOI 10.1016/j.media.2018.12.001; Graham S, 2018, PROC SPIE, V10581, DOI 10.1117/12.2293855; Graham Simon, 2019, ARXIV181206499; Hamilton S.R., 2000, Pathology and genetics of tumours of the digestive system, V48; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Javed Sajid, 2018, COMPUTATIONAL PATHOL; Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]; Keenan SJ, 2000, J PATHOL, V192, P351, DOI 10.1002/1096-9896(2000)9999:9999<::AID-PATH708>3.0.CO;2-I; Nguyen K, 2012, PATTERN RECOGN LETT, V33, P951, DOI 10.1016/j.patrec.2011.10.001; Kipf T. N, 2017, P INT C LEARN REPR; Li QM, 2018, AAAI CONF ARTIF INTE, P3538; Micheli A, 2009, IEEE T NEURAL NETWOR, V20, P498, DOI 10.1109/TNN.2008.2010350; Oztan Basak, 2012, MED IMAGING 2012 COM; Paszke A, 2017, PROC 31 INT C NEURAL; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schnorrenberg F, 1996, Technol Health Care, V4, P147; Shaban Muhammad, 2019, ARXIV PREPRINT ARXIV; Shi Jun, GRAPH CONVOLUTIONAL; Sirinukunwattana K, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31799-3; Sirinukunwattana Korsuk, 2018, MED IMAGE COMPUTING; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594, 10.1109/cvpr.2015.7298594, 10.48550/arXiv.1409.4842]; Weyn B, 1999, CYTOMETRY, V35, P23, DOI 10.1002/(SICI)1097-0320(19990101)35:1<23::AID-CYTO4>3.0.CO;2-P; Yener B, 2017, COMMUN ACM, V60, P74, DOI 10.1145/2960404; Ying Zhitao, 2018, ADV NEURAL INFORM PR; Zana F, 2001, IEEE T IMAGE PROCESS, V10, P1010, DOI 10.1109/83.931095; Zhou Yanning, 2019, INT C INF PROC MED I	54	118	129	3	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	2473-9936		978-1-7281-5023-9	IEEE INT CONF COMP V			2019							388	398		10.1109/ICCVW.2019.00050	http://dx.doi.org/10.1109/ICCVW.2019.00050			11	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Imaging Science & Photographic Technology	BP4UH		Green Submitted			2024-09-18	WOS:000554591600044
J	Zhao, TF; Fu, C; Tie, M; Sham, CW; Ma, HF				Zhao, Tengfei; Fu, Chong; Tie, Ming; Sham, Chiu-Wing; Ma, Hongfeng			RGSB-UNet: Hybrid Deep Learning Framework for Tumour Segmentation in Digital Pathology Images	BIOENGINEERING-BASEL			English	Article						hybrid deep learning framework; tumour segmentation; whole slide image; Residual-Ghost-SN; bottleneck transformer		Colorectal cancer (CRC) is a prevalent gastrointestinal tumour with high incidence and mortality rates. Early screening for CRC can improve cure rates and reduce mortality. Recently, deep convolution neural network (CNN)-based pathological image diagnosis has been intensively studied to meet the challenge of time-consuming and labour-intense manual analysis of high-resolution whole slide images (WSIs). Despite the achievements made, deep CNN-based methods still suffer from some limitations, and the fundamental problem is that they cannot capture global features. To address this issue, we propose a hybrid deep learning framework (RGSB-UNet) for automatic tumour segmentation in WSIs. The framework adopts a UNet architecture that consists of the newly-designed residual ghost block with switchable normalization (RGS) and the bottleneck transformer (BoT) for downsampling to extract refined features, and the transposed convolution and 1 x 1 convolution with ReLU for upsampling to restore the feature map resolution to that of the original image. The proposed framework combines the advantages of the spatial-local correlation of CNNs and the long-distance feature dependencies of BoT, ensuring its capacity of extracting more refined features and robustness to varying batch sizes. Additionally, we consider a class-wise dice loss (CDL) function to train the segmentation network. The proposed network achieves state-of-the-art segmentation performance under small batch sizes. Experimental results on DigestPath2019 and GlaS datasets demonstrate that our proposed model produces superior evaluation scores and state-of-the-art segmentation results.	[Zhao, Tengfei; Fu, Chong] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China; [Fu, Chong] Minist Educ, Engn Res Ctr Secur Technol Complex Network Syst, Shenyang 110819, Peoples R China; [Fu, Chong] Northeastern Univ, Minist Educ, Key Lab Intelligent Comp Med Image, Shenyang 110819, Peoples R China; [Tie, Ming] Sci & Technol Space Phys Lab, Beijing 100076, Peoples R China; [Sham, Chiu-Wing] Univ Auckland, Sch Comp Sci, Auckland 1142, New Zealand; [Ma, Hongfeng] Dopamine Grp Ltd, Auckland 1542, New Zealand	Northeastern University - China; Northeastern University - China; University of Auckland	Fu, C (corresponding author), Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China.; Fu, C (corresponding author), Minist Educ, Engn Res Ctr Secur Technol Complex Network Syst, Shenyang 110819, Peoples R China.; Fu, C (corresponding author), Northeastern Univ, Minist Educ, Key Lab Intelligent Comp Med Image, Shenyang 110819, Peoples R China.	fuchong@mail.neu.edu.cn	Sham, Chiu-Wing/C-3819-2014	Sham, Chiu-Wing/0000-0001-7007-6746; Zhao, Tengfei/0000-0002-0692-0815; Fu, Chong/0000-0002-4549-744X	National Natural Science Foundation of China [62032013]; Fundamental Research Funds for the Central Universities [N2224001-7]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This research is supported by the National Natural Science Foundation of China (No.62032013), and the Fundamental Research Funds for the Central Universities (No. N2224001-7).	[Anonymous], 2015, arXiv: Learning; Ba LJ., 2016, arXiv; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cho S, 2021, I S BIOMED IMAGING, P761, DOI 10.1109/ISBI48211.2021.9434105; Cordonnier J.B., 2020, P 8 INT C LEARNING R; Da Q, 2022, MED IMAGE ANAL, V80, DOI 10.1016/j.media.2022.102485; Deng X, 2023, IEEE T IMAGE PROCESS, V32, P1078, DOI 10.1109/TIP.2023.3240024; Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013; Dumoulin V, 2018, Arxiv, DOI arXiv:1603.07285; Feng RW, 2021, IEEE J BIOMED HEALTH, V25, P3700, DOI 10.1109/JBHI.2020.3040269; Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Jin K, 2023, J CLIN MED, V12, DOI 10.3390/jcm12020400; Kumar N, 2020, J DIGIT IMAGING, V33, P1034, DOI 10.1007/s10278-020-00351-z; Kun Fan, 2019, Innovation in Medicine and Healthcare Systems, and Multimedia. Proceedings of KES-InMed-19 and KES-IIMSS-19 Conferences. Smart Innovation, Systems and Technologies (SIST 145), P137, DOI 10.1007/978-981-13-8566-7_13; Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918; Liu Y, 2017, Arxiv, DOI [arXiv:1703.02442, DOI 10.48550/ARXIV.1703.02442]; Luo P., 2019, P 7 INT C LEARNING R; Luo P, 2021, IEEE T PATTERN ANAL, V43, P712, DOI 10.1109/TPAMI.2019.2932062; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Pravitasari A, 2020, TELKOMNIKA Telecommunication, Computing, Electronics and Control, V18, P1310, DOI DOI 10.12928/TELKOMNIKA.V18I3.14753; Qadri SF, 2023, INT J INTELL SYST, V2023, DOI 10.1155/2023/2345835; Qaiser T, 2019, MED IMAGE ANAL, V55, P1, DOI 10.1016/j.media.2019.03.014; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Siegel RL, 2020, CA-CANCER J CLIN, V70, P145, DOI 10.3322/caac.21601; Sirinukunwattana K, 2017, MED IMAGE ANAL, V35, P489, DOI 10.1016/j.media.2016.08.008; Song W, 2022, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03643-6; Song W, 2022, NEURAL COMPUT APPL, V34, P5743, DOI 10.1007/s00521-021-06725-w; Sri S.V., 2021, ASIAN J APPL SCI TEC, V5, P10, DOI [10.38177/ajast.2021.5202, DOI 10.38177/AJAST.2021.5202]; Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625; Vega P, 2015, WORLD J GASTRO ONCOL, V7, P422, DOI 10.4251/wjgo.v7.i12.422; Johnson JW, 2018, Arxiv, DOI arXiv:1805.00500; Wang SD, 2019, AM J PATHOL, V189, P1686, DOI 10.1016/j.ajpath.2019.05.007; Wang SD, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-27707-4; Wang XS, 2021, IEEE DATA MINING, P699, DOI [10.1109/ICPADS53394.2021.00093, 10.1109/ICDM51629.2021.00081]; Wright AM, 2013, ARCH PATHOL LAB MED, V137, P618, DOI 10.5858/arpa.2012-0430-OA; Xia CF, 2022, CHINESE MED J-PEKING, V135, P584, DOI 10.1097/CM9.0000000000002108; Xiong SM, 2023, COMPLEX INTELL SYST, V9, P3399, DOI 10.1007/s40747-022-00926-z; Xu J, 2016, NEUROCOMPUTING, V191, P214, DOI 10.1016/j.neucom.2016.01.034; Zhai Z., 2021, P 2021 CHINESE INTEL, P425; Zhang WC, 2022, NEUROCOMPUTING, V499, P23, DOI 10.1016/j.neucom.2022.05.034; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]; Zhu C, 2021, NEUROCOMPUTING, V438, P165, DOI 10.1016/j.neucom.2020.04.154	47	4	4	3	13	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND		2306-5354		BIOENGINEERING-BASEL	Bioengineering-Basel	AUG	2023	10	8							957	10.3390/bioengineering10080957	http://dx.doi.org/10.3390/bioengineering10080957			16	Biotechnology & Applied Microbiology; Engineering, Biomedical	Science Citation Index Expanded (SCI-EXPANDED)	Biotechnology & Applied Microbiology; Engineering	Q3YG8	37627842	Green Published, gold			2024-09-18	WOS:001056902000001
J	Li, J; Wang, P; Zhou, Y; Liang, H; Luan, K				Li, Jin; Wang, Peng; Zhou, Yang; Liang, Hong; Luan, Kuan			Application of Deep Transfer Learning to the Classification of Colorectal Cancer Lymph Node Metastasis	JOURNAL OF IMAGING SCIENCE AND TECHNOLOGY			English	Article							CONVOLUTIONAL NEURAL-NETWORKS; RECTAL-CANCER; ACCURACY; MRI	Accurate classifications of colorectal cancer (CRC) lymph node metastasis (LNM) could assist radiologists in increasing the diagnostic accuracy and help surgeons establish a correct surgical plan. This study aims to present an efficient pipeline with deep transfer learning for CRC LNM classification. Hence, 11 deep pre-trained models have been investigated on a CRC LN dataset. The dataset of this experiment is from Harbin Medical University Cancer Hospital. This dataset contains samples of 619 patients. Among these samples, 312 were positive and 307 were negative. In addition, datasets with different dimensions and various training epochs were also studied to ascertain the minimum training dataset and training times. In order to improve the interpretability of the model classification performance, a visual convolution layer feature map was first established to compute the similarity distance between the feature map and original data. The experimental results revealed that resnet_152 was the best deep pre-trained model for the classification of CRC LNM, with an accuracy of 97.2%, with 600 raw data samples being the minimum dimension of a dataset and 30 epochs the minimum training times in the CRC LNM classification. This study suggests that the proposed deep transfer learning pipeline could classify the CRC LNM with high efficiency, without requiring sophisticated computational knowledge for radiologists. (C) 2021 Society for Imaging Science and Technology.	[Li, Jin; Wang, Peng; Zhou, Yang; Liang, Hong; Luan, Kuan] Harbin Engn Univ, Automat Coll, Harbin 150001, Heilongjiang, Peoples R China; [Zhou, Yang] Harbin Med Univ, Dept Radiol, Canc Hosp, Harbin 150001, Heilongjiang, Peoples R China	Harbin Engineering University; Harbin Medical University	Luan, K (corresponding author), Harbin Engn Univ, Automat Coll, Harbin 150001, Heilongjiang, Peoples R China.	luankuan@hrbeu.edu.cn			National Natural Science Foundation of China [61773134, 61803117]; Fundamental Research Funds for the Central Universities [HEUCFG201824]; National Key Scientific Instrument and Equipment Development Projects of China [2012YQ04014010]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Key Scientific Instrument and Equipment Development Projects of China(National Key Research & Development Program of China)	The project is partially supported by the National Natural Science Foundation of China (Grant No. 61773134, No. 61803117), the Fundamental Research Funds for the Central Universities (HEUCFG201824), and the National Key Scientific Instrument and Equipment Development Projects of China (2012YQ04014010).	Al-Sukhni E, 2013, ANN SURG ONCOL, V20, P1148, DOI 10.1245/s10434-012-2738-z; [Anonymous], 2018, 2018 IEEE 31 INT S C; Balyasnikova S, 2016, CURR TREAT OPTION ON, V17, DOI 10.1007/s11864-016-0403-7; Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1; Bianchini M, 2014, IEEE T NEUR NET LEAR, V25, P1553, DOI 10.1109/TNNLS.2013.2293637; Bishop C.M., 1995, J. Am. Stat. Assoc, V92, P482; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Bychkov D, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21758-3; CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI [10.1109/CVPR.2017.195, DOI 10.1109/CVPR.2017.195]; Chua TC, 2009, WORLD J SURG, V33, P1488, DOI 10.1007/s00268-009-0059-6; Chuanqi Tan, 2018, Artificial Neural Networks and Machine Learning - ICANN 2018. 27th International Conference on Artificial Neural Networks. Proceedings: Lecture Notes in Computer Science (LNCS 11141), P270, DOI 10.1007/978-3-030-01424-7_27; Ciompi F, 2017, I S BIOMED IMAGING, P160, DOI 10.1109/ISBI.2017.7950492; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Ding L, 2019, CHINESE MED J-PEKING, V132, P379, DOI 10.1097/CM9.0000000000000095; Eldan R., 2016, C LEARN THEOR, P907; Favoriti P, 2016, UPDATES SURG, V68, P7, DOI 10.1007/s13304-016-0359-y; Golatkar A, 2018, LECT NOTES COMPUT SC, V10882, P837, DOI 10.1007/978-3-319-93000-8_95; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Nguyen HV, 2011, LECT NOTES COMPUT SC, V6493, P709; Huang G., 2017, ARXIV160806993V5, P4700, DOI [DOI 10.1109/CVPR.2017.243, 10.1109/CVPR.2017.243]; Ishihara S, 2017, DIS COLON RECTUM, V60, P469, DOI 10.1097/DCR.0000000000000752; Janecek A., 2008, PMLR, P90, DOI DOI 10.1023/A:1010933404324; Karabulut EM, 2012, PROC TECH, V1, P323, DOI 10.1016/j.protcy.2012.02.068; Khosravi P, 2018, EBIOMEDICINE, V27, P317, DOI 10.1016/j.ebiom.2017.12.026; Kim TH, 2008, ANN SURG ONCOL, V15, P729, DOI 10.1245/s10434-007-9696-x; Kong XY, 2018, EBIOMEDICINE, V27, P94, DOI 10.1016/j.ebiom.2017.12.015; Korbar Bruno, 2017, J Pathol Inform, V8, P30, DOI 10.4103/jpi.jpi_34_17; Kumar A, 2017, IEEE J BIOMED HEALTH, V21, P31, DOI 10.1109/JBHI.2016.2635663; KYU KN, 2015, YONSMEDICAL J, V56, P1461; Lambregts DMJ, 2011, ANN SURG, V253, P539, DOI 10.1097/SLA.0b013e31820b01f1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Liu Y., 2018, 2017 10 INT C IM SIG, P1; Liu YF, 2016, ONCOTARGET, V7, P14755, DOI 10.18632/oncotarget.7548; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lu L, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-42999-1; Lu Y, 2018, CANCER RES, V78, P5135, DOI 10.1158/0008-5472.CAN-18-0494; Lu Z, 2017, ADV NEUR IN, V30; Ma L, 2017, PROC SPIE, V10137, DOI 10.1117/12.2255562; Montúfar G, 2014, ADV NEUR IN, V27; Nakaya D., 2017, J COLO ASS JPN, V41, P99; Ong MLH, 2016, WORLD J GASTRO SURG, V8, P179, DOI 10.4240/wjgs.v8.i3.179; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Park JS, 2014, DIS COLON RECTUM, V57, P32, DOI 10.1097/DCR.0000000000000004; Pascanu R., 2013, ARTHRITIS RHEUMATISM, V58, P1823; Raghu M, 2017, PR MACH LEARN RES, V70; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shang WL, 2016, PR MACH LEARN RES, V48; Simard PY, 2003, PROC INT CONF DOC, P958; Simjanoska M, 2014, ADV INTELL SYST, V231, P177, DOI 10.1007/978-3-319-01466-1_17; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Song QZ, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/8314740; Szegedy C., 2015, IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594, 10.1109/cvpr.2015.7298594, 10.48550/arXiv.1409.4842]; SZEGEDY C, 2016, PROC CVPR IEEE, P2818, DOI [DOI 10.1109/CVPR.2016.308, 10.1109/CVPR.2016.308]; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tang Y., 2013, DEEP LEARNING USING; Tytherleigh MG, 2008, ANZ J SURG, V78, P194, DOI 10.1111/j.1445-2197.2007.04402.x; Tzeng E., 2014, ARXIV; Ueno M, 2005, BRIT J SURG, V92, P756, DOI 10.1002/bjs.4975; Vesal S, 2018, LECT NOTES COMPUT SC, V10882, P812, DOI 10.1007/978-3-319-93000-8_92; Yosinski J, 2014, ADV NEUR IN, V27; Zhou J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092779; Zhou L, 2017, EUR REV MED PHARMACO, V21, P1219	66	0	0	2	45	I S & T-SOC IMAGING SCIENCE TECHNOLOGY	SPRINGFIELD	7003 KILWORTH LA, SPRINGFIELD, VA 22151 USA	1062-3701	1943-3522		J IMAGING SCI TECHN	J. Imaging Sci. Technol.	MAY	2021	65	3							030401	10.2352/J.ImagingSci.Technol.2021.65.3.030401	http://dx.doi.org/10.2352/J.ImagingSci.Technol.2021.65.3.030401			15	Imaging Science & Photographic Technology	Science Citation Index Expanded (SCI-EXPANDED)	Imaging Science & Photographic Technology	SO3JN					2024-09-18	WOS:000658872200003
C	Shin, Y; Balasingham, I			IEEE	Shin, Younghak; Balasingham, Ilangko			Comparison of Hand-craft Feature based SVM and CNN based Deep Learning Framework for Automatic Polyp Classification	2017 39TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY (EMBC)	IEEE Engineering in Medicine and Biology Society Conference Proceedings		English	Proceedings Paper	39th Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC)	JUL 11-15, 2017	SOUTH KOREA	IEEE Engn Med & Biol Soc, PubMed, MEDLINE, Korean Soc Med & Biol Engn				Colonoscopy is a standard method for screening polyps by highly trained physicians. Miss-detected polyps in colonoscopy are potential risk factor for colorectal cancer. In this study, we investigate an automatic polyp classification framework. We aim to compare two different approaches named hand-craft feature method and convolutional neural network (CNN) based deep learning method. Combined shape and color features are used for hand craft feature extraction and support vector machine (SVM) method is adopted for classification. For CNN approach, three convolution and pooling based deep learning framework is used for classification purpose. The proposed framework is evaluated using three public polyp databases. From the experimental results, we have shown that the CNN based deep learning framework shows better classification performance than the hand-craft feature based methods. It achieves over 90% of classification accuracy, sensitivity, specificity and precision.	[Shin, Younghak] Norwegian Univ Sci & Technol NTNU, Dept Elect & Telecommun, Trondheim, Norway; [Balasingham, Ilangko] Oslo Univ Hosp, Intervent Ctr, NO-0027 Oslo, Norway; [Balasingham, Ilangko] Univ Oslo, Inst Clin Med, Oslo, Norway; [Balasingham, Ilangko] Norwegian Univ Sci & Technol NTNU, Trondheim, Norway	Norwegian University of Science & Technology (NTNU); University of Oslo; University of Oslo; Norwegian University of Science & Technology (NTNU)	Shin, Y (corresponding author), Norwegian Univ Sci & Technol NTNU, Dept Elect & Telecommun, Trondheim, Norway.	shinyh0919@gmail.com; ilangkob@medisin.uio.no	Balasingham, Ilangko/AGU-7268-2022		European Research Consortium for Informatics and Mathematics (ERCIM) 'Alain Bensoussan' Fellowship Programme; Research Council of Norway through the MELODY project [225885/070]	European Research Consortium for Informatics and Mathematics (ERCIM) 'Alain Bensoussan' Fellowship Programme; Research Council of Norway through the MELODY project(Research Council of Norway)	Y. Shin acknowledges the financial support provided by the European Research Consortium for Informatics and Mathematics (ERCIM) 'Alain Bensoussan' Fellowship Programme. I. Balasingham acknowledges the financial support provided by the Research Council of Norway through the MELODY project under the contract number 225885/070.	Alexandre LA, 2008, INT CONF BIOMED, P38, DOI 10.1109/BMEI.2008.246; Bae SH, 2015, IEEE T MED IMAGING, V34, P2379, DOI 10.1109/TMI.2015.2434398; Chen Y., 2012, DIAGNOSTIC THERAPEUT; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3; Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794; Kingma D. P., 2017, arXiv; Park S., 2015, Polyp detection in colonoscopy videos using deeply-learned hierarchical features; Rabeneck L, 2003, AM J GASTROENTEROL, V98, P1186, DOI 10.1016/S0002-9270(03)00183-7; Wang WT, 2007, I S INTELL SIG PROC, P746	10	63	68	0	12	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1557-170X	1558-4615	978-1-5090-2809-2	IEEE ENG MED BIO			2017							3277	3280						4	Biophysics; Engineering, Biomedical	Conference Proceedings Citation Index - Science (CPCI-S)	Biophysics; Engineering	BJ7CZ	29060597				2024-09-18	WOS:000427085303175
J	Zhang, WY; Lu, FX; Su, HJ; Hu, YW				Zhang, Wenyu; Lu, Fuxiang; Su, Hongjing; Hu, Yawen			Dual-branch multi-information aggregation network with transformer and convolution for polyp segmentation	COMPUTERS IN BIOLOGY AND MEDICINE			English	Article						Polyp segmentation; Information aggregation; CNN; Transformer	CANCER STATISTICS	Computer-Aided Diagnosis (CAD) for polyp detection offers one of the most notable showcases. By using deep learning technologies, the accuracy of polyp segmentation is surpassing human experts. In such CAD process, a critical step is concerned with segmenting colorectal polyps from colonoscopy images. Despite remarkable successes attained by recent deep learning related works, much improvement is still anticipated to tackle challenging cases. For instance, the effects of motion blur and light reflection can introduce significant noise into the image. The same type of polyps has a diversity of size, color and texture. To address such challenges, this paper proposes a novel dual-branch multi-information aggregation network (DBMIA-Net) for polyp segmentation, which is able to accurately and reliably segment a variety of colorectal polyps with efficiency. Specifically, a dual-branch encoder with transformer and convolutional neural networks (CNN) is employed to extract polyp features, and two multi-information aggregation modules are applied in the decoder to fuse multi-scale features adaptively. Two multi-information aggregation modules include global information aggregation (GIA) module and edge information aggregation (EIA) module. In addition, to enhance the representation learning capability of the potential channel feature association, this paper also proposes a novel adaptive channel graph convolution (ACGC). To validate the effectiveness and advantages of the proposed network, we compare it with several state-of-the-art (SOTA) methods on five public datasets. Experimental results consistently demonstrate that the proposed DBMIA-Net obtains significantly superior segmentation performance across six popularly used evaluation matrices. Especially, we achieve 94.12% mean Dice on CVC-ClinicDB dataset which is 4.22% improvement compared to the previous state-of-the-art method PraNet. Compared with SOTA algorithms, DBMIA-Net has a better fitting ability and stronger generalization ability.	[Zhang, Wenyu; Lu, Fuxiang; Su, Hongjing; Hu, Yawen] Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Peoples R China	Lanzhou University	Lu, FX (corresponding author), Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Peoples R China.	lufux@lzu.edu.cn	Zhang, Wenyu/U-7261-2019	Lu, Fuxiang/0000-0002-5810-7631	Fundamental Research Funds for the Central Universities, China [lzujbky-2021-ct09]; NSFC, China [62233003, U22B2040]; Natural Science Foundation of Gansu Province, China [21JR7RA457]	Fundamental Research Funds for the Central Universities, China(Fundamental Research Funds for the Central Universities); NSFC, China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Gansu Province, China	This work is jointly supported by the Fundamental Research Funds for the Central Universities, China (lzujbky-2021-ct09) , NSFC, China (No. 62233003, U22B2040) and Natural Science Foundation of Gansu Province, China (No. 21JR7RA457) .	Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197; Bett Zablon Kipkemoi, 2020, Radiol Case Rep, V15, P2259, DOI 10.1016/j.radcr.2020.08.040; Brandao P, 2017, PROC SPIE, V10134, DOI 10.1117/12.2254361; Chao P, 2019, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2019.00365; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Gelly S., 2021, ICLR, DOI DOI 10.48550/ARXIV.2010.11929; Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304; Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lou AG, 2022, PROC SPIE, V12032, DOI 10.1117/12.2611802; Mou L, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101874; Panayides AS, 2020, IEEE J BIOMED HEALTH, V24, P1837, DOI 10.1109/JBHI.2020.2991043; Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766; Rahim T, 2020, COMPUT MED IMAG GRAP, V85, DOI 10.1016/j.compmedimag.2020.101767; Raymann J, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13010123; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Siegel R, 2014, CA-CANCER J CLIN, V64, P9, DOI 10.3322/caac.21208; Siegel RL, 2020, CA-CANCER J CLIN, V70, P7, DOI [10.3322/caac.21590, 10.3322/caac.21601]; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Srivastava A, 2022, IEEE J BIOMED HEALTH, V26, P2252, DOI 10.1109/JBHI.2021.3138024; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Yin ZJ, 2022, I S BIOMED IMAGING, DOI 10.1109/ISBI52829.2022.9761402; Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320; Zhao XQ, 2021, LECT NOTES COMPUT SC, V12901, P120, DOI 10.1007/978-3-030-87193-2_12; Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681; Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1; Zilong Zhong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13062, DOI 10.1109/CVPR42600.2020.01308	41	1	1	11	18	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0010-4825	1879-0534		COMPUT BIOL MED	Comput. Biol. Med.	JAN	2024	168								107760	10.1016/j.compbiomed.2023.107760	http://dx.doi.org/10.1016/j.compbiomed.2023.107760		DEC 2023	11	Biology; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Mathematical & Computational Biology	Science Citation Index Expanded (SCI-EXPANDED)	Life Sciences & Biomedicine - Other Topics; Computer Science; Engineering; Mathematical & Computational Biology	DX6E2	38064849				2024-09-18	WOS:001135414900001
J	Zhou, J; Hu, N; Huang, ZY; Song, B; Wu, CC; Zeng, FX; Wu, M				Zhou, Jun; Hu, Na; Huang, Zhi-Yin; Song, Bin; Wu, Chun-Cheng; Zeng, Fan-Xin; Wu, Min			Application of artificial intelligence in gastrointestinal disease: a narrative review	ANNALS OF TRANSLATIONAL MEDICINE			English	Review						Artifial intelligence (AI); machine learning (ML); endoscopy; gastrointestinal diseases	HELICOBACTER-PYLORI INFECTION; CONVOLUTIONAL NEURAL-NETWORK; CAPSULE ENDOSCOPY; ULCERATIVE-COLITIS; COLORECTAL-CANCER; GASTRIC-CANCER; DIAGNOSIS; POLYPS; CLASSIFICATION; VALIDATION	Objective: We collected evidence on the application of artificial intelligence (AI) in gastroenterology field. The review was carried out from two aspects of endoscopic types and gastrointestinal diseases, and briefly summarized the challenges and future directions in this field. Background: Due to the advancement of computational power and a surge of available data, a solid foundation has been laid for the growth of AI. Specifically, varied machine learning (ML) techniques have been emerging in endoscopic image analysis. To improve the accuracy and efficiency of clinicians, AI has been widely applied to gastrointestinal endoscopy. Methods: PubMed electronic database was searched using the keywords containing "AI", "ML", "deep learning (DL)", "convolution neural network", "endoscopy (such as white light endoscopy (WLE), narrow band imaging (NBI) endoscopy, magnifying endoscopy with narrow band imaging (ME-NBI), chromoendoscopy, endocytoscopy (EC), and capsule endoscopy (CE))". Search results were assessed for relevance and then used for detailed discussion. Conclusions: This review described the basic knowledge of AI, ML, and DL, and summarizes the application of AI in various endoscopes and gastrointestinal diseases. Finally, the challenges and directions of AI in clinical application were discussed. At present, the application of AI has solved some clinical problems, but more still needs to be done.	[Zhou, Jun; Wu, Min] Sichuan Univ, West China Hosp, Huaxi MR Res Ctr HMRRC, Dept Radiol, Chengdu, Peoples R China; [Zhou, Jun; Zeng, Fan-Xin; Wu, Min] Dazhou Cent Hosp, Dept Clin Res Ctr, Dazhou, Peoples R China; [Hu, Na; Song, Bin] Sichuan Univ, West China Hosp, Dept Radiol, Chengdu, Peoples R China; [Huang, Zhi-Yin; Wu, Chun-Cheng] Sichuan Univ, West China Hosp, Dept Gastroenterol, Chengdu, Peoples R China	Sichuan University; Sichuan University; Sichuan University	Wu, M (corresponding author), Sichuan Univ, West China Hosp, Huaxi MR Res Ctr HMRRC, Dept Radiol, Chengdu, Peoples R China.; Zeng, FX (corresponding author), 56 Nanyuemiao St, Tongchuan Dist, Dazhou, Peoples R China.	zengfx@pku.edu.cn; wuminscu@scu.edu.cn			National Natural Science Foundation of China [81501462, 81902861]; 1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital, Sichuan University; Sichuan Science and Technology Program [2019YJ0116]; Chengdu International Science and Technology Cooperation Fund [2019GH0200074HZ]; Functional and Molecular Imaging Key Laboratory of Sichuan Province [2012JO0011]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital, Sichuan University; Sichuan Science and Technology Program; Chengdu International Science and Technology Cooperation Fund; Functional and Molecular Imaging Key Laboratory of Sichuan Province	This work was supported by the National Natural Science Foundation of China (No. 81501462 and 81902861) ; the 1 center dot 3 center dot 5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital, Sichuan University; the Sichuan Science and Technology Program (No. 2019YJ0116) ; the Chengdu International Science and Technology Cooperation Fund (No. 2019GH0200074HZ) ; and the Functional and Molecular Imaging Key Laboratory of Sichuan Province (No. 2012JO0011) .	Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197; [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]; Bagheri M, 2019, IEEE ENG MED BIO, P6742, DOI [10.1109/EMBC.2019.8856793, 10.1109/embc.2019.8856793]; Billah M, 2017, INT J BIOMED IMAGING, V2017, DOI 10.1155/2017/9545920; Blanes-Vidal V, 2019, ACTA ONCOL, V58, pS29, DOI 10.1080/0284186X.2019.1584404; Bossuyt P, 2020, GUT, V69, P1778, DOI 10.1136/gutjnl-2019-320056; Byrne MF, 2019, GUT, V68, P94, DOI 10.1136/gutjnl-2017-314547; Cai SL, 2019, GASTROINTEST ENDOSC, V90, P745, DOI 10.1016/j.gie.2019.06.044; Chen PJ, 2018, GASTROENTEROLOGY, V154, P568, DOI 10.1053/j.gastro.2017.10.010; Cho BJ, 2020, J CLIN MED, V9, DOI 10.3390/jcm9061858; Cho BJ, 2019, ENDOSCOPY, V51, P1121, DOI 10.1055/a-0981-6133; Ebigbo A, 2019, GUT, V68, P1143, DOI 10.1136/gutjnl-2018-317573; Everson M, 2019, UNITED EUR GASTROENT, V7, P297, DOI 10.1177/2050640618821800; Forrester JD, 2020, J AM COLL SURGEONS, V230, P1098, DOI 10.1016/j.jamcollsurg.2020.03.030; Gong DX, 2020, LANCET GASTROENTEROL, V5, P352, DOI 10.1016/S2468-1253(19)30413-3; Gono K, 2004, J BIOMED OPT, V9, P568, DOI 10.1117/1.1695563; Guimaraes P, 2020, GUT, V69, P4, DOI 10.1136/gutjnl-2019-319347; Guo LJ, 2020, GASTROINTEST ENDOSC, V91, P41, DOI 10.1016/j.gie.2019.08.018; Guo XD, 2019, MED PHYS, V46, P5666, DOI 10.1002/mp.13865; He JY, 2018, IEEE T IMAGE PROCESS, V27, P2379, DOI 10.1109/TIP.2018.2801119; Hirasawa T, 2018, GASTRIC CANCER, V21, P653, DOI 10.1007/s10120-018-0793-2; Horiuchi Y, 2020, DIGEST DIS SCI, V65, P1355, DOI 10.1007/s10620-019-05862-6; Horiuchi Y, 2020, GASTROINTEST ENDOSC, V92, P856, DOI 10.1016/j.gie.2020.04.079; Huang YQ, 2016, J CLIN ONCOL, V34, P2157, DOI 10.1200/JCO.2015.65.9128; Ikenoyama Y, 2021, DIGEST ENDOSC, V33, P141, DOI 10.1111/den.13688; Itoh T, 2018, ENDOSC INT OPEN, V6, pE139, DOI 10.1055/s-0043-120830; Iwagami H, 2021, J GASTROEN HEPATOL, V36, P131, DOI 10.1111/jgh.15136; Jia X, 2016, IEEE ENG MED BIO, P639, DOI 10.1109/EMBC.2016.7590783; Klang E, 2020, GASTROINTEST ENDOSC, V91, P606, DOI 10.1016/j.gie.2019.11.012; Kumagai Y, 2019, ESOPHAGUS-TOKYO, V16, P180, DOI 10.1007/s10388-018-0651-7; Leenhardt R, 2019, GASTROINTEST ENDOSC, V89, P189, DOI 10.1016/j.gie.2018.06.036; Li CF, 2018, CANCER COMMUN, V38, DOI 10.1186/s40880-018-0325-9; Luo HY, 2019, LANCET ONCOL, V20, P1645, DOI 10.1016/S1470-2045(19)30637-0; Machida H, 2004, ENDOSCOPY, V36, P1094, DOI 10.1055/s-2004-826040; Mori Y, 2018, ANN INTERN MED, V169, P357, DOI 10.7326/M18-0249; Nakagawa K, 2019, GASTROINTEST ENDOSC, V90, P407, DOI 10.1016/j.gie.2019.04.245; Nakashima H, 2020, GASTRIC CANCER, V23, P1033, DOI 10.1007/s10120-020-01077-1; Nakashima H, 2018, ANN GASTROENTEROL, V31, P462, DOI 10.20524/aog.2018.0269; Namikawa K, 2020, ENDOSCOPY, V52, P1077, DOI 10.1055/a-1194-8771; Nishiyama S, 2015, J GASTROENTEROL, V50, P1087, DOI 10.1007/s00535-015-1059-y; Ozawa T, 2019, GASTROINTEST ENDOSC, V89, P416, DOI 10.1016/j.gie.2018.10.020; Poon CCY, 2020, NPJ DIGIT MED, V3, DOI 10.1038/s41746-020-0281-z; Sakai Y, 2018, IEEE ENG MED BIO, P4138, DOI 10.1109/EMBC.2018.8513274; Sato H, 2015, ENDOSCOPY, V47, P122, DOI 10.1055/s-0034-1390858; Shichijo S, 2019, SCAND J GASTROENTERO, V54, P158, DOI 10.1080/00365521.2019.1577486; Shichijo S, 2017, EBIOMEDICINE, V25, P106, DOI 10.1016/j.ebiom.2017.10.014; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Takenaka K, 2020, GASTROENTEROLOGY, V158, P2150, DOI 10.1053/j.gastro.2020.02.012; Thorlacius H, 2007, INFLAMM BOWEL DIS, V13, P911, DOI 10.1002/ibd.20118; Tokai Y, 2020, ESOPHAGUS-TOKYO, V17, P250, DOI 10.1007/s10388-020-00716-x; Tsuboi A, 2020, DIGEST ENDOSC, V32, P382, DOI 10.1111/den.13507; Turing AM., 1950, MIND, V59, P433, DOI [10.1093/mind/LIX.236.433, DOI 10.1093/MIND/LIX.236.433]; Ueyama H, 2021, J GASTROEN HEPATOL, V36, P482, DOI 10.1111/jgh.15190; Urban G, 2018, GASTROENTEROLOGY, V155, P1069, DOI 10.1053/j.gastro.2018.06.037; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Wang S, 2019, COMPUT MATH METHOD M, V2019, DOI 10.1155/2019/7546215; WINAWER SJ, 1993, NEW ENGL J MED, V329, P1977, DOI 10.1056/NEJM199312303292701; Wu LL, 2019, ENDOSCOPY, V51, P522, DOI 10.1055/a-0855-3532; Wu X, 2016, IEEE T MED IMAGING, V35, P1741, DOI 10.1109/TMI.2016.2527736; Yamada A, 2021, ENDOSCOPY, V53, P832, DOI 10.1055/a-1266-1066; Yoon HJ, 2019, J CLIN MED, V8, DOI 10.3390/jcm8091310; Yu LQ, 2017, IEEE J BIOMED HEALTH, V21, P65, DOI 10.1109/JBHI.2016.2637004; Yuan YX, 2017, MED PHYS, V44, P1379, DOI 10.1002/mp.12147; Zhang YQ, 2020, DIGEST LIVER DIS, V52, P566, DOI 10.1016/j.dld.2019.12.146; Zhao YY, 2019, ENDOSCOPY, V51, P333, DOI 10.1055/a-0756-8754; Zheng WF, 2019, CLIN TRANSL GASTROEN, V10, DOI 10.14309/ctg.0000000000000109; Zheng YL, 2018, IEEE ENG MED BIO, P4142, DOI 10.1109/EMBC.2018.8513337; Zhou DJ, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16777-6; Zhu Y, 2019, GASTROINTEST ENDOSC, V89, P806, DOI 10.1016/j.gie.2018.11.011	69	7	7	2	34	AME PUBLISHING COMPANY	SHATIN	FLAT-RM C 16F, KINGS WING PLAZA 1, NO 3 KWAN ST, SHATIN, HONG KONG 00000, PEOPLES R CHINA	2305-5839	2305-5847		ANN TRANSL MED	ANN. TRANSL. MED.	JUL	2021	9	14							3001	10.21037/atm-21-3001	http://dx.doi.org/10.21037/atm-21-3001			15	Oncology; Medicine, Research & Experimental	Science Citation Index Expanded (SCI-EXPANDED)	Oncology; Research & Experimental Medicine	UA6OZ	34430629	gold, Green Published			2024-09-18	WOS:000685280900007
J	Abbas, A; Abdelsamea, MM; Gaber, MM				Abbas, Asmaa; Abdelsamea, M. O. H. A. M. M. E. D. M.; Gaber, Mohamed Medhat			DeTrac: Transfer Learning of Class Decomposed Medical Images in Convolutional Neural Networks	IEEE ACCESS			English	Article						Biomedical imaging; Feature extraction; Lung; Task analysis; Machine learning; Computed tomography; Training; Convolution neural networks; class decomposition; data irregularity; medical image classification; transfer learning	CLASSIFICATION; PREDICTION	Due to the high availability of large-scale annotated image datasets, paramount progress has been made in deep convolutional neural networks (CNNs) for image classification tasks. CNNs enable learning highly representative and hierarchical local image features directly from data. However, the availability of annotated data, especially in the medical imaging domain, remains the biggest challenge in the field. Transfer learning can provide a promising and effective solution by transferring knowledge from generic image recognition tasks to the medical image classification. However, due to irregularities in the dataset distribution, transfer learning usually fails to provide a robust solution. Class decomposition facilitates easier to learn class boundaries of a dataset, and consequently can deal with any irregularities in the data distribution. Motivated by this challenging problem, the paper presents Decompose, Transfer, and Compose (DeTraC) approach, a novel CNN architecture based on class decomposition to improve the performance of medical image classification using transfer learning and class decomposition approach. DeTraC enables learning at the subclass level that can be more separable with a prospect to faster convergence. We validated our proposed approach with three different cohorts of chest X-ray images, histological images of human colorectal cancer, and digital mammograms. We compared DeTraC with the state-of-the-art CNN models to demonstrate its high performance in terms of accuracy, sensitivity, and specificity.	[Abbas, Asmaa; Abdelsamea, M. O. H. A. M. M. E. D. M.] Assiut Univ, Fac Sci, Dept Math, Assiut 71511, Egypt; [Abdelsamea, M. O. H. A. M. M. E. D. M.; Gaber, Mohamed Medhat] Birmingham City Univ, Sch Comp & Digital Technol, Birmingham B5 5JU, W Midlands, England	Egyptian Knowledge Bank (EKB); Assiut University; Birmingham City University	Abdelsamea, MM (corresponding author), Assiut Univ, Fac Sci, Dept Math, Assiut 71511, Egypt.; Abdelsamea, MM (corresponding author), Birmingham City Univ, Sch Comp & Digital Technol, Birmingham B5 5JU, W Midlands, England.	mohamrned.abdelsamea@bcu.ac.uk	Abdelsamea, Mohammed/AAF-1031-2019; Abbas, Asmaa/AAU-6110-2020	Abdelsamea, Mohammed M./0000-0002-2728-1127; Gaber, Mohamed/0000-0003-0339-4474				Abbas A, 2018, PROCEEDINGS OF 2018 13TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND SYSTEMS (ICCES), P122, DOI 10.1109/ICCES.2018.8639200; Abdallah Z. Said, 2011, Proceedings 2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM 2011), P283, DOI 10.1109/CIDM.2011.5949435; Akbani R, 2004, LECT NOTES COMPUT SC, V3201, P39, DOI 10.1007/978-3-540-30115-8_7; Amanatiadis A, 2009, MEAS SCI TECHNOL, V20, DOI 10.1088/0957-0233/20/10/104015; [Anonymous], 2004, ACM SIGKDD EXPLOR NE, DOI DOI 10.1145/1007730.1007733; Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865; Bi J, 2019, IEEE T AUTOM SCI ENG, V16, P1763, DOI 10.1109/TASE.2019.2895801; Bi J, 2019, INFORM SCIENCES, V481, P57, DOI 10.1016/j.ins.2018.12.027; Bi J, 2018, COMPUT IND, V97, P76, DOI 10.1016/j.compind.2018.01.021; Candemir S, 2014, IEEE T MED IMAGING, V33, P577, DOI 10.1109/TMI.2013.2290491; Cheng JZ, 2016, SCI REP-UK, V6, DOI [10.1038/srep24454, 10.1038/srep25671]; Dandil E, 2014, 2014 6TH INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION (SOCPAR), P382, DOI 10.1109/SOCPAR.2014.7008037; El-Regaily SA, 2018, CURR MED IMAGING, V14, P3, DOI 10.2174/1573405613666170602123329; Elyan E, 2017, INFORM SCIENCES, V384, P220, DOI 10.1016/j.ins.2016.08.007; Elyan E, 2016, NEURAL COMPUT APPL, V27, P2279, DOI 10.1007/s00521-015-2064-z; Fradkin D, 2008, PROC INT C TOOLS ART, P439, DOI 10.1109/ICTAI.2008.29; Gangeh MJ, 2010, LECT NOTES COMPUT SC, V6363, P595; Gao MC, 2018, COMP M BIO BIO E-IV, V6, P1, DOI 10.1080/21681163.2015.1124249; Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018; Govindarajan S, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1222-8; Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013; Hattikatti P, 2017, 2017 INTERNATIONAL CONFERENCE ON BIG DATA, IOT AND DATA SCIENCE (BID), P18, DOI 10.1109/BID.2017.8336567; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hua KL, 2015, ONCOTARGETS THER, V8, P2015, DOI 10.2147/OTT.S80733; Jaeger S, 2014, IEEE T MED IMAGING, V33, P233, DOI 10.1109/TMI.2013.2284099; Karargyris A, 2016, INT J COMPUT ASS RAD, V11, P99, DOI 10.1007/s11548-015-1242-x; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Ker J, 2018, IEEE ACCESS, V6, P9375, DOI 10.1109/ACCESS.2017.2788044; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuruvilla J, 2014, COMPUT METH PROG BIO, V113, P202, DOI 10.1016/j.cmpb.2013.10.011; Lakshmanaprabu SK, 2019, FUTURE GENER COMP SY, V92, P374, DOI 10.1016/j.future.2018.10.009; Lam JH, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-12920-0; Li Q, 2014, I C CONT AUTOMAT ROB, P844, DOI 10.1109/ICARCV.2014.7064414; Li Y, 2019, METHODS, V166, P4, DOI 10.1016/j.ymeth.2019.04.008; Li Y, 2018, BIOINFORMATICS, V34, P760, DOI 10.1093/bioinformatics/btx680; Manikandan T, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0539-9; Monkam P, 2018, BIOMED ENG ONLINE, V17, DOI 10.1186/s12938-018-0529-x; Negahdar M, 2019, PROC SPIE, V10950, DOI 10.1117/12.2513044; Nibali A, 2017, INT J COMPUT ASS RAD, V12, P1799, DOI 10.1007/s11548-017-1605-6; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Polaka I., 2014, TECHNOL EC DEV EC, V16, P765; Polaka I, 2013, APPL INF COMMUN TEC, P29; Qin CL, 2018, BIOMED ENG ONLINE, V17, DOI 10.1186/s12938-018-0544-y; Qureshi AS, 2017, APPL SOFT COMPUT, V58, P742, DOI 10.1016/j.asoc.2017.05.031; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Sangamithraa PB, 2016, PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, SIGNAL PROCESSING AND NETWORKING (WISPNET), P2201, DOI 10.1109/WiSPNET.2016.7566533; Shen SW, 2019, EXPERT SYST APPL, V128, P84, DOI 10.1016/j.eswa.2019.01.048; Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002; Sorensen L, 2010, IEEE T MED IMAGING, V29, P559, DOI 10.1109/TMI.2009.2038575; Stefanowski Jerzy., 2013, Emerging paradigms in machine learning, P277; SUCKLING J, 1994, INT CONGR SER, V1069, P375; Sun WQ, 2015, PROC SPIE, V9785, DOI 10.1117/12.2216307; Sun YM, 2007, PATTERN RECOGN, V40, P3358, DOI 10.1016/j.patcog.2007.04.009; Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302; Tang RX, 2019, PROC SPIE, V10950, DOI 10.1117/12.2513576; Vilalta R, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P673; Vilalta R, 2003, LECT NOTES ARTIF INT, V2837, P444; Wang CM, 2018, BIOMED ENG ONLINE, V17, DOI 10.1186/s12938-018-0496-2; Wang GM, 2019, IEEE T AUTOM SCI ENG, V16, P874, DOI 10.1109/TASE.2018.2865663; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Wu JJ, 2010, DATA MIN KNOWL DISC, V20, P191, DOI 10.1007/s10618-009-0146-1; Wu S., 2020, ACM Computing Surveys; Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2; Xie HT, 2019, PATTERN RECOGN, V85, P109, DOI 10.1016/j.patcog.2018.07.031; Zhang GB, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1327-0; Zou ZZ, 2019, FRONT GENET, V9, DOI 10.3389/fgene.2018.00714	70	54	56	3	30	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2169-3536			IEEE ACCESS	IEEE Access		2020	8						74901	74913		10.1109/ACCESS.2020.2989273	http://dx.doi.org/10.1109/ACCESS.2020.2989273			13	Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Telecommunications	LK4KW		Green Accepted, gold			2024-09-18	WOS:000530830800112
J	Sharma, P; Balabantaray, BK; Bora, K; Mallik, S; Kasugai, K; Zhao, ZM				Sharma, Pallabi; Balabantaray, Bunil Kumar; Bora, Kangkana; Mallik, Saurav; Kasugai, Kunio; Zhao, Zhongming			An Ensemble-Based Deep Convolutional Neural Network for Computer-Aided Polyps Identification From Colonoscopy	FRONTIERS IN GENETICS			English	Article						colorectal cancer; deep learning; polyp detection; colonoscopy; ensemble classifier	CLASSIFICATION; VALIDATION; LESIONS	Colorectal cancer (CRC) is the third leading cause of cancer death globally. Early detection and removal of precancerous polyps can significantly reduce the chance of CRC patient death. Currently, the polyp detection rate mainly depends on the skill and expertise of gastroenterologists. Over time, unidentified polyps can develop into cancer. Machine learning has recently emerged as a powerful method in assisting clinical diagnosis. Several classification models have been proposed to identify polyps, but their performance has not been comparable to an expert endoscopist yet. Here, we propose a multiple classifier consultation strategy to create an effective and powerful classifier for polyp identification. This strategy benefits from recent findings that different classification models can better learn and extract various information within the image. Therefore, our Ensemble classifier can derive a more consequential decision than each individual classifier. The extracted combined information inherits the ResNet's advantage of residual connection, while it also extracts objects when covered by occlusions through depth-wise separable convolution layer of the Xception model. Here, we applied our strategy to still frames extracted from a colonoscopy video. It outperformed other state-of-the-art techniques with a performance measure greater than 95% in each of the algorithm parameters. Our method will help researchers and gastroenterologists develop clinically applicable, computational-guided tools for colonoscopy screening. It may be extended to other clinical diagnoses that rely on image.				zhongming.zhao@uth.tmc.edu	Balabantaray, Bunil/M-9711-2013; Mallik, Saurav/D-1608-2014	SHARMA, PALLABI/0000-0003-3447-9251; Mallik, Saurav/0000-0003-4107-6784				Akbari M, 2018, IEEE ENG MED BIO, P65, DOI 10.1109/EMBC.2018.8512226; Ayaz M, 2021, PHYS ENG SCI MED, V44, P183, DOI 10.1007/s13246-020-00966-0; Bandyopadhyay S, 2014, IEEE ACM T COMPUT BI, V11, P95, DOI 10.1109/TCBB.2013.147; Bedrikovetski S, 2021, ARTIF INTELL MED, V113, DOI 10.1016/j.artmed.2021.102022; Bernal J, 2017, IEEE T MED IMAGING, V36, P1231, DOI 10.1109/TMI.2017.2664042; Bolón-Canedo V, 2012, PATTERN RECOGN, V45, P531, DOI 10.1016/j.patcog.2011.06.006; Bose SSC, 2021, J AMB INTEL HUM COMP, V12, P6713, DOI 10.1007/s12652-020-02295-2; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Demsar J, 2006, J MACH LEARN RES, V7, P1; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Graham S, 2019, MED IMAGE ANAL, V52, P199, DOI 10.1016/j.media.2018.12.001; Häfner M, 2015, MED IMAGE ANAL, V26, P92, DOI 10.1016/j.media.2015.08.007; Huang GL, 2017, IEEE ICC; Hussain E, 2020, TISSUE CELL, V65, DOI 10.1016/j.tice.2020.101347; Kudo SE, 1996, GASTROINTEST ENDOSC, V44, P8, DOI 10.1016/S0016-5107(96)70222-5; Liew WS, 2021, COMPUT METH PROG BIO, V206, DOI 10.1016/j.cmpb.2021.106114; Mahfouz MA, 2021, ARTIF INTELL MED, V111, DOI 10.1016/j.artmed.2020.101985; McNemar Q, 1947, PSYCHOMETRIKA, V12, P153, DOI 10.1007/BF02295996; Mesejo P, 2016, IEEE T MED IMAGING, V35, P2051, DOI 10.1109/TMI.2016.2547947; Oh J, 2007, MED IMAGE ANAL, V11, P110, DOI 10.1016/j.media.2006.10.003; Pacal I, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104003; Patino-Barrientos S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020501; Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P164, DOI 10.1145/3083187.3083212; Ribeiro E, 2016, COMPUT MATH METHOD M, V2016, DOI 10.1155/2016/6584725; Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002; Sánchez-Peralta LF, 2020, ARTIF INTELL MED, V108, DOI 10.1016/j.artmed.2020.101923; Sevo I, 2016, COMPUT BIOL MED, V72, P138, DOI 10.1016/j.compbiomed.2016.03.017; Shakeel PM, 2022, NEURAL COMPUT APPL, V34, P9579, DOI 10.1007/s00521-020-04842-6; Sharif MS, 2020, IEEE ACCESS, V8, P37482, DOI 10.1109/ACCESS.2020.2975135; Sharma P, 2020, ONCOLOGIE, V22, P129, DOI 10.32604/oncologie.2020.013870; Sharma P, 2020, 2020 INTERNATIONAL CONFERENCE ON COMPUTATIONAL PERFORMANCE EVALUATION (COMPE-2020), P316, DOI 10.1109/ComPE49325.2020.9200003; Shin Y, 2017, IEEE ENG MED BIO, P3277, DOI 10.1109/EMBC.2017.8037556; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Soh NYT, 2018, INT J COLORECTAL DIS, V33, P991, DOI 10.1007/s00384-018-3039-1; Sornapudi S, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9122404; Sun ZB, 2015, PATTERN RECOGN, V48, P1623, DOI 10.1016/j.patcog.2014.11.014; Sundaram PS, 2019, J MED SYST, V43, DOI 10.1007/s10916-018-1153-9; Szegedy C., 2015, IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.1109/CVPR.2015.7298594, 10.1109/cvpr.2015.7298594, 10.48550/arXiv.1409.4842]; Urban G, 2018, GASTROENTEROLOGY, V155, P1069, DOI 10.1053/j.gastro.2018.06.037; Wang P, 2018, NAT BIOMED ENG, V2, P741, DOI 10.1038/s41551-018-0301-3; Wimmer G, 2016, MED IMAGE ANAL, V31, P16, DOI 10.1016/j.media.2016.02.001; Yang C, 2020, INT J COLORECTAL DIS, V35, P101, DOI 10.1007/s00384-019-03455-3; Yang JJ, 2016, COMPUT METH PROG BIO, V124, P45, DOI 10.1016/j.cmpb.2015.10.007; Zhang RK, 2017, IEEE J BIOMED HEALTH, V21, P41, DOI 10.1109/JBHI.2016.2635662	44	22	22	4	8	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND		1664-8021		FRONT GENET	Front. Genet.	APR 26	2022	13								844391	10.3389/fgene.2022.844391	http://dx.doi.org/10.3389/fgene.2022.844391			11	Genetics & Heredity	Science Citation Index Expanded (SCI-EXPANDED)	Genetics & Heredity	1V2RU	35559018	gold, Green Published			2024-09-18	WOS:000805944100001
J	Theodosi, A; Ouzounis, S; Kostopoulos, S; Glotsos, D; Kalatzis, I; Tzelepi, V; Ravazoula, P; Asvestas, P; Cavouras, D; Sakellaropoulos, G				Theodosi, Angeliki; Ouzounis, Sotiris; Kostopoulos, Spiros; Glotsos, Dimitris; Kalatzis, Ioannis; Tzelepi, Vassiliki; Ravazoula, Panagiota; Asvestas, Pantelis; Cavouras, Dionisis; Sakellaropoulos, George			Design of a hybrid deep learning system for discriminating between low- and high-grade colorectal cancer lesions, using microscopy images of IHC stained for AIB1 expression biopsy material	MACHINE VISION AND APPLICATIONS			English	Article						Machine learning; Deep learning; Colorectal carcinoma; Immunohistochemistry	RECEPTOR COACTIVATOR GENE; ARCHITECTURAL MORPHOMETRY; ULCERATIVE-COLITIS; TEXTURAL FEATURES; BOWEL-CANCER; CLASSIFICATION; AMPLIFICATION; OVEREXPRESSION; ADENOCARCINOMA; SEGMENTATION	To design a hybrid deep learning system (hDL-system) for discriminating low-grade from high-grade colorectal cancer (CRC) lesions, using immunohistochemically stained biopsy specimens for AIB1 expression. AIB1 has oncogenic function in tumour genesis, and it is an important prognostic factor regarding various types of cancers, including CRC. Clinical material consisted of biopsy specimens of sixty-seven patients with verified CRC (26 low-grade, 41 high-grade cases). From each patient, we digitized images, at x 50 and x 200 lens magnifications. We designed the hDL-system, employing the VGG16 pre-trained convolution neural network for generating DL-features, the SVM classifier, and the bootstrap evaluation method for assessing the discrimination accuracy between low-grade and high-grade CRC lesions. Furthermore, we compared the hDL-system's discrimination accuracy with that of a supervised machine learning system (sML-system). We designed the sML-system by (i) generating sixty-nine (69) textural and colour features from each image, (ii) employing the probabilistic neural network (PNN) classifier, and (iii) using the bootstrapping method for evaluating sML-system performance. The system design was enabled by employing the CUDA platform for programming in parallel the multiprocessors of the Nvidia graphics processing unit card. The hDL-system provided the highest discrimination accuracy of 99.1% using the x 200 lens magnification images as compared to the 92.5.% best accuracy achieved by the sML-system, employing both the x 50 and x 200 lens magnification images. Our results showed that the hDL-system was superior to the sML-system (i) in discriminating low-grade from high-grade CRC-lesions and (ii) by requiring fewer images for its best design, only those at the x 200 lens magnification. The sML-system by employing textural and colour features in its design revealed that high-grade CRC lesions are characterized by (i) loss in the definition of structures, (ii) coarser texture in larger structures, (iii) hazy formless texture, (iv) lower AIB1 uptake, (v) lower local correlation and (vi) slower varying image contrast.	[Theodosi, Angeliki; Sakellaropoulos, George] Univ Patras, Fac Med, Sch Hlth Sci, Dept Med Phys, Patras, Greece; [Ouzounis, Sotiris; Kostopoulos, Spiros; Glotsos, Dimitris; Kalatzis, Ioannis; Asvestas, Pantelis; Cavouras, Dionisis] Univ West Attica, Dept Biomed Engn, Med Image & Signal Proc Lab, Athens, Greece; [Tzelepi, Vassiliki; Ravazoula, Panagiota] Univ Hosp Patras, Dept Pathol, Patras, Greece	University of Patras; University of West Attica; University of Patras	Kostopoulos, S (corresponding author), Univ West Attica, Dept Biomed Engn, Med Image & Signal Proc Lab, Athens, Greece.	skostopoulos@uniwa.gr	Kalatzis, Ioannis/AAP-4418-2021; Cavouras, Dionisis/HRB-9563-2023; Kostopoulos, Spiros/AAP-7438-2021; Asvestas, Pantelis/U-8912-2019; Glo, Dim/U-9028-2019; Ouzounis, Sotiris/HTR-3869-2023	Glotsos, Dimitris/0000-0003-3252-4055; OUZOUNIS, SOTIRIS/0000-0001-7541-6885; Kostopoulos, Spiros/0000-0003-0223-2502; Kalatzis, Ioannis/0000-0002-2192-3520				ALLEN DC, 1987, HISTOPATHOLOGY, V11, P913, DOI 10.1111/j.1365-2559.1987.tb01898.x; ALLEN DC, 1988, HISTOPATHOLOGY, V12, P611, DOI 10.1111/j.1365-2559.1988.tb01985.x; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; ANDRION A, 1995, J CLIN PATHOL, V48, P856, DOI 10.1136/jcp.48.9.856; [Anonymous], 2002, AJCC CANC STAGING MA, DOI DOI 10.1007/978-1-4757-3656-4; Anzick SL, 1997, SCIENCE, V277, P965, DOI 10.1126/science.277.5328.965; Awan R, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16516-w; BLENKINSOPP WK, 1981, J CLIN PATHOL, V34, P509, DOI 10.1136/jcp.34.5.509; Chaddad A, 2017, ANAL CELL PATHOL, V2017, P1, DOI 10.1155/2017/8428102; Chaddad A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0149893; Chen LH, 2016, J CANCER, V7, P2052, DOI 10.7150/jca.16069; Cheng HD, 2000, IEEE T IMAGE PROCESS, V9, P2071, DOI 10.1109/83.887975; Esgiar AN, 2002, IEEE T INF TECHNOL B, V6, P54, DOI 10.1109/4233.992163; Esgiar AN, 1998, ANAL QUANT CYTOL, V20, P297; Ficsor L, 2008, CYTOM PART A, V73A, P230, DOI 10.1002/cyto.a.20527; FOLEY DH, 1972, IEEE T INFORM THEORY, V18, P618, DOI 10.1109/TIT.1972.1054863; FREEDMAN LS, 1984, LANCET, V2, P733; Galloway MM., 1975, COMPUT GRAPHICS IMAG, V4, DOI [10.1016/S0146-664X(75)80008-6, DOI 10.1016/S0146-664X(75)80008-6]; Ghadimi BM, 1999, AM J PATHOL, V154, P525, DOI 10.1016/S0002-9440(10)65298-4; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; HAMILTON PW, 1987, HISTOPATHOLOGY, V11, P901, DOI 10.1111/j.1365-2559.1987.tb01897.x; HAMILTON PW, 1990, HISTOPATHOLOGY, V17, P59, DOI 10.1111/j.1365-2559.1990.tb00664.x; Hamilton PW, 1997, J PATHOL, V182, P68; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Jass J R, 2002, Histopathology, V41, P59; Jiao L., 2013, WORLD C MED PHYS BIO, P1283; Kalkan H, 2012, INT C PATT RECOG, P61; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Kumar Rajesh, 2015, J Med Eng, V2015, P457906, DOI 10.1155/2015/457906; Kunhoth S., 2015, INT J LIFE SCI BIOTE, V4, P122; Liu MZ, 2008, AM J CLIN PATHOL, V129, P728, DOI 10.1309/QMDTL82JKEX6E7H2; Masood K, 2009, I S BIOMED IMAGING, P1011, DOI 10.1109/ISBI.2009.5193226; Pacal I, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104003; Peyret R, 2018, NEUROCOMPUTING, V275, P83, DOI 10.1016/j.neucom.2017.05.010; PHILLIPS RKS, 1984, BRIT J SURG, V71, P604, DOI 10.1002/bjs.1800710813; Rathore S, 2015, COMPUT BIOL MED, V65, P279, DOI 10.1016/j.compbiomed.2015.03.004; Rathore S, 2014, COMPUT BIOL MED, V47, P76, DOI 10.1016/j.compbiomed.2013.12.010; Rathore S, 2013, IEEE ACM T COMPUT BI, V10, P545, DOI 10.1109/TCBB.2013.84; Sakakura C, 2000, INT J CANCER, V89, P217; Sakellaropoulos G., 2018, APPL IMMUNOHISTO M M; Shaban M, 2020, IEEE T MED IMAGING, V39, P2395, DOI 10.1109/TMI.2020.2971006; Sidiropoulos K, 2012, COMPUT BIOL MED, V42, P376, DOI 10.1016/j.compbiomed.2011.12.004; Siegel RL, 2017, CA-CANCER J CLIN, V67, P177, DOI 10.3322/caac.21395; Simonyan K., 2015, INT C LEARN REPR; SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q; TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999; Theodoridis S., 2010, Introduction to Pattern Recognition: A MATLAB Approach; Theodoridis S., 2003, PATTERN RECOGN; THOMAS GDH, 1983, J CLIN PATHOL, V36, P385, DOI 10.1136/jcp.36.4.385; Turner JK, 2013, HISTOPATHOLOGY, V62, P916, DOI 10.1111/his.12110; Tzelepi V, 2009, VIRCHOWS ARCH, V454, P389, DOI 10.1007/s00428-009-0740-z; Wang Y, 2002, CANCER-AM CANCER SOC, V95, P2346, DOI 10.1002/cncr.10963; Xie D, 2005, HUM PATHOL, V36, P777, DOI 10.1016/j.humpath.2005.05.007; Xu FP, 2007, CANCER LETT, V245, P69, DOI 10.1016/j.canlet.2005.12.030; Xu Y, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1685-x; Yan Xu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1626, DOI 10.1109/ICASSP.2014.6853873; Zhou YN, 2019, IEEE INT CONF COMP V, P388, DOI 10.1109/ICCVW.2019.00050	57	5	5	0	5	SPRINGER	NEW YORK	ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES	0932-8092	1432-1769		MACH VISION APPL	Mach. Vis. Appl.	MAR 11	2021	32	3							58	10.1007/s00138-021-01184-8	http://dx.doi.org/10.1007/s00138-021-01184-8			17	Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QX8QH					2024-09-18	WOS:000629607600001
J	Lin, AQ; Qi, C; Li, MJ; Guan, R; Imyanitov, EN; Mitiushkina, NV; Cheng, Q; Liu, ZQ; Wang, XJ; Lyu, QW; Zhang, J; Luo, P				Lin, Anqi; Qi, Chang; Li, Mujiao; Guan, Rui; Imyanitov, Evgeny N.; Mitiushkina, Natalia V.; Cheng, Quan; Liu, Zaoqu; Wang, Xiaojun; Lyu, Qingwen; Zhang, Jian; Luo, Peng			Deep Learning Analysis of the Adipose Tissue and the Prediction of Prognosis in Colorectal Cancer	FRONTIERS IN NUTRITION			English	Article						deep learning; adipose tissue; colorectal cancer; prognosis; hematoxylin and eosin	BODY-MASS INDEX; INFLAMMATION; CELLS; MICROENVIRONMENT; OUTCOMES; LEPTIN; MODEL; MICE	Research has shown that the lipid microenvironment surrounding colorectal cancer (CRC) is closely associated with the occurrence, development, and metastasis of CRC. According to pathological images from the National Center for Tumor diseases (NCT), the University Medical Center Mannheim (UMM) database and the ImageNet data set, a model called VGG19 was pre-trained. A deep convolutional neural network (CNN), VGG19CRC, was trained by the migration learning method. According to the VGG19CRC model, adipose tissue scores were calculated for TCGA-CRC hematoxylin and eosin (H&E) images and images from patients at Zhujiang Hospital of Southern Medical University and First People's Hospital of Chenzhou. Kaplan-Meier (KM) analysis was used to compare the overall survival (OS) of patients. The XCell and MCP-Counter algorithms were used to evaluate the immune cell scores of the patients. Gene set enrichment analysis (GSEA) and single-sample GSEA (ssGSEA) were used to analyze upregulated and downregulated pathways. In TCGA-CRC, patients with high-adipocytes (high-ADI) CRC had significantly shorter OS times than those with low-ADI CRC. In a validation queue from Zhujiang Hospital of Southern Medical University (Local-CRC1), patients with high-ADI had worse OS than CRC patients with low-ADI. In another validation queue from First People's Hospital of Chenzhou (Local-CRC2), patients with low-ADI CRC had significantly longer OS than patients with high-ADI CRC. We developed a deep convolution network to segment various tissues from pathological H&E images of CRC and automatically quantify ADI. This allowed us to further analyze and predict the survival of CRC patients according to information from their segmented pathological tissue images, such as tissue components and the tumor microenvironment.	[Lin, Anqi; Qi, Chang; Guan, Rui; Zhang, Jian; Luo, Peng] Southern Med Univ, Zhujiang Hosp, Dept Oncol, Guangzhou, Peoples R China; [Li, Mujiao] Southern Med Univ, Coll Biomed Engn, Guangzhou, Peoples R China; [Li, Mujiao; Lyu, Qingwen] Southern Med Univ, Zhujiang Hosp, Dept Informat, Guangzhou, Peoples R China; [Imyanitov, Evgeny N.; Mitiushkina, Natalia V.] NN Petrov Inst Oncol, Dept Tumor Growth Biol, St Petersburg, Russia; [Cheng, Quan] Cent South Univ, Xiangya Hosp, Dept Neurosurg, Changsha, Peoples R China; [Liu, Zaoqu] Zhengzhou Univ, Dept Intervent Radiol, Affiliated Hosp 1, Zhengzhou, Peoples R China; [Wang, Xiaojun] First Peoples Hosp Chenzhou City, Chenzhou, Peoples R China	Southern Medical University - China; Southern Medical University - China; Southern Medical University - China; N.N. Petrov Research Institute of Oncology; Central South University; Zhengzhou University	Zhang, J; Luo, P (corresponding author), Southern Med Univ, Zhujiang Hosp, Dept Oncol, Guangzhou, Peoples R China.; Lyu, QW (corresponding author), Southern Med Univ, Zhujiang Hosp, Dept Informat, Guangzhou, Peoples R China.	gzbeer@smu.edu.cn; zhangjian@i.smu.edu.cn; luopeng@smu.edu.cn	Luo, Peng/I-4790-2019; Liu, Zaoqu/AAV-9348-2021; zhao, wenqing/KEZ-9488-2024; Wu, Yongming/IZE-2454-2023; Wang, Xiaojun/I-5025-2017; Li, Mujiao/JFL-0678-2023; CHENG, QUAN/AAJ-6264-2021; Mitiushkina, Natalia/N-4855-2016; Luo, Peng/C-5323-2017	Zhang, Jian/0000-0001-7217-0111; Cheng, Quan/0000-0003-2401-5349; Luo, Peng/0000-0002-8215-2045	Natural Science Foundation of Guangdong Province [2018A030313846, 2021A1515012593]; Science and Technology Planning Project of Guangdong Province [2019A030317020]; National Natural Science Foundation of China [81802257, 81871859, 81772457, 82172750, 82172811]	Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); Science and Technology Planning Project of Guangdong Province; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	Funding This work was supported by the Natural Science Foundation of Guangdong Province (Grant Nos. 2018A030313846 and 2021A1515012593), the Science and Technology Planning Project of Guangdong Province (Grant No. 2019A030317020), and the National Natural Science Foundation of China (Grant Nos. 81802257, 81871859, 81772457, 82172750, and 82172811).	Aguirre-Portolés C, 2017, NUTRIENTS, V9, DOI 10.3390/nu9101076; Aran D, 2017, GENOME BIOL, V18, DOI 10.1186/s13059-017-1349-1; Becht E, 2016, GENOME BIOL, V17, DOI 10.1186/s13059-016-1070-5; Blain H, 2010, J NUTR HEALTH AGING, V14, P595, DOI 10.1007/s12603-010-0111-0; Cabrero-de las Heras S, 2018, WORLD J GASTROENTERO, V24, P4738, DOI 10.3748/wjg.v24.i42.4738; Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51; Coupland LA, 2014, SEMIN ONCOL, V41, P422, DOI 10.1053/j.seminoncol.2014.04.003; Coussens LM, 2002, NATURE, V420, P860, DOI 10.1038/nature01322; Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346; Danielsen HE, 2018, ANN ONCOL, V29, P616, DOI 10.1093/annonc/mdx794; de Guevara DL, 2019, REV MED CHILE, V147, P828, DOI 10.4067/S0034-98872019000700828; de Hollander EL, 2012, J NUTR HEALTH AGING, V16, P100, DOI 10.1007/s12603-011-0077-6; Del Prete A, 2011, BIOCHEM MEDICA, V21, P264; Dignam JJ, 2006, J NATL CANCER I, V98, P1647, DOI 10.1093/jnci/djj442; Dmitrieva OS, 2016, BIOCHEMISTRY-MOSCOW+, V81, P80, DOI 10.1134/S0006297916020024; Emile JF, 2017, EUR J CANCER, V82, P16, DOI 10.1016/j.ejca.2017.04.025; Gale CR, 2007, INT J EPIDEMIOL, V36, P228, DOI 10.1093/ije/dyl224; Grizzi F, 2018, INFLAMM RES, V67, P375, DOI 10.1007/s00011-017-1128-1; Gu ZG, 2016, BIOINFORMATICS, V32, P2847, DOI 10.1093/bioinformatics/btw313; Guaita-Esteruelas S, 2018, MOL CELL ENDOCRINOL, V462, P107, DOI 10.1016/j.mce.2017.02.002; Hänzelmann S, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-7; Haydon AMM, 2006, GUT, V55, P62, DOI 10.1136/gut.2005.068189; Healy LA, 2012, COLORECTAL DIS, V14, P157, DOI 10.1111/j.1463-1318.2011.02562.x; Huang ZX, 2020, MEDICINE, V99, DOI 10.1097/MD.0000000000019428; Källén H, 2016, I S BIOMED IMAGING, P1163, DOI 10.1109/ISBI.2016.7493473; Kather JN, 2019, PLOS MED, V16, DOI 10.1371/journal.pmed.1002730; Kawamura J, 2018, CANCER SCI, V109, P1545, DOI 10.1111/cas.13547; Korbar Bruno, 2017, J Pathol Inform, V8, P30, DOI 10.4103/jpi.jpi_34_17; Kothari S, 2013, J AM MED INFORM ASSN, V20, P1099, DOI 10.1136/amiajnl-2012-001540; Kozak MM, 2017, AM J CLIN ONCOL-CANC, V40, P405, DOI 10.1097/COC.0000000000000183; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuper H, 2001, J INTERN MED, V249, P61, DOI 10.1046/j.1365-2796.2001.00742.x; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li M, 2020, AGING-US, V12, P1285, DOI 10.18632/aging.102683; Liberzon A, 2011, BIOINFORMATICS, V27, P1739, DOI 10.1093/bioinformatics/btr260; Lin AQ, 2020, FRONT IMMUNOL, V11, DOI 10.3389/fimmu.2020.02039; Lin AQ, 2019, MOL CANCER, V18, DOI 10.1186/s12943-019-1062-7; Linnekamp JF, 2015, CANCER RES, V75, P245, DOI 10.1158/0008-5472.CAN-14-2240; Matsutani S, 2018, ANTICANCER RES, V38, P6721, DOI 10.21873/anticanres.13041; Nieswandt B, 1999, CANCER RES, V59, P1295; Nigro E, 2018, MOL CELL BIOCHEM, V448, P125, DOI 10.1007/s11010-018-3319-7; Nimri L, 2015, ONCOTARGET, V6, P38195, DOI 10.18632/oncotarget.5561; Nozawa H, 2006, P NATL ACAD SCI USA, V103, P12493, DOI 10.1073/pnas.0601807103; Ouchi N, 2011, NAT REV IMMUNOL, V11, P85, DOI 10.1038/nri2921; Ozawa T, 2018, GASTROENTEROLOGY, V154, P844, DOI 10.1053/j.gastro.2017.11.275; Peng W, 2018, ONCOL REP, V40, P1026, DOI 10.3892/or.2018.6510; Pucino V, 2014, CHEM IMMUNOL ALLERGY, V99, P155, DOI 10.1159/000353557; Rodríguez-Fraile M, 2020, REV ESP MED NUCL IMA, V39, P57, DOI 10.1016/j.remn.2019.09.009; Roux Ludovic, 2013, J Pathol Inform, V4, P8, DOI 10.4103/2153-3539.112693; Rozek LS, 2016, JNCI-J NATL CANCER I, V108, DOI 10.1093/jnci/djw027; Schneider NI, 2014, CANCER MANAG RES, V6, P291, DOI 10.2147/CMAR.S38827; Shapero TF, 2017, CAN J GASTROENTEROL, V2017, DOI 10.1155/2017/8750967; Shibutani M, 2018, IN VIVO, V32, P151, DOI 10.21873/invivo.11218; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Sirinukunwattana K, 2016, IEEE T MED IMAGING, V35, P1196, DOI 10.1109/TMI.2016.2525803; Sirinukunwattana K, 2015, IEEE T MED IMAGING, V34, P2366, DOI 10.1109/TMI.2015.2433900; Song YL, 2020, FRONT GENET, V11, DOI 10.3389/fgene.2020.00096; Stone RL, 2012, NEW ENGL J MED, V366, P610, DOI 10.1056/NEJMoa1110352; Tecchio C, 2014, FRONT IMMUNOL, V5, P1, DOI 10.3389/fimmu.2014.00508; Teng JA, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0149822; Therneau T.M., 2015, R Top Doc, V128, P28; Thorsson V, 2019, IMMUNITY, V51, P411, DOI [10.1016/j.immuni.2019.08.004, 10.1016/j.immuni.2018.03.023]; Tokunaga R, 2020, COLORECTAL DIS, V22, P62, DOI 10.1111/codi.14793; Turkki Riku, 2016, J Pathol Inform, V7, P38, DOI 10.4103/2153-3539.189703; Wang HB, 2014, J MED IMAGING, V1, DOI 10.1117/1.JMI.1.3.034003; Weinstein RS, 2009, HUM PATHOL, V40, P1057, DOI 10.1016/j.humpath.2009.04.006; Weiser MR, 2008, J CLIN ONCOL, V26, P380, DOI 10.1200/JCO.2007.14.1291; Wen YA, 2017, CELL DEATH DIS, V8, DOI 10.1038/cddis.2017.21; Winter JE, 2014, AM J CLIN NUTR, V99, P875, DOI 10.3945/ajcn.113.068122; Wulczyn E, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00427-2; Xu Y, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1685-x; Yoon KW, 2014, ONCOL REP, V31, P2493, DOI 10.3892/or.2014.3128; Yuan Y, 2016, ONCOL REP, V35, P2499, DOI 10.3892/or.2016.4660; Zhang JX, 2021, CANCER IMMUNOL IMMUN, V70, P137, DOI 10.1007/s00262-020-02668-8; Zhao K, 2020, EBIOMEDICINE, V61, DOI 10.1016/j.ebiom.2020.103054; Zheng R, 2018, BRIT J CANCER, V119, P130, DOI 10.1038/s41416-018-0121-y	76	5	5	0	11	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND	2296-861X			FRONT NUTR	Front. Nutr.	MAY 11	2022	9								869263	10.3389/fnut.2022.869263	http://dx.doi.org/10.3389/fnut.2022.869263			15	Nutrition & Dietetics	Science Citation Index Expanded (SCI-EXPANDED)	Nutrition & Dietetics	1M5VX	35634419	gold, Green Published			2024-09-18	WOS:000800037800001
J	Jheng, YC; Wang, YP; Lin, HE; Sung, KY; Chu, YC; Wang, HS; Jiang, JK; Hou, MC; Lee, FY; Lu, CL				Jheng, Ying-Chun; Wang, Yen-Po; Lin, Hung-En; Sung, Kuang-Yi; Chu, Yuan-Chia; Wang, Huann-Sheng; Jiang, Jeng-Kai; Hou, Ming-Chih; Lee, Fa-Yauh; Lu, Ching-Liang			A novel machine learning-based algorithm to identify and classify lesions and anatomical landmarks in colonoscopy images	SURGICAL ENDOSCOPY AND OTHER INTERVENTIONAL TECHNIQUES			English	Article						Artificial intelligence; Colonoscopy; Colon diseases; Computer-aided diagnosis system; convolution neural network; Heat map	ARTIFICIAL-INTELLIGENCE; COLORECTAL POLYPS; TIME	Objectives Computer-aided diagnosis (CAD)-based artificial intelligence (AI) has been shown to be highly accurate for detecting and characterizing colon polyps. However, the application of AI to identify normal colon landmarks and differentiate multiple colon diseases has not yet been established. We aimed to develop a convolutional neural network (CNN)-based algorithm (GUTAID) to recognize different colon lesions and anatomical landmarks. Methods Colonoscopic images were obtained to train and validate the AI classifiers. An independent dataset was collected for verification. The architecture of GUTAID contains two major sub-models: the Normal, Polyp, Diverticulum, Cecum and CAncer (NPDCCA) and Narrow-Band Imaging for Adenomatous/Hyperplastic polyps (NBI-AH) models. The development of GUTAID was based on the 16-layer Visual Geometry Group (VGG16) architecture and implemented on Google Cloud Platform. Results In total, 7838 colonoscopy images were used for developing and validating the AI model. An additional 1273 images were independently applied to verify the GUTAID. The accuracy for GUTAID in detecting various colon lesions/landmarks is 93.3% for polyps, 93.9% for diverticula, 91.7% for cecum, 97.5% for cancer, and 83.5% for adenomatous/hyperplastic polyps. Conclusions A CNN-based algorithm (GUTAID) to identify colonic abnormalities and landmarks was successfully established with high accuracy. This GUTAID system can further characterize polyps for optical diagnosis. We demonstrated that AI classification methodology is feasible to identify multiple and different colon diseases.	[Jheng, Ying-Chun; Wang, Yen-Po; Lin, Hung-En; Sung, Kuang-Yi; Wang, Huann-Sheng; Hou, Ming-Chih; Lu, Ching-Liang] Taipei Vet Gen Hosp, Endoscopy Ctr Diag & Treatment, Taipei, Taiwan; [Jheng, Ying-Chun; Wang, Yen-Po; Lin, Hung-En; Sung, Kuang-Yi; Hou, Ming-Chih; Lee, Fa-Yauh; Lu, Ching-Liang] Taipei Vet Gen Hosp, Dept Med, Div Gastroenterol, Taipei, Taiwan; [Wang, Huann-Sheng; Jiang, Jeng-Kai] Taipei Vet Gen Hosp, Dept Surg, Div Colon & Rectum Surg, Taipei, Taiwan; [Chu, Yuan-Chia] Taipei Vet Gen Hosp, Informat Management Off, Taipei, Taiwan; [Jheng, Ying-Chun] Taipei Vet Gen Hosp, Dept Med Res, Taipei, Taiwan; [Wang, Yen-Po; Lu, Ching-Liang] Natl Yang Ming Univ, Inst Brain Sci, Sch Med, Taipei, Taiwan; [Jheng, Ying-Chun; Wang, Yen-Po; Lin, Hung-En; Sung, Kuang-Yi; Chu, Yuan-Chia; Wang, Huann-Sheng; Jiang, Jeng-Kai; Hou, Ming-Chih; Lee, Fa-Yauh; Lu, Ching-Liang] Natl Yang Ming Univ, Fac Med, Sch Med, Taipei, Taiwan	Taipei Veterans General Hospital; Taipei Veterans General Hospital; Taipei Veterans General Hospital; Taipei Veterans General Hospital; Taipei Veterans General Hospital; National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung University	Lu, CL (corresponding author), Taipei Vet Gen Hosp, Endoscopy Ctr Diag & Treatment, Taipei, Taiwan.; Lu, CL (corresponding author), Taipei Vet Gen Hosp, Dept Med, Div Gastroenterol, Taipei, Taiwan.; Lu, CL (corresponding author), Natl Yang Ming Univ, Inst Brain Sci, Sch Med, Taipei, Taiwan.; Lu, CL (corresponding author), Natl Yang Ming Univ, Fac Med, Sch Med, Taipei, Taiwan.	cllu@ym.edu.tw	Jheng, Ying-Chun/ABC-1179-2021; Shou Ming, Ou/GPJ-9589-2022	Wang, Yen-Po/0000-0002-3769-0020	Taipei Veterans General Hospital [V108B-020, V109B-041, V108E-004-4, V109E-002-5]	Taipei Veterans General Hospital(Taipei Veterans General Hospital)	This study was supported by Grants from the Taipei Veterans General Hospital (V108B-020, V109B-041, V108E-004-4, V109E-002-5).	Abu Dayyeh BK, 2015, GASTROINTEST ENDOSC, V81, P502, DOI 10.1016/j.gie.2014.12.022; Becq A, 2020, J CLIN GASTROENTEROL, V54, P554, DOI 10.1097/MCG.0000000000001272; Berzin TM, 2020, LANCET, V395, P485, DOI 10.1016/S0140-6736(20)30294-4; Borgli H, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00622-y; Byrne MF, 2019, GUT, V68, P94, DOI 10.1136/gutjnl-2017-314547; Chan GCY, 2017, IEEE I C SIGNAL IMAG, P493, DOI 10.1109/ICSIPA.2017.8120662; Chen PJ, 2018, GASTROENTEROLOGY, V154, P568, DOI 10.1053/j.gastro.2017.10.010; Cho M, 2018, INT J COLORECTAL DIS, V33, P549, DOI 10.1007/s00384-018-2980-3; Ding Z, 2019, GASTROENTEROLOGY, V157, P1044, DOI 10.1053/j.gastro.2019.06.025; Freedman MT, 2008, ACAD RADIOL, V15, P249, DOI 10.1016/j.acra.2007.07.010; Gong DX, 2020, LANCET GASTROENTEROL, V5, P352, DOI 10.1016/S2468-1253(19)30413-3; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hogarty DT, 2020, AM J CLIN DERMATOL, V21, P41, DOI 10.1007/s40257-019-00462-6; Hosny A, 2018, NAT REV CANCER, V18, P500, DOI 10.1038/s41568-018-0016-5; Hwang DK, 2019, THERANOSTICS, V9, P232, DOI 10.7150/thno.28447; Ioffe S., 2015, P INT C MACH LEARN L, DOI [10.48550/arXiv.1502.03167, DOI 10.48550/ARXIV.1502.03167]; Jin EH, 2020, GASTROENTEROLOGY, V158, P2169, DOI [10.1053/j.gastro.2020.02.036, 10.1053/j.gastro.2020.02.03]; Kudo S, 2020, CLIN GASTROENTEROL H, V18, P1874, DOI 10.1016/j.cgh.2019.09.009; Kudo SE, 2019, DIGEST ENDOSC, V31, P363, DOI 10.1111/den.13340; Li Jun, 2010, Diagn Ther Endosc, V2010, DOI 10.1155/2010/419796; Lui TKL, 2020, GASTROINTEST ENDOSC, V92, P11, DOI 10.1016/j.gie.2020.02.033; Martineau M, 2018, LECT NOTES COMPUT SC, V11182, P426, DOI 10.1007/978-3-030-01449-0_36; Misawa M, 2018, GASTROENTEROLOGY, V154, P2027, DOI 10.1053/j.gastro.2018.04.003; Mori Y, 2019, ENDOSCOPY, V51, P219, DOI 10.1055/a-0754-5556; Mori Y, 2018, ANN INTERN MED, V169, P357, DOI 10.7326/M18-0249; Mori Y, 2017, GASTROINTEST ENDOSC, V85, pAB510, DOI 10.1016/j.gie.2017.03.1178; Obuch Joshua C, 2015, Curr Treat Options Gastroenterol, V13, P156, DOI 10.1007/s11938-015-0046-y; Owais M, 2019, J CLIN MED, V8, DOI 10.3390/jcm8070986; Ozawa T, 2020, THER ADV GASTROENTER, V13, DOI 10.1177/1756284820910659; Rajpurkar P, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002686; Rangarajan AK, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-59108-x; Rezende Edmar, 2018, Information Technology - New Generations. 15th International Conference on Information Technology. Advances in Intelligent Systems and Computing (AISC 738), P51, DOI 10.1007/978-3-319-77028-4_9; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi XJ, 2015, ADV NEUR IN, V28; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Takenaka K, 2020, GASTROENTEROLOGY, V158, P2150, DOI 10.1053/j.gastro.2020.02.012; Topol EJ, 2019, NAT MED, V25, P44, DOI 10.1038/s41591-018-0300-7; Wang P, 2019, GUT, V68, P1813, DOI 10.1136/gutjnl-2018-317500; Wang SD, 2020, OPEN MED-WARSAW, V15, P190, DOI 10.1515/med-2020-0028; Wolf AMD, 2018, CA-CANCER J CLIN, V68, P250, DOI 10.3322/caac.21457; WOLFF WI, 1974, CANCER, V34, P912, DOI 10.1002/1097-0142(197409)34:3+<912::AID-CNCR2820340720>3.0.CO;2-P; Wu, 2018, GASTROENTEROLOGY, V154, pS, DOI [10.1016/S0016-5085(18)32100-0, DOI 10.1016/S0016-5085(18)32100-0]	44	11	11	0	8	SPRINGER	NEW YORK	ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES	0930-2794	1432-2218		SURG ENDOSC	Surg. Endosc.	JAN	2022	36	1					640	650		10.1007/s00464-021-08331-2	http://dx.doi.org/10.1007/s00464-021-08331-2		FEB 2021	11	Surgery	Science Citation Index Expanded (SCI-EXPANDED)	Surgery	YD3WN	33591447				2024-09-18	WOS:000618592500001
J	Theodosi, A; Ouzounis, S; Kostopoulos, S; Glotsos, D; Kalatzis, I; Asvestas, P; Tzelepi, V; Ravazoula, P; Cavouras, D; Sakellaropoulos, G				Theodosi, Angeliki; Ouzounis, Sotiris; Kostopoulos, Spiros; Glotsos, Dimitris; Kalatzis, Ioannis; Asvestas, Pantelis; Tzelepi, Vassiliki; Ravazoula, Panagiota; Cavouras, Dionisis; Sakellaropoulos, George			Employing machine learning and microscopy images of AIB1-stained biopsy material to assess the 5-year survival of patients with colorectal cancer	MICROSCOPY RESEARCH AND TECHNIQUE			English	Article						5&#8208; year survival; AIB1 expression; colorectal cancer; deep learning; immunohistochemistry; machine learning	INDEPENDENT PROGNOSTIC MARKER; TEXTURE ANALYSIS; BIOMARKERS; EXPRESSION; AIB1; OVEREXPRESSION; CLASSIFICATION; AMPLIFICATION; CARCINOMA; DIAGNOSIS	Our purpose was to employ microscopy images of amplified in breast cancer 1 (AIB1)-stained biopsy material of patients with colorectal cancer (CRC) to: (a) find statistically significant differences (SSDs) in the texture and color of the epithelial gland tissue, between 5-year survivors and non-survivors after the first diagnosis and (b) employ machine learning (ML) methods for predicting the CRC-patient 5-year survival. We collected biopsy material from 54 patients with diagnosed CRC from the archives of the University Hospital of Patras, Greece. Twenty-six of the patients had survived 5 years after the first diagnosis. We selected regions of interest containing the epithelial gland at different microscope lens magnifications. We computed 69 textural and color features. Furthermore, we identified features with SSDs between the two groups of patients and we designed a supervised ML system for predicting the CRC-patient 5-year survival. Additionally, we employed the VGG16 pretrained convolution neural network to extract deep learning (DL) features, the support vector machines classifier, and the bootstrap cross-validation method for boosting the accuracy of predicting 5-year survival. Fourteen features sustained SSDs between the two groups of patients. The supervised ML system achieved 87% accuracy in predicting 5-year survival. In comparison, the DL system, using images from all magnifications, gave 97% classification accuracy. Glandular texture in 5-year non-survivors appeared to be of lower contrast, coarseness, roughness, local pixel correlation, and lower AIB1 variation, all indicating loss of textural definition. The supervised ML system revealed useful information regarding features that discriminate between 5-year survivors and non-survivors while the DL system displayed superior accuracy by employing DL features.	[Theodosi, Angeliki; Sakellaropoulos, George] Univ Patras, Fac Med, Sch Hlth Sci, Dept Med Phys, Patras, Greece; [Ouzounis, Sotiris; Kostopoulos, Spiros; Glotsos, Dimitris; Kalatzis, Ioannis; Asvestas, Pantelis; Cavouras, Dionisis] Univ West Attica, Med Image & Signal Proc Lab, Dept Biomed Engn, Ag Spyridonos St 17, Athens 12243, Greece; [Tzelepi, Vassiliki; Ravazoula, Panagiota] Univ Hosp Patras, Dept Pathol, Patras, Greece	University of Patras; University of West Attica; University of Patras	Kostopoulos, S (corresponding author), Univ West Attica, Med Image & Signal Proc Lab, Dept Biomed Engn, Ag Spyridonos St 17, Athens 12243, Greece.	skostopoulos@uniwa.gr	Asvestas, Pantelis/U-8912-2019; Kalatzis, Ioannis/AAP-4418-2021; Ouzounis, Sotiris/HTR-3869-2023; Kostopoulos, Spiros/ADF-8129-2022; Cavouras, Dionisis/HRB-9563-2023; Glo, Dim/U-9028-2019	Glotsos, Dimitris/0000-0003-3252-4055; Kostopoulos, Spiros/0000-0003-0223-2502; Kalatzis, Ioannis/0000-0002-2192-3520; OUZOUNIS, SOTIRIS/0000-0001-7541-6885				Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Chee CG, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0182883; De Cecco CN, 2016, ABDOM RADIOL, V41, P1728, DOI 10.1007/s00261-016-0733-8; Eyl RE, 2018, HEALTH QUAL LIFE OUT, V16, DOI 10.1186/s12955-018-0934-7; Greene F.L., 2002, AJCC CANC STAGING MA, V6th; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; He LR, 2010, ANN ONCOL, V21, P1675, DOI 10.1093/annonc/mdp592; Jalil O, 2017, COLORECTAL DIS, V19, P349, DOI 10.1111/codi.13496; Kather JN, 2019, PLOS MED, V16, DOI 10.1371/journal.pmed.1002730; Lovinfosse P, 2018, EUR J NUCL MED MOL I, V45, P365, DOI 10.1007/s00259-017-3855-5; Luo JH, 2008, INT J CANCER, V122, P2554, DOI 10.1002/ijc.23399; Lv Y, 2018, INT J SURG, V59, P80, DOI 10.1016/j.ijsu.2018.09.020; MA Z, 2019, PATHOL RES PRACT, V215; Maurel J, 2015, CURR CANCER DRUG TAR, V15, P703, DOI 10.2174/156800961508151001102822; Miles KA, 2009, RADIOLOGY, V250, P444, DOI 10.1148/radiol.2502071879; Ng F, 2013, RADIOLOGY, V266, P177, DOI 10.1148/radiol.12120254; Piao LH, 2019, PATHOL RES PRACT, V215, DOI 10.1016/j.prp.2019.152437; Shang YY, 2018, PATHOL RES PRACT, V214, P1613, DOI 10.1016/j.prp.2018.08.012; Sidiropoulos K, 2012, COMPUT BIOL MED, V42, P376, DOI 10.1016/j.compbiomed.2011.12.004; Siegel RL, 2017, CA-CANCER J CLIN, V67, P177, DOI 10.3322/caac.21395; Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556; Soerjomataram I, 2012, CANCER CAUSE CONTROL, V23, P1421, DOI 10.1007/s10552-012-0010-2; Song XZ, 2019, CANCER LETT, V442, P310, DOI 10.1016/j.canlet.2018.11.012; SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q; Stenzinger A, 2012, HUM PATHOL, V43, P1314, DOI 10.1016/j.humpath.2011.10.012; Tamura H., 1978, IEEE Transactions on Systems, Man and Cybernetics, VSMC-8, P460, DOI 10.1109/TSMC.1978.4309999; Theodoridis S., 2010, Introduction to Pattern Recognition: A MATLAB Approach; Theodoridis S, 2003, Pattern_Recognition, V2nd; Theodosi A, 2019, APPL IMMUNOHISTO M M, V27, P749, DOI 10.1097/PAI.0000000000000691; Vacante M, 2018, WORLD J CLIN CASES, V6, P869, DOI 10.12998/wjcc.v6.i15.869; Wang Y, 2002, CANCER-AM CANCER SOC, V95, P2346, DOI 10.1002/cncr.10963; Xu Y, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1685-x; Yanzhe X, 2020, SCI REP-UK, V10, DOI [10.1038/s41598-019-57223-y, 10.1038/s41598-020-59115-y]; Ye CW, 2019, PATHOL RES PRACT, V215, DOI 10.1016/j.prp.2019.152478; Zarkavelis G, 2017, ANN GASTROENTEROL, V30, P613, DOI 10.20524/aog.2017.0191	35	3	3	0	2	WILEY	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	1059-910X	1097-0029		MICROSC RES TECHNIQ	Microsc. Res. Tech.	OCT	2021	84	10					2421	2433		10.1002/jemt.23797	http://dx.doi.org/10.1002/jemt.23797		APR 2021	13	Anatomy & Morphology; Biology; Microscopy	Science Citation Index Expanded (SCI-EXPANDED)	Anatomy & Morphology; Life Sciences & Biomedicine - Other Topics; Microscopy	UP2VY	33929071				2024-09-18	WOS:000645592300001
J	Ji, ZX; Qian, H; Ma, X				Ji, Zexuan; Qian, Hao; Ma, Xiao			Progressive Group Convolution Fusion network for colon polyp segmentation	BIOMEDICAL SIGNAL PROCESSING AND CONTROL			English	Article						Colon polyp segmentation; Progressive group convolution; Multi-scale feature fusion; Difference information		In the field of medical imaging, the automatic detection and segmentation of colon polyps is particularly crucial for the early diagnosis of colorectal cancer. However, existing methods often face limitations when processing polyp images, especially under low-contrast and blurred boundary conditions, which hinder the recognition of complex features and thus affect the accuracy and efficiency of diagnosis. The challenge is further compounded by a lack of flexibility and precision in differentiating polyps of various sizes and shapes. To address these challenges, this study presents an advanced segmentation method that integrates a Pyramid Vision Transformer (PVT) encoder with a Convolutional Neural Network (CNN) decoder. The encoder, which utilizes the multi-level transformer modules of the PVT, effectively captures the intricate details and contextual information of the image, enabling precise extraction of complex features within polyp images. The decoder incorporates a Progressive Grouped Convolutional Fusion (PGCF) module that extracts multi-scale features through dilated convolutional kernels with different dilation rates. Coupled with attention mechanisms and differential subtraction strategies, our method not only enhances the feature fusion capability but also significantly improves the delineation of polyp boundaries. By integrating the PGCF module, differential operations, and multi-scale fusion strategies, our approach overcomes the limitations of existing colon polyp segmentation techniques. Experimental results on a large-scale annotated colon polyp image dataset show that our method demonstrates excellent performance and robustness in localizing and segmenting polyps of diverse sizes, shapes, and textures. The source codes are available at: https://github.com/peanutHao/PGCF.	[Ji, Zexuan; Qian, Hao; Ma, Xiao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China	Nanjing University of Science & Technology	Qian, H (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.	qianhao@njust.edu.cn		Ma, Xiao/0000-0002-1842-5029; Qian, Hao/0009-0003-3249-7015	National Natural Science Founda-tion of China [62072241]	National Natural Science Founda-tion of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Founda-tion of China (Nos. 62072241) .	Alexandre Luis A., 2007, EUR C PRINC DAT MIN; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Dong Bo, 2023, Polyp-PVT: Polyp segmentation with PyramidVision transformers; Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Jain S, 2023, IEEE T MED IMAGING, V42, P3987, DOI 10.1109/TMI.2023.3320151; Kim T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2167, DOI 10.1145/3474085.3475375; Lin YJ, 2024, BIOMED SIGNAL PROCES, V89, DOI 10.1016/j.bspc.2023.105749; Liu XG, 2024, BIOMED SIGNAL PROCES, V89, DOI 10.1016/j.bspc.2023.105792; Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986; Lou AG, 2022, PROC SPIE, V12032, DOI 10.1117/12.2611802; Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142; Manikandan K., 2020, Journal of Physics: Conference Series, V1706, DOI 10.1088/1742-6596/1706/1/012106; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Siegel RL, 2023, CA-CANCER J CLIN, V73, P17, DOI 10.3322/caac.21763; Su YZ, 2023, BIOMED SIGNAL PROCES, V83, DOI 10.1016/j.bspc.2023.104699; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997; Wang JF, 2022, LECT NOTES COMPUT SC, V13433, P110, DOI 10.1007/978-3-031-16437-8_11; Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8; Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061; Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66; Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321; Wu C, 2022, COMPUT BIOL MED, V151, DOI 10.1016/j.compbiomed.2022.106274; Zhang RF, 2022, LECT NOTES COMPUT SC, V13433, P99, DOI 10.1007/978-3-031-16437-8_10; Zhang YY, 2023, BIOMED SIGNAL PROCES, V86, DOI 10.1016/j.bspc.2023.105133; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhao XQ, 2023, Arxiv, DOI arXiv:2303.10894; Zhao XQ, 2021, LECT NOTES COMPUT SC, V12901, P120, DOI 10.1007/978-3-030-87193-2_12; Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464	31	0	0	3	3	ELSEVIER SCI LTD	London	125 London Wall, London, ENGLAND	1746-8094	1746-8108		BIOMED SIGNAL PROCES	Biomed. Signal Process. Control	OCT	2024	96		B						106586	10.1016/j.bspc.2024.106586	http://dx.doi.org/10.1016/j.bspc.2024.106586		JUL 2024	10	Engineering, Biomedical	Science Citation Index Expanded (SCI-EXPANDED)	Engineering	YF6J3					2024-09-18	WOS:001267107300001
J	Liu, GQ; Chen, ZY; Liu, D; Chang, BF; Dou, Z				Liu, Guoqi; Chen, Zongyu; Liu, Dong; Chang, Baofang; Dou, Zhi			FTMF-Net: A Fourier Transform-Multiscale Feature Fusion Network for Segmentation of Small Polyp Objects	IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT			English	Article						Dilated convolution; Fourier transform (FT); multiscale feature fusion (MFF); small polyp object segmentation	IMAGES; PROSTATE; SHAPE; OCT	The detection and resection of small polyp objects in colonoscopy images are of great significance for the prevention of colorectal cancer (CRC). At present, blurred edges, variable lesion shapes, and intraclass dissimilarity pose challenges for accurately segmenting small polyp objects. In recent years, many deep learning methods based on convolutional neural networks (CNNs) have been proposed and successfully applied to polyp segmentation tasks. However, these methods still have three limitations: 1) limited ability to mine boundary detail information; 2) insufficient ability to capture rich global context information; and 3) introduced additional complex feature extraction operations. To alleviate these challenges, we propose a Fourier transform-multiscale feature fusion network (FTMF-Net) for segmentation of small polyp objects. The core idea includes two points: 1) Fourier transform (FT) module extracts more detailed boundary information and 2) multiscale feature fusion (MFF) module enriches global semantic feature information. FTMF-Net mainly has the following advantages: 1) the proposed model has excellent performance for small polyp object segmentation; 2) this method greatly reduces the complexity of the model without significantly increasing the number of network parameters; and 3) the network is relatively simple and easy to understand. Extensive experiments with 11 state-of-the-art (SOTA) methods on five small polyp object datasets show that our proposed FTMF-Net has superior segmentation performance.	[Liu, Guoqi; Chen, Zongyu; Liu, Dong; Chang, Baofang; Dou, Zhi] Henan Normal Univ, Coll Comp & Informat Engn, Xinxiang 453000, Peoples R China; [Liu, Guoqi; Chen, Zongyu; Liu, Dong] Key Lab Artificial Intelligence & Personalized Lea, Xinxiang 453000, Peoples R China	Henan Normal University	Chen, ZY (corresponding author), Henan Normal Univ, Coll Comp & Informat Engn, Xinxiang 453000, Peoples R China.	gqliu@htu.edu.cn; chenzongyu1010@163.com; liudong@htu.edu.cn; changbaofang@htu.edu.cn; 2015160@htu.edu.cn		Chen, Zongyu/0000-0001-9010-8135; Liu, Guoqi/0000-0002-4106-3222; Liu, Dong/0000-0003-4346-9565	National Natural Science Foundation of China [61901160, U1904123]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grant 61901160 and Grant U1904123.	Andersen JKH, 2020, PR MACH LEARN RES, V121, P19; Banik D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3015607; Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002; Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007; Brown JRG, 2022, CLIN GASTROENTEROL H, V20, P1499, DOI 10.1016/j.cgh.2021.09.009; Chen JW, 2018, IEEE T INSTRUM MEAS, V67, P257, DOI 10.1109/TIM.2017.2775345; Chen Rui, 2022, 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P942, DOI 10.1109/BIBM55620.2022.9995217; Cong RM, 2022, IEEE T CONSUM ELECTR, V68, P376, DOI 10.1109/TCE.2022.3205376; Cong RM, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196430; Dai Y, 2022, J HEALTHC ENG, V2022, DOI 10.1155/2022/9479563; Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26; Ding XM, 2020, PHOTONIX, V1, DOI 10.1186/s43074-020-00016-8; Dong B., 2023, CAAI ARTIF INTELL RE, P1; Faramarzi A, 2022, IMAGE ANAL STEREOL, V41, P133, DOI 10.5566/ias.2719; Felipe-Riverón EM, 2005, PATTERN RECOGN LETT, V26, P2579, DOI 10.1016/j.patrec.2005.06.004; Ganz M, 2012, IEEE T BIO-MED ENG, V59, P2144, DOI 10.1109/TBME.2012.2195314; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Ghani H. A., 2020, Int. J. Electr. Comput. Eng., V10, P1346; Gómez-Echavarría A, 2020, BIOCYBERN BIOMED ENG, V40, P1081, DOI 10.1016/j.bbe.2020.05.004; Gross S., 2009, Algorithmen - Systeme - Anwendungen Proceedings des Workshops, Bildverarbeitung fur die Medizin, P252, DOI 10.1007/978-3-540-93860-6_51; Guo S, 2019, LECT NOTES COMPUT SC, V11729, P213, DOI 10.1007/978-3-030-30508-6_18; Guo YB, 2020, COMM COM INF SC, V1065, P377, DOI 10.1007/978-3-030-39343-4_32; Hesamian MH, 2019, J DIGIT IMAGING, V32, P582, DOI 10.1007/s10278-019-00227-x; Hodge AC, 2006, COMPUT METH PROG BIO, V84, P99, DOI 10.1016/j.cmpb.2006.07.001; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]; Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]; Huy Ta Duc, 2022, ISBI, P1; Jha D, 2021, IEEE ACCESS, V9, P40496, DOI 10.1109/ACCESS.2021.3063716; Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049; Jia X, 2020, IEEE T AUTOM SCI ENG, V17, P1570, DOI 10.1109/TASE.2020.2964827; John Ann Maria, 2020, 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), P389, DOI 10.1109/I-SMAC49090.2020.9243510; Kolligs FT, 2016, VISC MED, V32, P158, DOI 10.1159/000446488; Lang A, 2013, BIOMED OPT EXPRESS, V4, P1133, DOI 10.1364/BOE.4.001133; Lee HJ, 2020, PROC CVPR IEEE, P4816, DOI 10.1109/CVPR42600.2020.00487; Liao X, 2017, MULTIMED TOOLS APPL, V76, P20739, DOI 10.1007/s11042-016-3971-4; Lin A., 2022, IEEE T INSTRUM MEAS, V71, P1; Lin L, 2021, LECT NOTES COMPUT SC, V12908, P65, DOI 10.1007/978-3-030-87237-3_7; Liu Q, 2021, Arxiv, DOI arXiv:2111.00193; Liu ZG, 2020, IEEE T INSTRUM MEAS, V69, P700, DOI 10.1109/TIM.2019.2905905; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lou A., 2022, P SPIE, P1; Luo Z., 2013, INT J SIGNAL PROCESS, V6, P441; Ma Y, 2023, BIOMED SIGNAL PROCES, V80, DOI 10.1016/j.bspc.2022.104320; Meng YD, 2022, IEEE T MED IMAGING, V41, P690, DOI 10.1109/TMI.2021.3123567; Nguyen D. H., 2018, TRANSLATING ARCHITEC, DOI [10.1007/978-3-031-16437-8_8, DOI 10.1007/978-3-031-16437-8_8]; Paing MP, 2023, IEEE ACCESS, V11, P16644, DOI 10.1109/ACCESS.2023.3246730; Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P164, DOI 10.1145/3083187.3083212; Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25; Safarov S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041441; Sánchez-Peralta LF, 2020, ARTIF INTELL MED, V108, DOI 10.1016/j.artmed.2020.101923; Sasmal P, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3082315; Shao YQ, 2015, MED IMAGE ANAL, V26, P345, DOI 10.1016/j.media.2015.06.007; Shi Yanhui, 2011, 2011 IEEE/ICME International Conference on Complex Medical Engineering - CME 2011, P478, DOI 10.1109/ICCME.2011.5876788; Siegel RL, 2020, CA-CANCER J CLIN, V70, P7, DOI [10.3322/caac.21590, 10.3322/caac.21601]; Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3; Sui LS, 2019, OPT LASER ENG, V113, P29, DOI 10.1016/j.optlaseng.2018.10.002; Nguyen TC, 2021, LECT NOTES COMPUT SC, V12901, P633, DOI 10.1007/978-3-030-87193-2_60; Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190; Wang C., 2018, P SPIE, V10574, P130; Wang JC, 2023, IEEE T MED IMAGING, V42, P1735, DOI 10.1109/TMI.2023.3236037; Wang JC, 2021, LECT NOTES COMPUT SC, V12901, P206, DOI 10.1007/978-3-030-87193-2_20; Wang KN, 2022, LECT NOTES COMPUT SC, V13433, P78, DOI 10.1007/978-3-031-16437-8_8; Wang M, 2022, LECT NOTES COMPUT SC, V13576, P125, DOI 10.1007/978-3-031-16525-2_13; Wang Ruxin, 2022, Med Image Anal, V78, P102395, DOI 10.1016/j.media.2022.102395; Wang S., MED IMAGE COMPUTING; Wu HS, 2022, MED IMAGE ANAL, V76, DOI 10.1016/j.media.2021.102327; Xi Y, 2021, TRANSL ONCOL, V14, DOI 10.1016/j.tranon.2021.101174; Xia HY, 2021, KNOWL-BASED SYST, V226, DOI 10.1016/j.knosys.2021.107140; Xie LX, 2020, IEEE T MED IMAGING, V39, P514, DOI 10.1109/TMI.2019.2930679; Xie YH, 2020, SIGNAL TRANSDUCT TAR, V5, DOI 10.1038/s41392-020-0116-z; Xinzi Sun, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P851, DOI 10.1109/ICMLA.2019.00148; Yamada M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50567-5; Yang C, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102457; Yang MJ, 2013, IEEE T BIO-MED ENG, V60, P479, DOI 10.1109/TBME.2012.2228644; Yang X., 2021, IEEE T INSTRUM MEAS, V70, P1; Yang Y. Gu, 2023, IEEE Trans. Instrum. Meas., V72, P1; Yao JH, 2004, IEEE T MED IMAGING, V23, P1344, DOI 10.1109/TMI.2004.826941; Yu J, 2021, IEEE T CYBERNETICS, V51, P1731, DOI 10.1109/TCYB.2020.2969046; Yu SS, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105816; Yue GH, 2022, IEEE J BIOMED HEALTH, V26, P4090, DOI 10.1109/JBHI.2022.3173948; Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2; Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944; Zhong JP, 2019, IEEE T INSTRUM MEAS, V68, P2849, DOI 10.1109/TIM.2018.2871353; Zhou CA, 2020, IEEE INT C BIOINFORM, P934, DOI 10.1109/BIBM49941.2020.9313542; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]; Zhu QK, 2020, IEEE T MED IMAGING, V39, P753, DOI 10.1109/TMI.2019.2935018	88	4	4	9	32	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	0018-9456	1557-9662		IEEE T INSTRUM MEAS	IEEE Trans. Instrum. Meas.		2023	72								5020815	10.1109/TIM.2023.3293880	http://dx.doi.org/10.1109/TIM.2023.3293880			15	Engineering, Electrical & Electronic; Instruments & Instrumentation	Science Citation Index Expanded (SCI-EXPANDED)	Engineering; Instruments & Instrumentation	N3OD2					2024-09-18	WOS:001036137100020
